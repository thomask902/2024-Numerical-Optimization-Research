The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-17:39:47
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 257.7006
Batch 50, Loss: 128.8146
Batch 75, Loss: 51.2588
Batch 100, Loss: 30.8073
Batch 125, Loss: 22.4267
Batch 150, Loss: 17.5482
Batch 175, Loss: 14.6941
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 295.8841006755829 seconds
Epoch 1 accuracy: 11.1%
Batch 25, Loss: 11.6825
Batch 50, Loss: 10.4273
Batch 75, Loss: 9.4917
Batch 100, Loss: 8.7637
Batch 125, Loss: 8.1691
Batch 150, Loss: 7.6784
Batch 175, Loss: 7.2738
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 282.1890695095062 seconds
Epoch 2 accuracy: 11.87%
Batch 25, Loss: 6.7076
Batch 50, Loss: 6.4188
Batch 75, Loss: 6.1588
Batch 100, Loss: 5.9222
Batch 125, Loss: 5.7064
Batch 150, Loss: 5.5085
Batch 175, Loss: 5.3264
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 330.9645080566406 seconds
Epoch 3 accuracy: 12.05%
Batch 25, Loss: 5.0515
Batch 50, Loss: 4.9039
Batch 75, Loss: 4.7662
Batch 100, Loss: 4.6377
Batch 125, Loss: 4.5174
Batch 150, Loss: 4.4046
Batch 175, Loss: 4.2984
Noise applied in 140 out of 192 batches, 72.92
Epoch 4 learning rate: 0.01
Epoch 4 time: 544.0125861167908 seconds
Epoch 4 accuracy: 12.11%
Batch 25, Loss: 4.1332
Batch 50, Loss: 4.0419
Batch 75, Loss: 3.9549
Batch 100, Loss: 3.8717
Batch 125, Loss: 3.7923
Batch 150, Loss: 3.7161
Batch 175, Loss: 3.6426
Noise applied in 311 out of 192 batches, 161.98
Epoch 5 learning rate: 0.01
Epoch 5 time: 526.4763605594635 seconds
Epoch 5 accuracy: 11.97%
Batch 25, Loss: 3.5246
Batch 50, Loss: 3.4573
Batch 75, Loss: 3.3915
Batch 100, Loss: 3.3271
Batch 125, Loss: 3.2639
Batch 150, Loss: 3.2019
Batch 175, Loss: 3.1410
Noise applied in 384 out of 192 batches, 200.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 469.6955325603485 seconds
Epoch 6 accuracy: 12.16%
Batch 25, Loss: 3.0412
Batch 50, Loss: 2.9846
Batch 75, Loss: 2.9316
Batch 100, Loss: 2.8832
Batch 125, Loss: 2.8397
Batch 150, Loss: 2.8001
Batch 175, Loss: 2.7638
Noise applied in 384 out of 192 batches, 200.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 469.36525440216064 seconds
Epoch 7 accuracy: 11.61%
Batch 25, Loss: 2.7085
Batch 50, Loss: 2.6784
Batch 75, Loss: 2.6501
Batch 100, Loss: 2.6233
Batch 125, Loss: 2.5979
Batch 150, Loss: 2.5739
Batch 175, Loss: 2.5512
Noise applied in 384 out of 192 batches, 200.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 470.8821437358856 seconds
Epoch 8 accuracy: 12.43%
Batch 25, Loss: 2.5157
Batch 50, Loss: 2.4959
Batch 75, Loss: 2.4771
Batch 100, Loss: 2.4590
Batch 125, Loss: 2.4418
Batch 150, Loss: 2.4254
Batch 175, Loss: 2.4097
Noise applied in 384 out of 192 batches, 200.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 470.64786553382874 seconds
Epoch 9 accuracy: 12.7%
Batch 25, Loss: 2.3848
Batch 50, Loss: 2.3706
Batch 75, Loss: 2.3570
Batch 100, Loss: 2.3440
Batch 125, Loss: 2.3314
Batch 150, Loss: 2.3192
Batch 175, Loss: 2.3075
Noise applied in 384 out of 192 batches, 200.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 474.2308359146118 seconds
Epoch 10 accuracy: 12.77%
Batch 25, Loss: 2.2887
Batch 50, Loss: 2.2779
Batch 75, Loss: 2.2674
Batch 100, Loss: 2.2572
Batch 125, Loss: 2.2474
Batch 150, Loss: 2.2379
Batch 175, Loss: 2.2287
Noise applied in 384 out of 192 batches, 200.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 478.0415997505188 seconds
Epoch 11 accuracy: 12.63%
Batch 25, Loss: 2.2138
Batch 50, Loss: 2.2053
Batch 75, Loss: 2.1970
Batch 100, Loss: 2.1888
Batch 125, Loss: 2.1808
Batch 150, Loss: 2.1730
Batch 175, Loss: 2.1653
Noise applied in 384 out of 192 batches, 200.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 488.047189950943 seconds
Epoch 12 accuracy: 12.84%
Batch 25, Loss: 2.1528
Batch 50, Loss: 2.1455
Batch 75, Loss: 2.1383
Batch 100, Loss: 2.1313
Batch 125, Loss: 2.1244
Batch 150, Loss: 2.1177
Batch 175, Loss: 2.1111
Noise applied in 384 out of 192 batches, 200.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 511.4992470741272 seconds
Epoch 13 accuracy: 12.94%
Batch 25, Loss: 2.1002
Batch 50, Loss: 2.0939
Batch 75, Loss: 2.0877
Batch 100, Loss: 2.0816
Batch 125, Loss: 2.0756
Batch 150, Loss: 2.0697
Batch 175, Loss: 2.0639
Noise applied in 384 out of 192 batches, 200.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 476.71868228912354 seconds
Epoch 14 accuracy: 12.93%
Batch 25, Loss: 2.0544
Batch 50, Loss: 2.0488
Batch 75, Loss: 2.0434
Batch 100, Loss: 2.0380
Batch 125, Loss: 2.0326
Batch 150, Loss: 2.0274
Batch 175, Loss: 2.0222
Noise applied in 384 out of 192 batches, 200.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 477.84291911125183 seconds
Epoch 15 accuracy: 12.83%
Batch 25, Loss: 2.0136
Batch 50, Loss: 2.0086
Batch 75, Loss: 2.0036
Batch 100, Loss: 1.9987
Batch 125, Loss: 1.9940
Batch 150, Loss: 1.9892
Batch 175, Loss: 1.9845
Noise applied in 384 out of 192 batches, 200.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 483.5505700111389 seconds
Epoch 16 accuracy: 12.91%
Batch 25, Loss: 1.9768
Batch 50, Loss: 1.9722
Batch 75, Loss: 1.9677
Batch 100, Loss: 1.9633
Batch 125, Loss: 1.9589
Batch 150, Loss: 1.9546
Batch 175, Loss: 1.9503
Noise applied in 384 out of 192 batches, 200.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 473.7009301185608 seconds
Epoch 17 accuracy: 12.97%
Batch 25, Loss: 1.9432
Batch 50, Loss: 1.9390
Batch 75, Loss: 1.9349
Batch 100, Loss: 1.9308
Batch 125, Loss: 1.9268
Batch 150, Loss: 1.9228
Batch 175, Loss: 1.9188
Noise applied in 384 out of 192 batches, 200.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 478.8105978965759 seconds
Epoch 18 accuracy: 13.17%
Batch 25, Loss: 1.9122
Batch 50, Loss: 1.9084
Batch 75, Loss: 1.9045
Batch 100, Loss: 1.9007
Batch 125, Loss: 1.8970
Batch 150, Loss: 1.8932
Batch 175, Loss: 1.8895
Noise applied in 384 out of 192 batches, 200.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 484.3505618572235 seconds
Epoch 19 accuracy: 13.07%
Batch 25, Loss: 1.8833
Batch 50, Loss: 1.8798
Batch 75, Loss: 1.8762
Batch 100, Loss: 1.8727
slurmstepd: error: *** JOB 24621250 ON gra942 CANCELLED AT 2024-09-04T20:09:40 DUE TO TIME LIMIT ***
