The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:07:45
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 1628.1906
Batch 50, Loss: 1115.5016
Batch 75, Loss: 543.1041
Batch 100, Loss: 314.9533
Batch 125, Loss: 189.2450
Batch 150, Loss: 122.7670
Batch 175, Loss: 89.4601
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 273.99962973594666 seconds
Epoch 1 accuracy: 10.89%
Batch 25, Loss: 68.0414
Batch 50, Loss: 56.9373
Batch 75, Loss: 49.6142
Batch 100, Loss: 44.9406
Batch 125, Loss: 41.6549
Batch 150, Loss: 39.0183
Batch 175, Loss: 36.7021
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 261.14963555336 seconds
Epoch 2 accuracy: 10.2%
Batch 25, Loss: 33.4748
Batch 50, Loss: 32.0052
Batch 75, Loss: 30.7070
Batch 100, Loss: 29.5462
Batch 125, Loss: 28.4879
Batch 150, Loss: 27.5281
Batch 175, Loss: 26.6493
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 261.3909242153168 seconds
Epoch 3 accuracy: 10.06%
Batch 25, Loss: 25.3254
Batch 50, Loss: 24.6073
Batch 75, Loss: 23.9278
Batch 100, Loss: 23.3416
Batch 125, Loss: 22.8144
Batch 150, Loss: 22.2237
Batch 175, Loss: 21.7720
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 262.1388509273529 seconds
Epoch 4 accuracy: 10.13%
Batch 25, Loss: 21.1310
Batch 50, Loss: 20.7840
Batch 75, Loss: 20.4574
Batch 100, Loss: 20.1448
Batch 125, Loss: 19.8413
Batch 150, Loss: 19.5430
Batch 175, Loss: 19.2445
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 262.8360013961792 seconds
Epoch 5 accuracy: 10.09%
Batch 25, Loss: 18.7788
Batch 50, Loss: 18.5190
Batch 75, Loss: 18.2666
Batch 100, Loss: 18.0197
Batch 125, Loss: 17.7817
Batch 150, Loss: 17.5555
Batch 175, Loss: 17.3396
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 262.3889696598053 seconds
Epoch 6 accuracy: 10.06%
Batch 25, Loss: 16.9848
Batch 50, Loss: 16.7719
Batch 75, Loss: 16.5261
Batch 100, Loss: 16.2863
Batch 125, Loss: 16.1063
Batch 150, Loss: 15.9447
Batch 175, Loss: 15.7922
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 262.5045726299286 seconds
Epoch 7 accuracy: 10.04%
Batch 25, Loss: 15.5504
Batch 50, Loss: 15.4128
Batch 75, Loss: 15.2785
Batch 100, Loss: 15.1479
Batch 125, Loss: 15.0213
Batch 150, Loss: 14.8980
Batch 175, Loss: 14.7784
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 262.88836193084717 seconds
Epoch 8 accuracy: 10.08%
Batch 25, Loss: 14.5849
Batch 50, Loss: 14.4727
Batch 75, Loss: 14.3600
Batch 100, Loss: 14.2218
Batch 125, Loss: 14.1006
Batch 150, Loss: 14.0003
Batch 175, Loss: 13.8961
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 264.66203451156616 seconds
Epoch 9 accuracy: 10.11%
Batch 25, Loss: 13.7323
Batch 50, Loss: 13.6436
Batch 75, Loss: 13.5561
Batch 100, Loss: 13.4682
Batch 125, Loss: 13.3776
Batch 150, Loss: 13.2825
Batch 175, Loss: 13.1934
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 262.88004207611084 seconds
Epoch 10 accuracy: 10.05%
Batch 25, Loss: 13.0605
Batch 50, Loss: 12.9850
Batch 75, Loss: 12.9106
Batch 100, Loss: 12.8380
Batch 125, Loss: 12.7665
Batch 150, Loss: 12.6965
Batch 175, Loss: 12.6276
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 262.2323443889618 seconds
Epoch 11 accuracy: 10.06%
Batch 25, Loss: 12.5145
Batch 50, Loss: 12.4485
Batch 75, Loss: 12.3828
Batch 100, Loss: 12.3176
Batch 125, Loss: 12.2527
Batch 150, Loss: 12.1886
Batch 175, Loss: 12.1253
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 262.65802907943726 seconds
Epoch 12 accuracy: 10.1%
Batch 25, Loss: 12.0206
Batch 50, Loss: 11.9593
Batch 75, Loss: 11.8990
Batch 100, Loss: 11.8399
Batch 125, Loss: 11.7826
Batch 150, Loss: 11.7263
Batch 175, Loss: 11.6710
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 262.4435091018677 seconds
Epoch 13 accuracy: 10.08%
Batch 25, Loss: 11.5812
Batch 50, Loss: 11.5293
Batch 75, Loss: 11.4782
Batch 100, Loss: 11.4277
Batch 125, Loss: 11.3780
Batch 150, Loss: 11.3289
Batch 175, Loss: 11.2806
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 262.29262924194336 seconds
Epoch 14 accuracy: 10.11%
Batch 25, Loss: 11.2010
Batch 50, Loss: 11.1546
Batch 75, Loss: 11.1088
Batch 100, Loss: 11.0638
Batch 125, Loss: 11.0195
Batch 150, Loss: 10.9756
Batch 175, Loss: 10.9322
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 262.9116909503937 seconds
Epoch 15 accuracy: 10.14%
Batch 25, Loss: 10.8607
Batch 50, Loss: 10.8187
Batch 75, Loss: 10.7770
Batch 100, Loss: 10.7348
Batch 125, Loss: 10.6882
Batch 150, Loss: 10.6365
Batch 175, Loss: 10.5960
Noise applied in 0 out of 3072 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 262.6397111415863 seconds
Epoch 16 accuracy: 10.16%
Batch 25, Loss: 10.5343
Batch 50, Loss: 10.4993
Batch 75, Loss: 10.4649
Batch 100, Loss: 10.4311
Batch 125, Loss: 10.3979
Batch 150, Loss: 10.3652
Batch 175, Loss: 10.3329
Noise applied in 0 out of 3264 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 263.00899386405945 seconds
Epoch 17 accuracy: 10.22%
Batch 25, Loss: 10.2791
Batch 50, Loss: 10.2476
Batch 75, Loss: 10.2164
Batch 100, Loss: 10.1855
Batch 125, Loss: 10.1546
Batch 150, Loss: 10.1238
Batch 175, Loss: 10.0932
Noise applied in 0 out of 3456 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 290.00012469291687 seconds
Epoch 18 accuracy: 10.25%
Batch 25, Loss: 10.0424
Batch 50, Loss: 10.0126
Batch 75, Loss: 9.9830
Batch 100, Loss: 9.9537
Batch 125, Loss: 9.9246
Batch 150, Loss: 9.8957
Batch 175, Loss: 9.8670
Noise applied in 0 out of 3648 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 293.7329771518707 seconds
Epoch 19 accuracy: 10.25%
Batch 25, Loss: 9.8191
Batch 50, Loss: 9.7907
Batch 75, Loss: 9.7624
Batch 100, Loss: 9.7341
Batch 125, Loss: 9.7060
Batch 150, Loss: 9.6779
Batch 175, Loss: 9.6498
Noise applied in 0 out of 3840 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 263.9270329475403 seconds
Epoch 20 accuracy: 10.29%
rho:  0.04 , alpha:  0.3
Total training time: 5322.702099084854 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 4.7592
Norm of the Gradient: 8.0916204453e+00
Smallest Hessian Eigenvalue: -0.9633
Noise Threshold: 0.2
Noise Radius: 0.5
