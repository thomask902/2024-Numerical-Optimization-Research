The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:216: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM/basicaug/lr-0.05/batchsize-128/2024-08-05-18:35:15
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 130.7568
Batch 20, Loss: 66.6629
Batch 30, Loss: 38.2491
Batch 40, Loss: 19.9913
Batch 50, Loss: 13.1662
Batch 60, Loss: 8.1226
Batch 70, Loss: 6.7955
Batch 80, Loss: 6.6752
Batch 90, Loss: 5.2558
Batch 100, Loss: 4.3638
Batch 110, Loss: 3.8400
Batch 120, Loss: 3.6856
Batch 130, Loss: 3.3865
Batch 140, Loss: 3.0457
Batch 150, Loss: 3.0326
Batch 160, Loss: 3.0349
Batch 170, Loss: 2.9017
Batch 180, Loss: 2.7158
Batch 190, Loss: 2.6690
Batch 200, Loss: 2.6197
Batch 210, Loss: 2.5514
Batch 220, Loss: 2.4841
Batch 230, Loss: 2.5725
Batch 240, Loss: 2.5706
Batch 250, Loss: 2.5415
Batch 260, Loss: 2.3774
Batch 270, Loss: 2.4695
Batch 280, Loss: 2.3360
Batch 290, Loss: 2.2841
Batch 300, Loss: 2.4235
Batch 310, Loss: 2.2967
Batch 320, Loss: 2.3430
Batch 330, Loss: 2.2607
Batch 340, Loss: 2.3078
Batch 350, Loss: 2.3024
Batch 360, Loss: 2.2380
Batch 370, Loss: 2.2857
Batch 380, Loss: 2.2109
Batch 390, Loss: 2.2438
Epoch 1 learning rate: 0.05
Epoch 1 time: 129.2532615661621 seconds
Epoch 1 accuracy: 12.72%
Batch 10, Loss: 2.1777
Batch 20, Loss: 2.1478
Batch 30, Loss: 2.1676
Batch 40, Loss: 2.1425
Batch 50, Loss: 2.1791
Batch 60, Loss: 2.1742
Batch 70, Loss: 2.1945
Batch 80, Loss: 2.1571
Batch 90, Loss: 2.1967
Batch 100, Loss: 2.1737
Batch 110, Loss: 2.1067
Batch 120, Loss: 2.1707
Batch 130, Loss: 2.1151
Batch 140, Loss: 2.1697
Batch 150, Loss: 2.1527
Batch 160, Loss: 2.1089
Batch 170, Loss: 2.0815
Batch 180, Loss: 2.1244
Batch 190, Loss: 2.1234
Batch 200, Loss: 2.1880
Batch 210, Loss: 2.0817
Batch 220, Loss: 2.1005
Batch 230, Loss: 2.1222
Batch 240, Loss: 2.0699
Batch 250, Loss: 2.1243
Batch 260, Loss: 2.1742
Batch 270, Loss: 2.0695
Batch 280, Loss: 2.0465
Batch 290, Loss: 2.1001
Batch 300, Loss: 2.1288
Batch 310, Loss: 2.1312
Batch 320, Loss: 2.1340
Batch 330, Loss: 2.0711
Batch 340, Loss: 2.0617
Batch 350, Loss: 2.1022
Batch 360, Loss: 2.1004
Batch 370, Loss: 2.0710
Batch 380, Loss: 2.0202
Batch 390, Loss: 2.0826
Epoch 2 learning rate: 0.05
Epoch 2 time: 119.63897490501404 seconds
Epoch 2 accuracy: 15.0%
Batch 10, Loss: 2.0569
Batch 20, Loss: 2.0467
Batch 30, Loss: 1.9947
Batch 40, Loss: 2.0578
Batch 50, Loss: 2.0920
Batch 60, Loss: 2.1437
Batch 70, Loss: 2.1332
Batch 80, Loss: 2.0860
Batch 90, Loss: 2.0701
Batch 100, Loss: 2.0619
Batch 110, Loss: 2.0435
Batch 120, Loss: 2.0118
Batch 130, Loss: 2.0599
Batch 140, Loss: 1.9894
Batch 150, Loss: 2.0742
Batch 160, Loss: 2.0509
Batch 170, Loss: 2.0157
Batch 180, Loss: 2.0094
Batch 190, Loss: 2.0395
Batch 200, Loss: 2.0199
Batch 210, Loss: 2.0477
Batch 220, Loss: 2.0625
Batch 230, Loss: 2.0024
Batch 240, Loss: 2.0105
Batch 250, Loss: 1.9775
Batch 260, Loss: 1.9727
Batch 270, Loss: 2.0436
Batch 280, Loss: 2.0126
Batch 290, Loss: 2.0199
Batch 300, Loss: 1.9882
Batch 310, Loss: 1.9924
Batch 320, Loss: 1.9457
Batch 330, Loss: 1.9887
Batch 340, Loss: 2.0014
Batch 350, Loss: 2.0301
Batch 360, Loss: 1.9805
Batch 370, Loss: 2.0310
Batch 380, Loss: 2.0722
Batch 390, Loss: 2.0395
Epoch 3 learning rate: 0.05
Epoch 3 time: 120.03378891944885 seconds
Epoch 3 accuracy: 14.53%
Batch 10, Loss: 1.9883
Batch 20, Loss: 1.9819
Batch 30, Loss: 2.0010
Batch 40, Loss: 2.0253
Batch 50, Loss: 2.0219
Batch 60, Loss: 2.0266
Batch 70, Loss: 2.0009
Batch 80, Loss: 2.0181
Batch 90, Loss: 2.0244
Batch 100, Loss: 2.0119
Batch 110, Loss: 2.0097
Batch 120, Loss: 1.9650
Batch 130, Loss: 2.0540
Batch 140, Loss: 2.0218
Batch 150, Loss: 1.9680
Batch 160, Loss: 2.0005
Batch 170, Loss: 2.0250
Batch 180, Loss: 2.0025
Batch 190, Loss: 1.9481
Batch 200, Loss: 1.9336
Batch 210, Loss: 1.9860
Batch 220, Loss: 2.0160
Batch 230, Loss: 2.0018
Batch 240, Loss: 1.9811
Batch 250, Loss: 1.9815
Batch 260, Loss: 1.9953
Batch 270, Loss: 2.0251
Batch 280, Loss: 1.9903
Batch 290, Loss: 1.9485
Batch 300, Loss: 1.9562
Batch 310, Loss: 1.9433
Batch 320, Loss: 1.9482
Batch 330, Loss: 1.9461
Batch 340, Loss: 1.9345
Batch 350, Loss: 2.0030
Batch 360, Loss: 1.9382
Batch 370, Loss: 2.0228
Batch 380, Loss: 1.9545
Batch 390, Loss: 1.9330
Epoch 4 learning rate: 0.05
Epoch 4 time: 119.59197092056274 seconds
Epoch 4 accuracy: 15.39%
Batch 10, Loss: 2.0050
Batch 20, Loss: 1.9895
Batch 30, Loss: 1.9378
Batch 40, Loss: 1.9388
Batch 50, Loss: 1.9342
Batch 60, Loss: 1.9454
Batch 70, Loss: 1.9649
Batch 80, Loss: 1.9442
Batch 90, Loss: 1.9363
Batch 100, Loss: 1.9725
Batch 110, Loss: 1.9378
Batch 120, Loss: 1.9700
Batch 130, Loss: 1.9167
Batch 140, Loss: 1.9549
Batch 150, Loss: 1.9331
Batch 160, Loss: 1.9788
Batch 170, Loss: 1.9238
Batch 180, Loss: 1.9367
Batch 190, Loss: 1.9526
Batch 200, Loss: 1.8877
Batch 210, Loss: 1.9219
Batch 220, Loss: 2.0223
Batch 230, Loss: 1.9391
Batch 240, Loss: 1.9525
Batch 250, Loss: 1.9109
Batch 260, Loss: 1.9190
Batch 270, Loss: 1.9245
Batch 280, Loss: 1.9773
Batch 290, Loss: 1.9457
Batch 300, Loss: 1.9161
Batch 310, Loss: 1.9256
Batch 320, Loss: 1.9661
Batch 330, Loss: 1.9669
Batch 340, Loss: 1.9201
Batch 350, Loss: 1.9546
Batch 360, Loss: 1.9572
Batch 370, Loss: 1.9373
Batch 380, Loss: 1.9295
Batch 390, Loss: 1.9621
Epoch 5 learning rate: 0.05
Epoch 5 time: 119.6063003540039 seconds
Epoch 5 accuracy: 15.67%
Batch 10, Loss: 1.9410
Batch 20, Loss: 1.9617
Batch 30, Loss: 1.8695
Batch 40, Loss: 1.9648
Batch 50, Loss: 1.9427
Batch 60, Loss: 1.9369
Batch 70, Loss: 1.8912
Batch 80, Loss: 1.9074
Batch 90, Loss: 1.9323
Batch 100, Loss: 1.8861
Batch 110, Loss: 1.9050
Batch 120, Loss: 1.9050
Batch 130, Loss: 1.8973
Batch 140, Loss: 1.8944
Batch 150, Loss: 1.9310
Batch 160, Loss: 1.9318
Batch 170, Loss: 1.8888
Batch 180, Loss: 1.9158
Batch 190, Loss: 1.9522
Batch 200, Loss: 1.8987
Batch 210, Loss: 1.8951
Batch 220, Loss: 1.9040
Batch 230, Loss: 1.9034
Batch 240, Loss: 1.8928
Batch 250, Loss: 1.9432
Batch 260, Loss: 1.9253
Batch 270, Loss: 1.8952
Batch 280, Loss: 1.8668
Batch 290, Loss: 1.9064
Batch 300, Loss: 1.8742
Batch 310, Loss: 1.9104
Batch 320, Loss: 1.9059
Batch 330, Loss: 1.8772
Batch 340, Loss: 1.9040
Batch 350, Loss: 1.9303
Batch 360, Loss: 1.8771
Batch 370, Loss: 1.8695
Batch 380, Loss: 1.9132
Batch 390, Loss: 1.8593
Epoch 6 learning rate: 0.05
Epoch 6 time: 119.60399508476257 seconds
Epoch 6 accuracy: 14.58%
Batch 10, Loss: 1.8785
Batch 20, Loss: 1.8969
Batch 30, Loss: 1.8846
Batch 40, Loss: 1.8716
Batch 50, Loss: 1.8778
Batch 60, Loss: 1.8491
Batch 70, Loss: 1.8649
Batch 80, Loss: 1.8453
Batch 90, Loss: 1.8653
Batch 100, Loss: 1.8737
Batch 110, Loss: 1.8723
Batch 120, Loss: 1.8455
Batch 130, Loss: 1.8370
Batch 140, Loss: 1.8556
Batch 150, Loss: 1.8568
Batch 160, Loss: 1.8467
Batch 170, Loss: 1.8602
Batch 180, Loss: 1.8392
Batch 190, Loss: 1.8737
Batch 200, Loss: 1.8776
Batch 210, Loss: 1.8547
Batch 220, Loss: 1.8385
Batch 230, Loss: 1.8406
Batch 240, Loss: 1.8515
Batch 250, Loss: 1.8840
Batch 260, Loss: 1.8645
Batch 270, Loss: 1.8523
Batch 280, Loss: 1.9046
Batch 290, Loss: 1.8717
Batch 300, Loss: 1.8661
Batch 310, Loss: 1.8512
Batch 320, Loss: 1.8364
Batch 330, Loss: 1.8639
Batch 340, Loss: 1.8331
Batch 350, Loss: 1.8478
Batch 360, Loss: 1.8454
Batch 370, Loss: 1.8211
Batch 380, Loss: 1.8475
Batch 390, Loss: 1.8159
Epoch 7 learning rate: 0.05
Epoch 7 time: 119.57159948348999 seconds
Epoch 7 accuracy: 15.01%
Batch 10, Loss: 1.8300
Batch 20, Loss: 1.8313
Batch 30, Loss: 1.8278
Batch 40, Loss: 1.8368
Batch 50, Loss: 1.8303
Batch 60, Loss: 1.8444
Batch 70, Loss: 1.8427
Batch 80, Loss: 1.8085
Batch 90, Loss: 1.8232
Batch 100, Loss: 1.8161
Batch 110, Loss: 1.8209
Batch 120, Loss: 1.8207
Batch 130, Loss: 1.8145
Batch 140, Loss: 1.8182
Batch 150, Loss: 1.8138
Batch 160, Loss: 1.8036
Batch 170, Loss: 1.8127
Batch 180, Loss: 1.8131
Batch 190, Loss: 1.8001
Batch 200, Loss: 1.7889
Batch 210, Loss: 1.7900
Batch 220, Loss: 1.7886
Batch 230, Loss: 1.8112
Batch 240, Loss: 1.8161
Batch 250, Loss: 1.7948
Batch 260, Loss: 1.8031
Batch 270, Loss: 1.7903
Batch 280, Loss: 1.7841
Batch 290, Loss: 1.7921
Batch 300, Loss: 1.7949
Batch 310, Loss: 1.7813
Batch 320, Loss: 1.7752
Batch 330, Loss: 1.7872
Batch 340, Loss: 1.7950
Batch 350, Loss: 1.7472
Batch 360, Loss: 1.7869
Batch 370, Loss: 1.7677
Batch 380, Loss: 1.7726
Batch 390, Loss: 1.7929
Epoch 8 learning rate: 0.05
Epoch 8 time: 119.8002724647522 seconds
Epoch 8 accuracy: 14.02%
Batch 10, Loss: 1.7870
Batch 20, Loss: 1.7680
Batch 30, Loss: 1.7658
Batch 40, Loss: 1.7609
Batch 50, Loss: 1.7788
Batch 60, Loss: 1.7633
Batch 70, Loss: 1.7612
Batch 80, Loss: 1.7664
Batch 90, Loss: 1.7722
Batch 100, Loss: 1.7668
Batch 110, Loss: 1.7636
Batch 120, Loss: 1.7456
Batch 130, Loss: 1.7549
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7643
Batch 160, Loss: 1.7606
Batch 170, Loss: 1.7837
Batch 180, Loss: 1.7592
Batch 190, Loss: 1.7420
Batch 200, Loss: 1.7460
Batch 210, Loss: 1.7512
Batch 220, Loss: 1.7560
Batch 230, Loss: 1.7561
Batch 240, Loss: 1.7508
Batch 250, Loss: 1.7530
Batch 260, Loss: 1.7658
Batch 270, Loss: 1.7463
Batch 280, Loss: 1.7507
Batch 290, Loss: 1.7358
Batch 300, Loss: 1.7463
Batch 310, Loss: 1.7569
Batch 320, Loss: 1.7494
Batch 330, Loss: 1.7486
Batch 340, Loss: 1.7460
Batch 350, Loss: 1.7466
Batch 360, Loss: 1.7403
Batch 370, Loss: 1.7424
Batch 380, Loss: 1.7594
Batch 390, Loss: 1.7697
Epoch 9 learning rate: 0.05
Epoch 9 time: 119.83865880966187 seconds
Epoch 9 accuracy: 14.9%
Batch 10, Loss: 1.7565
Batch 20, Loss: 1.7400
Batch 30, Loss: 1.7412
Batch 40, Loss: 1.7432
Batch 50, Loss: 1.7388
Batch 60, Loss: 1.7476
Batch 70, Loss: 1.7388
Batch 80, Loss: 1.7392
Batch 90, Loss: 1.7479
Batch 100, Loss: 1.7529
Batch 110, Loss: 1.7324
Batch 120, Loss: 1.7418
Batch 130, Loss: 1.7336
Batch 140, Loss: 1.7540
Batch 150, Loss: 1.7350
Batch 160, Loss: 1.7371
Batch 170, Loss: 1.7479
Batch 180, Loss: 1.7548
Batch 190, Loss: 1.7313
Batch 200, Loss: 1.7434
Batch 210, Loss: 1.7471
Batch 220, Loss: 1.7424
Batch 230, Loss: 1.7421
Batch 240, Loss: 1.7378
Batch 250, Loss: 1.7310
Batch 260, Loss: 1.7320
Batch 270, Loss: 1.7471
Batch 280, Loss: 1.7338
Batch 290, Loss: 1.7424
Batch 300, Loss: 1.7477
Batch 310, Loss: 1.7339
Batch 320, Loss: 1.7454
Batch 330, Loss: 1.7432
Batch 340, Loss: 1.7491
Batch 350, Loss: 1.7423
Batch 360, Loss: 1.7407
Batch 370, Loss: 1.7491
Batch 380, Loss: 1.7376
Batch 390, Loss: 1.7523
Epoch 10 learning rate: 0.05
Epoch 10 time: 119.92214727401733 seconds
Epoch 10 accuracy: 12.81%
Batch 10, Loss: 1.7375
Batch 20, Loss: 1.7386
Batch 30, Loss: 1.7344
Batch 40, Loss: 1.7315
Batch 50, Loss: 1.7418
Batch 60, Loss: 1.7378
Batch 70, Loss: 1.7450
Batch 80, Loss: 1.7380
Batch 90, Loss: 1.7466
Batch 100, Loss: 1.7503
Batch 110, Loss: 1.7486
Batch 120, Loss: 1.7488
Batch 130, Loss: 1.7426
Batch 140, Loss: 1.7370
Batch 150, Loss: 1.7360
Batch 160, Loss: 1.7395
Batch 170, Loss: 1.7426
Batch 180, Loss: 1.7367
Batch 190, Loss: 1.7375
Batch 200, Loss: 1.7368
Batch 210, Loss: 1.7350
Batch 220, Loss: 1.7374
Batch 230, Loss: 1.7440
Batch 240, Loss: 1.7351
Batch 250, Loss: 1.7359
Batch 260, Loss: 1.7414
Batch 270, Loss: 1.7455
Batch 280, Loss: 1.7311
Batch 290, Loss: 1.7429
Batch 300, Loss: 1.7313
Batch 310, Loss: 1.7375
Batch 320, Loss: 1.7470
Batch 330, Loss: 1.7406
Batch 340, Loss: 1.7315
Batch 350, Loss: 1.7361
Batch 360, Loss: 1.7388
Batch 370, Loss: 1.7435
Batch 380, Loss: 1.7373
Batch 390, Loss: 1.7404
Epoch 11 learning rate: 0.05
Epoch 11 time: 119.75846743583679 seconds
Epoch 11 accuracy: 10.38%
Batch 10, Loss: 1.7412
Batch 20, Loss: 1.7380
Batch 30, Loss: 1.7441
Batch 40, Loss: 1.7405
Batch 50, Loss: 1.7374
Batch 60, Loss: 1.7416
Batch 70, Loss: 1.7333
Batch 80, Loss: 1.7334
Batch 90, Loss: 1.7463
Batch 100, Loss: 1.7348
Batch 110, Loss: 1.7355
Batch 120, Loss: 1.7413
Batch 130, Loss: 1.7420
Batch 140, Loss: 1.7427
Batch 150, Loss: 1.7326
Batch 160, Loss: 1.7385
Batch 170, Loss: 1.7337
Batch 180, Loss: 1.7516
Batch 190, Loss: 1.7416
Batch 200, Loss: 1.7352
Batch 210, Loss: 1.7456
Batch 220, Loss: 1.7369
Batch 230, Loss: 1.7386
Batch 240, Loss: 1.7399
Batch 250, Loss: 1.7305
Batch 260, Loss: 1.7369
Batch 270, Loss: 1.7434
Batch 280, Loss: 1.7386
Batch 290, Loss: 1.7345
Batch 300, Loss: 1.7416
Batch 310, Loss: 1.7290
Batch 320, Loss: 1.7399
Batch 330, Loss: 1.7369
Batch 340, Loss: 1.7585
Batch 350, Loss: 1.7416
Batch 360, Loss: 1.7437
Batch 370, Loss: 1.7392
Batch 380, Loss: 1.7411
Batch 390, Loss: 1.7381
Epoch 12 learning rate: 0.05
Epoch 12 time: 119.80619955062866 seconds
Epoch 12 accuracy: 10.37%
Batch 10, Loss: 1.7384
Batch 20, Loss: 1.7375
Batch 30, Loss: 1.7355
Batch 40, Loss: 1.7393
Batch 50, Loss: 1.7380
Batch 60, Loss: 1.7353
Batch 70, Loss: 1.7325
Batch 80, Loss: 1.7443
Batch 90, Loss: 1.7379
Batch 100, Loss: 1.7442
Batch 110, Loss: 1.7359
Batch 120, Loss: 1.7401
Batch 130, Loss: 1.7403
Batch 140, Loss: 1.7371
Batch 150, Loss: 1.7461
Batch 160, Loss: 1.7378
Batch 170, Loss: 1.7451
Batch 180, Loss: 1.7363
Batch 190, Loss: 1.7331
Batch 200, Loss: 1.7454
Batch 210, Loss: 1.7374
Batch 220, Loss: 1.7337
Batch 230, Loss: 1.7416
Batch 240, Loss: 1.7383
Batch 250, Loss: 1.7516
Batch 260, Loss: 1.7485
Batch 270, Loss: 1.7346
Batch 280, Loss: 1.7463
Batch 290, Loss: 1.7376
Batch 300, Loss: 1.7403
Batch 310, Loss: 1.7448
Batch 320, Loss: 1.7363
Batch 330, Loss: 1.7497
Batch 340, Loss: 1.7378
Batch 350, Loss: 1.7368
Batch 360, Loss: 1.7309
Batch 370, Loss: 1.7427
Batch 380, Loss: 1.7410
Batch 390, Loss: 1.7398
Epoch 13 learning rate: 0.05
Epoch 13 time: 119.57047200202942 seconds
Epoch 13 accuracy: 9.59%
Batch 10, Loss: 1.7492
Batch 20, Loss: 1.7367
Batch 30, Loss: 1.7378
Batch 40, Loss: 1.7443
Batch 50, Loss: 1.7372
Batch 60, Loss: 1.7432
Batch 70, Loss: 1.7408
Batch 80, Loss: 1.7413
Batch 90, Loss: 1.7410
Batch 100, Loss: 1.7411
Batch 110, Loss: 1.7403
Batch 120, Loss: 1.7352
Batch 130, Loss: 1.7435
Batch 140, Loss: 1.7383
Batch 150, Loss: 1.7448
Batch 160, Loss: 1.7382
Batch 170, Loss: 1.7443
Batch 180, Loss: 1.7413
Batch 190, Loss: 1.7447
Batch 200, Loss: 1.7358
Batch 210, Loss: 1.7359
Batch 220, Loss: 1.7384
Batch 230, Loss: 1.7368
Batch 240, Loss: 1.7346
Batch 250, Loss: 1.7352
Batch 260, Loss: 1.7406
Batch 270, Loss: 1.7368
Batch 280, Loss: 1.7501
Batch 290, Loss: 1.7430
Batch 300, Loss: 1.7411
Batch 310, Loss: 1.7430
Batch 320, Loss: 1.7423
Batch 330, Loss: 1.7396
Batch 340, Loss: 1.7440
Batch 350, Loss: 1.7456
Batch 360, Loss: 1.7489
Batch 370, Loss: 1.7418
Batch 380, Loss: 1.7469
Batch 390, Loss: 1.7383
Epoch 14 learning rate: 0.05
Epoch 14 time: 119.62975573539734 seconds
Epoch 14 accuracy: 10.02%
Batch 10, Loss: 1.7462
Batch 20, Loss: 1.7413
Batch 30, Loss: 1.7440
Batch 40, Loss: 1.7408
Batch 50, Loss: 1.7434
Batch 60, Loss: 1.7413
Batch 70, Loss: 1.7369
Batch 80, Loss: 1.7363
Batch 90, Loss: 1.7363
Batch 100, Loss: 1.7347
Batch 110, Loss: 1.7477
Batch 120, Loss: 1.7438
Batch 130, Loss: 1.7449
Batch 140, Loss: 1.7409
Batch 150, Loss: 1.7341
Batch 160, Loss: 1.7384
Batch 170, Loss: 1.7418
Batch 180, Loss: 1.7417
Batch 190, Loss: 1.7480
Batch 200, Loss: 1.7459
Batch 210, Loss: 1.7427
Batch 220, Loss: 1.7433
Batch 230, Loss: 1.7449
Batch 240, Loss: 1.7417
Batch 250, Loss: 1.7426
Batch 260, Loss: 1.7373
Batch 270, Loss: 1.7388
Batch 280, Loss: 1.7420
Batch 290, Loss: 1.7437
Batch 300, Loss: 1.7387
Batch 310, Loss: 1.7521
Batch 320, Loss: 1.7404
Batch 330, Loss: 1.7414
Batch 340, Loss: 1.7429
Batch 350, Loss: 1.7358
Batch 360, Loss: 1.7587
Batch 370, Loss: 1.7429
Batch 380, Loss: 1.7461
Batch 390, Loss: 1.7397
Epoch 15 learning rate: 0.05
Epoch 15 time: 119.77226114273071 seconds
Epoch 15 accuracy: 9.9%
Batch 10, Loss: 1.7435
Batch 20, Loss: 1.7391
Batch 30, Loss: 1.7419
Batch 40, Loss: 1.7387
Batch 50, Loss: 1.7452
Batch 60, Loss: 1.7392
Batch 70, Loss: 1.7510
Batch 80, Loss: 1.7485
Batch 90, Loss: 1.7459
Batch 100, Loss: 1.7489
Batch 110, Loss: 1.7432
Batch 120, Loss: 1.7382
Batch 130, Loss: 1.7409
Batch 140, Loss: 1.7421
Batch 150, Loss: 1.7423
Batch 160, Loss: 1.7445
Batch 170, Loss: 1.7431
Batch 180, Loss: 1.7483
Batch 190, Loss: 1.7441
Batch 200, Loss: 1.7412
Batch 210, Loss: 1.7437
Batch 220, Loss: 1.7420
Batch 230, Loss: 1.7468
Batch 240, Loss: 1.7400
Batch 250, Loss: 1.7422
Batch 260, Loss: 1.7449
Batch 270, Loss: 1.7345
Batch 280, Loss: 1.7424
Batch 290, Loss: 1.7422
Batch 300, Loss: 1.7427
Batch 310, Loss: 1.7427
Batch 320, Loss: 1.7462
Batch 330, Loss: 1.7456
Batch 340, Loss: 1.7528
Batch 350, Loss: 1.7409
Batch 360, Loss: 1.7435
Batch 370, Loss: 1.7393
Batch 380, Loss: 1.7427
Batch 390, Loss: 1.7517
Epoch 16 learning rate: 0.05
Epoch 16 time: 119.56817054748535 seconds
Epoch 16 accuracy: 9.87%
Batch 10, Loss: 1.7488
Batch 20, Loss: 1.7478
Batch 30, Loss: 1.7418
Batch 40, Loss: 1.7423
Batch 50, Loss: 1.7427
Batch 60, Loss: 1.7429
Batch 70, Loss: 1.7477
Batch 80, Loss: 1.7442
Batch 90, Loss: 1.7458
Batch 100, Loss: 1.7430
Batch 110, Loss: 1.7466
Batch 120, Loss: 1.7423
Batch 130, Loss: 1.7455
Batch 140, Loss: 1.7443
Batch 150, Loss: 1.7424
Batch 160, Loss: 1.7413
Batch 170, Loss: 1.7463
Batch 180, Loss: 1.7504
Batch 190, Loss: 1.7436
Batch 200, Loss: 1.7437
Batch 210, Loss: 1.7433
Batch 220, Loss: 1.7451
Batch 230, Loss: 1.7481
Batch 240, Loss: 1.7457
Batch 250, Loss: 1.7420
Batch 260, Loss: 1.7437
Batch 270, Loss: 1.7424
Batch 280, Loss: 1.7413
Batch 290, Loss: 1.7407
Batch 300, Loss: 1.7434
Batch 310, Loss: 1.7456
Batch 320, Loss: 1.7467
Batch 330, Loss: 1.7489
Batch 340, Loss: 1.7471
Batch 350, Loss: 1.7432
Batch 360, Loss: 1.7456
Batch 370, Loss: 1.7474
Batch 380, Loss: 1.7438
Batch 390, Loss: 1.7429
Epoch 17 learning rate: 0.05
Epoch 17 time: 119.55790972709656 seconds
Epoch 17 accuracy: 10.67%
Batch 10, Loss: 1.7461
Batch 20, Loss: 1.7455
Batch 30, Loss: 1.7427
Batch 40, Loss: 1.7410
Batch 50, Loss: 1.7498
Batch 60, Loss: 1.7445
Batch 70, Loss: 1.7411
Batch 80, Loss: 1.7494
Batch 90, Loss: 1.7412
Batch 100, Loss: 1.7439
Batch 110, Loss: 1.7425
Batch 120, Loss: 1.7442
Batch 130, Loss: 1.7502
Batch 140, Loss: 1.7469
Batch 150, Loss: 1.7438
Batch 160, Loss: 1.7470
Batch 170, Loss: 1.7452
Batch 180, Loss: 1.7435
Batch 190, Loss: 1.7451
Batch 200, Loss: 1.7468
Batch 210, Loss: 1.7472
Batch 220, Loss: 1.7413
Batch 230, Loss: 1.7460
Batch 240, Loss: 1.7457
Batch 250, Loss: 1.7441
Batch 260, Loss: 1.7455
Batch 270, Loss: 1.7443
Batch 280, Loss: 1.7494
Batch 290, Loss: 1.7507
Batch 300, Loss: 1.7485
Batch 310, Loss: 1.7436
Batch 320, Loss: 1.7469
Batch 330, Loss: 1.7481
Batch 340, Loss: 1.7549
Batch 350, Loss: 1.7484
Batch 360, Loss: 1.7523
Batch 370, Loss: 1.7449
Batch 380, Loss: 1.7501
Batch 390, Loss: 1.7463
Epoch 18 learning rate: 0.05
Epoch 18 time: 119.75646734237671 seconds
Epoch 18 accuracy: 10.68%
Batch 10, Loss: 1.7484
Batch 20, Loss: 1.7468
Batch 30, Loss: 1.7420
Batch 40, Loss: 1.7478
Batch 50, Loss: 1.7485
Batch 60, Loss: 1.7466
Batch 70, Loss: 1.7415
Batch 80, Loss: 1.7426
Batch 90, Loss: 1.7428
Batch 100, Loss: 1.7431
Batch 110, Loss: 1.7476
Batch 120, Loss: 1.7492
Batch 130, Loss: 1.7484
Batch 140, Loss: 1.7489
Batch 150, Loss: 1.7445
Batch 160, Loss: 1.7491
Batch 170, Loss: 1.7439
Batch 180, Loss: 1.7526
Batch 190, Loss: 1.7443
Batch 200, Loss: 1.7468
Batch 210, Loss: 1.7522
Batch 220, Loss: 1.7489
Batch 230, Loss: 1.7454
Batch 240, Loss: 1.7451
Batch 250, Loss: 1.7477
Batch 260, Loss: 1.7499
Batch 270, Loss: 1.7479
Batch 280, Loss: 1.7499
Batch 290, Loss: 1.7463
Batch 300, Loss: 1.7443
Batch 310, Loss: 1.7524
Batch 320, Loss: 1.7498
Batch 330, Loss: 1.7485
Batch 340, Loss: 1.7495
Batch 350, Loss: 1.7491
Batch 360, Loss: 1.7461
Batch 370, Loss: 1.7469
Batch 380, Loss: 1.7471
Batch 390, Loss: 1.7449
Epoch 19 learning rate: 0.05
Epoch 19 time: 119.71286034584045 seconds
Epoch 19 accuracy: 10.6%
Batch 10, Loss: 1.7457
Batch 20, Loss: 1.7488
Batch 30, Loss: 1.7464
Batch 40, Loss: 1.7479
Batch 50, Loss: 1.7426
Batch 60, Loss: 1.7438
Batch 70, Loss: 1.7535
Batch 80, Loss: 1.7484
Batch 90, Loss: 1.7458
Batch 100, Loss: 1.7431
Batch 110, Loss: 1.7473
Batch 120, Loss: 1.7455
Batch 130, Loss: 1.7482
Batch 140, Loss: 1.7477
Batch 150, Loss: 1.7470
Batch 160, Loss: 1.7502
Batch 170, Loss: 1.7479
Batch 180, Loss: 1.7485
Batch 190, Loss: 1.7504
Batch 200, Loss: 1.7431
Batch 210, Loss: 1.7475
Batch 220, Loss: 1.7494
Batch 230, Loss: 1.7453
Batch 240, Loss: 1.7514
Batch 250, Loss: 1.7513
Batch 260, Loss: 1.7454
Batch 270, Loss: 1.7489
Batch 280, Loss: 1.7514
Batch 290, Loss: 1.7481
Batch 300, Loss: 1.7516
Batch 310, Loss: 1.7469
Batch 320, Loss: 1.7502
Batch 330, Loss: 1.7497
Batch 340, Loss: 1.7530
Batch 350, Loss: 1.7534
Batch 360, Loss: 1.7493
Batch 370, Loss: 1.7463
Batch 380, Loss: 1.7477
Batch 390, Loss: 1.7483
Epoch 20 learning rate: 0.05
Epoch 20 time: 119.78272032737732 seconds
Epoch 20 accuracy: 10.99%
Batch 10, Loss: 1.7475
Batch 20, Loss: 1.7516
Batch 30, Loss: 1.7444
Batch 40, Loss: 1.7493
Batch 50, Loss: 1.7478
Batch 60, Loss: 1.7475
Batch 70, Loss: 1.7537
Batch 80, Loss: 1.7484
Batch 90, Loss: 1.7492
Batch 100, Loss: 1.7492
Batch 110, Loss: 1.7477
Batch 120, Loss: 1.7464
Batch 130, Loss: 1.7503
Batch 140, Loss: 1.7449
Batch 150, Loss: 1.7484
Batch 160, Loss: 1.7500
Batch 170, Loss: 1.7482
Batch 180, Loss: 1.7471
Batch 190, Loss: 1.7501
Batch 200, Loss: 1.7497
Batch 210, Loss: 1.7485
Batch 220, Loss: 1.7477
Batch 230, Loss: 1.7468
Batch 240, Loss: 1.7481
Batch 250, Loss: 1.7541
Batch 260, Loss: 1.7490
Batch 270, Loss: 1.7484
Batch 280, Loss: 1.7521
Batch 290, Loss: 1.7519
Batch 300, Loss: 1.7512
Batch 310, Loss: 1.7470
Batch 320, Loss: 1.7466
Batch 330, Loss: 1.7518
Batch 340, Loss: 1.7501
Batch 350, Loss: 1.7485
Batch 360, Loss: 1.7460
Batch 370, Loss: 1.7497
Batch 380, Loss: 1.7512
Batch 390, Loss: 1.7514
Epoch 21 learning rate: 0.05
Epoch 21 time: 119.77762222290039 seconds
Epoch 21 accuracy: 11.27%
Batch 10, Loss: 1.7482
Batch 20, Loss: 1.7488
Batch 30, Loss: 1.7503
Batch 40, Loss: 1.7501
Batch 50, Loss: 1.7500
Batch 60, Loss: 1.7522
Batch 70, Loss: 1.7532
Batch 80, Loss: 1.7498
Batch 90, Loss: 1.7501
Batch 100, Loss: 1.7506
Batch 110, Loss: 1.7501
Batch 120, Loss: 1.7478
Batch 130, Loss: 1.7494
Batch 140, Loss: 1.7516
Batch 150, Loss: 1.7508
Batch 160, Loss: 1.7492
Batch 170, Loss: 1.7486
Batch 180, Loss: 1.7477
Batch 190, Loss: 1.7551
Batch 200, Loss: 1.7527
Batch 210, Loss: 1.7519
Batch 220, Loss: 1.7489
Batch 230, Loss: 1.7487
Batch 240, Loss: 1.7516
Batch 250, Loss: 1.7482
Batch 260, Loss: 1.7500
Batch 270, Loss: 1.7488
Batch 280, Loss: 1.7456
Batch 290, Loss: 1.7524
Batch 300, Loss: 1.7469
Batch 310, Loss: 1.7515
Batch 320, Loss: 1.7514
Batch 330, Loss: 1.7518
Batch 340, Loss: 1.7494
Batch 350, Loss: 1.7524
Batch 360, Loss: 1.7489
Batch 370, Loss: 1.7490
Batch 380, Loss: 1.7506
Batch 390, Loss: 1.7518
Epoch 22 learning rate: 0.05
Epoch 22 time: 119.88782978057861 seconds
Epoch 22 accuracy: 10.99%
Batch 10, Loss: 1.7523
Batch 20, Loss: 1.7497
Batch 30, Loss: 1.7480
Batch 40, Loss: 1.7534
Batch 50, Loss: 1.7490
Batch 60, Loss: 1.7508
Batch 70, Loss: 1.7537
Batch 80, Loss: 1.7489
Batch 90, Loss: 1.7524
Batch 100, Loss: 1.7490
Batch 110, Loss: 1.7495
Batch 120, Loss: 1.7491
Batch 130, Loss: 1.7517
Batch 140, Loss: 1.7507
Batch 150, Loss: 1.7508
Batch 160, Loss: 1.7549
Batch 170, Loss: 1.7552
Batch 180, Loss: 1.7517
Batch 190, Loss: 1.7503
Batch 200, Loss: 1.7509
Batch 210, Loss: 1.7521
Batch 220, Loss: 1.7514
Batch 230, Loss: 1.7503
Batch 240, Loss: 1.7466
Batch 250, Loss: 1.7495
Batch 260, Loss: 1.7488
Batch 270, Loss: 1.7555
Batch 280, Loss: 1.7527
Batch 290, Loss: 1.7532
Batch 300, Loss: 1.7500
Batch 310, Loss: 1.7483
Batch 320, Loss: 1.7515
Batch 330, Loss: 1.7499
Batch 340, Loss: 1.7501
Batch 350, Loss: 1.7503
Batch 360, Loss: 1.7519
Batch 370, Loss: 1.7512
Batch 380, Loss: 1.7528
Batch 390, Loss: 1.7505
Epoch 23 learning rate: 0.05
Epoch 23 time: 119.63591361045837 seconds
Epoch 23 accuracy: 9.9%
Batch 10, Loss: 1.7493
Batch 20, Loss: 1.7522
Batch 30, Loss: 1.7556
Batch 40, Loss: 1.7540
Batch 50, Loss: 1.7508
Batch 60, Loss: 1.7516
Batch 70, Loss: 1.7521
Batch 80, Loss: 1.7524
Batch 90, Loss: 1.7527
Batch 100, Loss: 1.7517
Batch 110, Loss: 1.7507
Batch 120, Loss: 1.7505
Batch 130, Loss: 1.7517
Batch 140, Loss: 1.7551
Batch 150, Loss: 1.7533
Batch 160, Loss: 1.7523
Batch 170, Loss: 1.7521
Batch 180, Loss: 1.7537
Batch 190, Loss: 1.7502
Batch 200, Loss: 1.7540
Batch 210, Loss: 1.7524
Batch 220, Loss: 1.7505
Batch 230, Loss: 1.7517
Batch 240, Loss: 1.7515
Batch 250, Loss: 1.7515
Batch 260, Loss: 1.7510
Batch 270, Loss: 1.7511
Batch 280, Loss: 1.7528
Batch 290, Loss: 1.7508
Batch 300, Loss: 1.7511
Batch 310, Loss: 1.7519
Batch 320, Loss: 1.7514
Batch 330, Loss: 1.7529
Batch 340, Loss: 1.7521
Batch 350, Loss: 1.7556
Batch 360, Loss: 1.7535
Batch 370, Loss: 1.7522
Batch 380, Loss: 1.7523
Batch 390, Loss: 1.7520
Epoch 24 learning rate: 0.05
Epoch 24 time: 119.79301691055298 seconds
Epoch 24 accuracy: 10.63%
Batch 10, Loss: 1.7548
Batch 20, Loss: 1.7516
Batch 30, Loss: 1.7535
Batch 40, Loss: 1.7517
Batch 50, Loss: 1.7511
Batch 60, Loss: 1.7516
Batch 70, Loss: 1.7522
Batch 80, Loss: 1.7527
Batch 90, Loss: 1.7525
Batch 100, Loss: 1.7524
Batch 110, Loss: 1.7531
Batch 120, Loss: 1.7511
Batch 130, Loss: 1.7537
Batch 140, Loss: 1.7534
Batch 150, Loss: 1.7532
Batch 160, Loss: 1.7529
Batch 170, Loss: 1.7532
Batch 180, Loss: 1.7539
Batch 190, Loss: 1.7520
Batch 200, Loss: 1.7528
Batch 210, Loss: 1.7516
Batch 220, Loss: 1.7544
Batch 230, Loss: 1.7573
Batch 240, Loss: 1.7517
Batch 250, Loss: 1.7519
Batch 260, Loss: 1.7546
Batch 270, Loss: 1.7530
Batch 280, Loss: 1.7517
Batch 290, Loss: 1.7559
Batch 300, Loss: 1.7527
Batch 310, Loss: 1.7533
Batch 320, Loss: 1.7514
Batch 330, Loss: 1.7550
Batch 340, Loss: 1.7533
Batch 350, Loss: 1.7514
Batch 360, Loss: 1.7533
Batch 370, Loss: 1.7550
Batch 380, Loss: 1.7543
Batch 390, Loss: 1.7547
Epoch 25 learning rate: 0.05
Epoch 25 time: 119.86128950119019 seconds
Epoch 25 accuracy: 11.14%
Batch 10, Loss: 1.7542
Batch 20, Loss: 1.7538
Batch 30, Loss: 1.7554
Batch 40, Loss: 1.7552
Batch 50, Loss: 1.7538
Batch 60, Loss: 1.7544
Batch 70, Loss: 1.7529
Batch 80, Loss: 1.7542
Batch 90, Loss: 1.7546
Batch 100, Loss: 1.7523
Batch 110, Loss: 1.7535
Batch 120, Loss: 1.7535
Batch 130, Loss: 1.7537
Batch 140, Loss: 1.7535
Batch 150, Loss: 1.7532
Batch 160, Loss: 1.7537
Batch 170, Loss: 1.7547
Batch 180, Loss: 1.7553
Batch 190, Loss: 1.7541
Batch 200, Loss: 1.7551
Batch 210, Loss: 1.7543
Batch 220, Loss: 1.7533
Batch 230, Loss: 1.7536
Batch 240, Loss: 1.7551
Batch 250, Loss: 1.7537
Batch 260, Loss: 1.7548
Batch 270, Loss: 1.7539
Batch 280, Loss: 1.7544
Batch 290, Loss: 1.7554
Batch 300, Loss: 1.7540
Batch 310, Loss: 1.7549
Batch 320, Loss: 1.7544
Batch 330, Loss: 1.7546
Batch 340, Loss: 1.7552
Batch 350, Loss: 1.7540
Batch 360, Loss: 1.7544
Batch 370, Loss: 1.7545
Batch 380, Loss: 1.7548
Batch 390, Loss: 1.7535
Epoch 26 learning rate: 0.05
Epoch 26 time: 119.66552758216858 seconds
Epoch 26 accuracy: 11.03%
Batch 10, Loss: 1.7534
Batch 20, Loss: 1.7534
Batch 30, Loss: 1.7540
Batch 40, Loss: 1.7552
Batch 50, Loss: 1.7547
Batch 60, Loss: 1.7556
Batch 70, Loss: 1.7541
Batch 80, Loss: 1.7542
Batch 90, Loss: 1.7549
Batch 100, Loss: 1.7554
Batch 110, Loss: 1.7544
Batch 120, Loss: 1.7559
Batch 130, Loss: 1.7541
Batch 140, Loss: 1.7541
Batch 150, Loss: 1.7571
Batch 160, Loss: 1.7561
Batch 170, Loss: 1.7555
Batch 180, Loss: 1.7530
Batch 190, Loss: 1.7535
Batch 200, Loss: 1.7549
Batch 210, Loss: 1.7553
Batch 220, Loss: 1.7543
Batch 230, Loss: 1.7543
Batch 240, Loss: 1.7550
Batch 250, Loss: 1.7552
Batch 260, Loss: 1.7555
Batch 270, Loss: 1.7550
Batch 280, Loss: 1.7545
Batch 290, Loss: 1.7547
Batch 300, Loss: 1.7555
Batch 310, Loss: 1.7556
Batch 320, Loss: 1.7551
Batch 330, Loss: 1.7548
Batch 340, Loss: 1.7552
Batch 350, Loss: 1.7567
Batch 360, Loss: 1.7553
Batch 370, Loss: 1.7563
Batch 380, Loss: 1.7557
Batch 390, Loss: 1.7558
Epoch 27 learning rate: 0.05
Epoch 27 time: 119.61297512054443 seconds
Epoch 27 accuracy: 10.08%
Batch 10, Loss: 1.7557
Batch 20, Loss: 1.7564
Batch 30, Loss: 1.7561
Batch 40, Loss: 1.7558
Batch 50, Loss: 1.7541
Batch 60, Loss: 1.7555
Batch 70, Loss: 1.7546
Batch 80, Loss: 1.7548
Batch 90, Loss: 1.7549
Batch 100, Loss: 1.7545
Batch 110, Loss: 1.7574
Batch 120, Loss: 1.7571
Batch 130, Loss: 1.7571
Batch 140, Loss: 1.7550
Batch 150, Loss: 1.7553
Batch 160, Loss: 1.7554
Batch 170, Loss: 1.7554
Batch 180, Loss: 1.7553
Batch 190, Loss: 1.7551
Batch 200, Loss: 1.7546
Batch 210, Loss: 1.7551
Batch 220, Loss: 1.7569
Batch 230, Loss: 1.7554
Batch 240, Loss: 1.7561
Batch 250, Loss: 1.7555
Batch 260, Loss: 1.7548
Batch 270, Loss: 1.7573
Batch 280, Loss: 1.7559
Batch 290, Loss: 1.7563
Batch 300, Loss: 1.7562
Batch 310, Loss: 1.7561
Batch 320, Loss: 1.7557
Batch 330, Loss: 1.7552
Batch 340, Loss: 1.7565
Batch 350, Loss: 1.7556
Batch 360, Loss: 1.7566
Batch 370, Loss: 1.7565
Batch 380, Loss: 1.7556
Batch 390, Loss: 1.7567
Epoch 28 learning rate: 0.05
Epoch 28 time: 119.37014985084534 seconds
Epoch 28 accuracy: 8.09%
Batch 10, Loss: 1.7569
Batch 20, Loss: 1.7556
Batch 30, Loss: 1.7555
Batch 40, Loss: 1.7553
Batch 50, Loss: 1.7571
Batch 60, Loss: 1.7563
Batch 70, Loss: 1.7560
Batch 80, Loss: 1.7551
Batch 90, Loss: 1.7566
Batch 100, Loss: 1.7558
Batch 110, Loss: 1.7571
Batch 120, Loss: 1.7553
Batch 130, Loss: 1.7570
Batch 140, Loss: 1.7566
Batch 150, Loss: 1.7563
Batch 160, Loss: 1.7560
Batch 170, Loss: 1.7564
Batch 180, Loss: 1.7559
Batch 190, Loss: 1.7565
Batch 200, Loss: 1.7560
Batch 210, Loss: 1.7564
Batch 220, Loss: 1.7568
Batch 230, Loss: 1.7565
Batch 240, Loss: 1.7570
Batch 250, Loss: 1.7560
Batch 260, Loss: 1.7567
Batch 270, Loss: 1.7568
Batch 280, Loss: 1.7567
Batch 290, Loss: 1.7569
Batch 300, Loss: 1.7568
Batch 310, Loss: 1.7562
Batch 320, Loss: 1.7563
Batch 330, Loss: 1.7568
Batch 340, Loss: 1.7560
Batch 350, Loss: 1.7557
Batch 360, Loss: 1.7563
Batch 370, Loss: 1.7563
Batch 380, Loss: 1.7567
Batch 390, Loss: 1.7564
Epoch 29 learning rate: 0.05
Epoch 29 time: 119.60030269622803 seconds
Epoch 29 accuracy: 9.83%
Batch 10, Loss: 1.7563
Batch 20, Loss: 1.7560
Batch 30, Loss: 1.7567
Batch 40, Loss: 1.7566
Batch 50, Loss: 1.7561
Batch 60, Loss: 1.7559
Batch 70, Loss: 1.7565
Batch 80, Loss: 1.7565
Batch 90, Loss: 1.7568
Batch 100, Loss: 1.7566
Batch 110, Loss: 1.7570
Batch 120, Loss: 1.7567
Batch 130, Loss: 1.7564
Batch 140, Loss: 1.7568
Batch 150, Loss: 1.7566
Batch 160, Loss: 1.7569
Batch 170, Loss: 1.7565
Batch 180, Loss: 1.7567
Batch 190, Loss: 1.7562
Batch 200, Loss: 1.7566
Batch 210, Loss: 1.7566
Batch 220, Loss: 1.7570
Batch 230, Loss: 1.7566
Batch 240, Loss: 1.7572
Batch 250, Loss: 1.7568
Batch 260, Loss: 1.7568
Batch 270, Loss: 1.7569
Batch 280, Loss: 1.7574
Batch 290, Loss: 1.7568
Batch 300, Loss: 1.7567
Batch 310, Loss: 1.7571
Batch 320, Loss: 1.7573
Batch 330, Loss: 1.7570
Batch 340, Loss: 1.7571
Batch 350, Loss: 1.7572
Batch 360, Loss: 1.7570
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7572
Batch 390, Loss: 1.7571
Epoch 30 learning rate: 0.05
Epoch 30 time: 119.86134648323059 seconds
Epoch 30 accuracy: 8.68%
Batch 10, Loss: 1.7565
Batch 20, Loss: 1.7568
Batch 30, Loss: 1.7567
Batch 40, Loss: 1.7572
Batch 50, Loss: 1.7577
Batch 60, Loss: 1.7573
Batch 70, Loss: 1.7570
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7570
Batch 100, Loss: 1.7571
Batch 110, Loss: 1.7561
Batch 120, Loss: 1.7568
Batch 130, Loss: 1.7574
Batch 140, Loss: 1.7570
Batch 150, Loss: 1.7571
Batch 160, Loss: 1.7573
Batch 170, Loss: 1.7568
Batch 180, Loss: 1.7574
Batch 190, Loss: 1.7570
Batch 200, Loss: 1.7572
Batch 210, Loss: 1.7569
Batch 220, Loss: 1.7567
Batch 230, Loss: 1.7568
Batch 240, Loss: 1.7573
Batch 250, Loss: 1.7575
Batch 260, Loss: 1.7575
Batch 270, Loss: 1.7580
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7573
Batch 300, Loss: 1.7573
Batch 310, Loss: 1.7573
Batch 320, Loss: 1.7570
Batch 330, Loss: 1.7571
Batch 340, Loss: 1.7575
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7573
Batch 370, Loss: 1.7573
Batch 380, Loss: 1.7576
Batch 390, Loss: 1.7574
Epoch 31 learning rate: 0.05
Epoch 31 time: 119.699542760849 seconds
Epoch 31 accuracy: 9.71%
Batch 10, Loss: 1.7570
Batch 20, Loss: 1.7572
Batch 30, Loss: 1.7571
Batch 40, Loss: 1.7573
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7571
Batch 70, Loss: 1.7572
Batch 80, Loss: 1.7572
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7571
Batch 110, Loss: 1.7574
Batch 120, Loss: 1.7572
Batch 130, Loss: 1.7573
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7571
Batch 160, Loss: 1.7574
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7574
Batch 190, Loss: 1.7579
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7573
Batch 220, Loss: 1.7574
Batch 230, Loss: 1.7574
Batch 240, Loss: 1.7576
Batch 250, Loss: 1.7575
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7572
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7576
Batch 300, Loss: 1.7577
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7581
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7572
Batch 360, Loss: 1.7576
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7575
Epoch 32 learning rate: 0.05
Epoch 32 time: 119.83497643470764 seconds
Epoch 32 accuracy: 10.36%
Batch 10, Loss: 1.7574
Batch 20, Loss: 1.7572
Batch 30, Loss: 1.7581
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7570
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7573
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7576
Batch 130, Loss: 1.7574
Batch 140, Loss: 1.7572
Batch 150, Loss: 1.7575
Batch 160, Loss: 1.7572
Batch 170, Loss: 1.7576
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7581
Batch 200, Loss: 1.7574
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7573
Batch 230, Loss: 1.7583
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7575
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7577
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7576
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7576
Batch 390, Loss: 1.7577
Epoch 33 learning rate: 0.05
Epoch 33 time: 119.65181136131287 seconds
Epoch 33 accuracy: 9.51%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7576
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7576
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7573
Batch 130, Loss: 1.7575
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7575
Batch 190, Loss: 1.7573
Batch 200, Loss: 1.7575
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7573
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7583
Batch 260, Loss: 1.7581
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7577
Batch 340, Loss: 1.7574
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7580
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7575
Batch 390, Loss: 1.7578
Epoch 34 learning rate: 0.05
Epoch 34 time: 119.66473960876465 seconds
Epoch 34 accuracy: 9.96%
Batch 10, Loss: 1.7576
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7577
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7575
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7583
Batch 130, Loss: 1.7576
Batch 140, Loss: 1.7580
Batch 150, Loss: 1.7577
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7575
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7581
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7576
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7577
Batch 340, Loss: 1.7582
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7579
Epoch 35 learning rate: 0.05
Epoch 35 time: 119.8021469116211 seconds
Epoch 35 accuracy: 9.06%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7575
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7581
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7582
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7578
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7577
Batch 230, Loss: 1.7575
Batch 240, Loss: 1.7576
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7580
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7582
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7577
Epoch 36 learning rate: 0.05
Epoch 36 time: 119.97524118423462 seconds
Epoch 36 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7576
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7575
Batch 80, Loss: 1.7581
Batch 90, Loss: 1.7580
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7577
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7580
Batch 180, Loss: 1.7576
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7575
Batch 220, Loss: 1.7584
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7576
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7580
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7581
Batch 330, Loss: 1.7582
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7575
Batch 380, Loss: 1.7582
Batch 390, Loss: 1.7583
Epoch 37 learning rate: 0.05
Epoch 37 time: 119.89750647544861 seconds
Epoch 37 accuracy: 10.0%
Batch 10, Loss: 1.7582
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7575
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7574
Batch 90, Loss: 1.7578
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7583
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7582
Batch 160, Loss: 1.7576
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7584
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7579
Batch 240, Loss: 1.7581
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7580
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7577
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7580
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7578
Epoch 38 learning rate: 0.05
Epoch 38 time: 119.5634388923645 seconds
Epoch 38 accuracy: 5.54%
Batch 10, Loss: 1.7576
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7583
Batch 40, Loss: 1.7583
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7578
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7580
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7581
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7575
Batch 210, Loss: 1.7581
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7583
Batch 240, Loss: 1.7575
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7582
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7583
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7579
Epoch 39 learning rate: 0.05
Epoch 39 time: 119.78192329406738 seconds
Epoch 39 accuracy: 10.0%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7581
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7576
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7576
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7582
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7576
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7581
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7582
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7578
Batch 330, Loss: 1.7576
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7576
Batch 370, Loss: 1.7580
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7580
Epoch 40 learning rate: 0.05
Epoch 40 time: 119.55553460121155 seconds
Epoch 40 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7574
Batch 30, Loss: 1.7573
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7583
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7578
Batch 100, Loss: 1.7576
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7581
Batch 140, Loss: 1.7583
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7579
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7576
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7580
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7582
Epoch 41 learning rate: 0.05
Epoch 41 time: 119.65206170082092 seconds
Epoch 41 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7580
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7577
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7578
Batch 110, Loss: 1.7578
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7580
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7582
Batch 230, Loss: 1.7575
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7580
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7580
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7579
Epoch 42 learning rate: 0.05
Epoch 42 time: 119.80789613723755 seconds
Epoch 42 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7576
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7581
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7575
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7581
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7576
Batch 210, Loss: 1.7582
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7580
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7577
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7580
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7577
Epoch 43 learning rate: 0.05
Epoch 43 time: 119.6665370464325 seconds
Epoch 43 accuracy: 10.0%
Batch 10, Loss: 1.7580
Batch 20, Loss: 1.7581
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7580
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7581
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7580
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7584
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7576
Batch 330, Loss: 1.7582
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7581
Batch 390, Loss: 1.7579
Epoch 44 learning rate: 0.05
Epoch 44 time: 119.79827499389648 seconds
Epoch 44 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7574
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7576
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7582
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7580
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7582
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7582
Batch 320, Loss: 1.7580
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7580
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7580
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7579
Epoch 45 learning rate: 0.05
Epoch 45 time: 119.78455567359924 seconds
Epoch 45 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7575
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7581
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7575
Batch 100, Loss: 1.7580
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7576
Batch 160, Loss: 1.7575
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7583
Batch 200, Loss: 1.7582
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7577
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7576
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7580
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7579
Epoch 46 learning rate: 0.05
Epoch 46 time: 119.85601568222046 seconds
Epoch 46 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7580
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7575
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7581
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7576
Batch 190, Loss: 1.7582
Batch 200, Loss: 1.7581
Batch 210, Loss: 1.7581
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7573
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7575
Batch 290, Loss: 1.7586
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7585
Batch 320, Loss: 1.7580
Batch 330, Loss: 1.7580
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7582
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7580
Epoch 47 learning rate: 0.05
Epoch 47 time: 119.74156260490417 seconds
Epoch 47 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7581
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7580
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7580
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7577
Batch 160, Loss: 1.7576
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7586
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7581
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7581
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7581
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7579
Epoch 48 learning rate: 0.05
Epoch 48 time: 119.72429466247559 seconds
Epoch 48 accuracy: 10.0%
Batch 10, Loss: 1.7580
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7580
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7575
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7579
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7582
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7579
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7576
Batch 260, Loss: 1.7574
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7584
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7583
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7578
Epoch 49 learning rate: 0.05
Epoch 49 time: 119.83788275718689 seconds
Epoch 49 accuracy: 10.0%
Batch 10, Loss: 1.7576
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7581
Batch 40, Loss: 1.7575
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7582
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7575
Batch 90, Loss: 1.7581
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7580
Batch 130, Loss: 1.7581
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7578
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7580
Batch 270, Loss: 1.7574
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7580
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7579
Epoch 50 learning rate: 0.05
Epoch 50 time: 119.60921025276184 seconds
Epoch 50 accuracy: 10.0%
rho:  0.04 , alpha:  0.3
Total training time: 5995.813672542572 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.0998
Norm of the Gradient: 6.2881708145e-02
Smallest Hessian Eigenvalue: -0.0332
