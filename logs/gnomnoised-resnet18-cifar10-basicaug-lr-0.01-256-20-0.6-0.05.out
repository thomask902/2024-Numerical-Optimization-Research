The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:02:59
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 101.1631
Batch 50, Loss: 27.4731
Batch 75, Loss: 13.4617
Batch 100, Loss: 10.4816
Batch 125, Loss: 8.9225
Batch 150, Loss: 7.8775
Batch 175, Loss: 7.1327
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 302.0004720687866 seconds
Epoch 1 accuracy: 11.03%
Batch 25, Loss: 6.2280
Batch 50, Loss: 5.8265
Batch 75, Loss: 5.4950
Batch 100, Loss: 5.2192
Batch 125, Loss: 4.9901
Batch 150, Loss: 4.7918
Batch 175, Loss: 4.6167
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 262.4431879520416 seconds
Epoch 2 accuracy: 9.35%
Batch 25, Loss: 4.3658
Batch 50, Loss: 4.2367
Batch 75, Loss: 4.1193
Batch 100, Loss: 4.0119
Batch 125, Loss: 3.9143
Batch 150, Loss: 3.8250
Batch 175, Loss: 3.7429
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 262.3541212081909 seconds
Epoch 3 accuracy: 9.93%
Batch 25, Loss: 3.6189
Batch 50, Loss: 3.5522
Batch 75, Loss: 3.4902
Batch 100, Loss: 3.4322
Batch 125, Loss: 3.3778
Batch 150, Loss: 3.3268
Batch 175, Loss: 3.2790
Noise applied in 138 out of 768 batches, 17.97
Epoch 4 learning rate: 0.01
Epoch 4 time: 328.094687461853 seconds
Epoch 4 accuracy: 12.35%
Batch 25, Loss: 3.2056
Batch 50, Loss: 3.1654
Batch 75, Loss: 3.1273
Batch 100, Loss: 3.0916
Batch 125, Loss: 3.0579
Batch 150, Loss: 3.0261
Batch 175, Loss: 2.9962
Noise applied in 330 out of 960 batches, 34.38
Epoch 5 learning rate: 0.01
Epoch 5 time: 354.5914478302002 seconds
Epoch 5 accuracy: 12.43%
Batch 25, Loss: 2.9495
Batch 50, Loss: 2.9233
Batch 75, Loss: 2.8981
Batch 100, Loss: 2.8740
Batch 125, Loss: 2.8509
Batch 150, Loss: 2.8287
Batch 175, Loss: 2.8073
Noise applied in 522 out of 1152 batches, 45.31
Epoch 6 learning rate: 0.01
Epoch 6 time: 354.35044407844543 seconds
Epoch 6 accuracy: 12.56%
Batch 25, Loss: 2.7732
Batch 50, Loss: 2.7540
Batch 75, Loss: 2.7355
Batch 100, Loss: 2.7178
Batch 125, Loss: 2.7006
Batch 150, Loss: 2.6842
Batch 175, Loss: 2.6684
Noise applied in 714 out of 1344 batches, 53.12
Epoch 7 learning rate: 0.01
Epoch 7 time: 354.3083529472351 seconds
Epoch 7 accuracy: 12.72%
Batch 25, Loss: 2.6433
Batch 50, Loss: 2.6291
Batch 75, Loss: 2.6155
Batch 100, Loss: 2.6025
Batch 125, Loss: 2.5899
Batch 150, Loss: 2.5777
Batch 175, Loss: 2.5660
Noise applied in 906 out of 1536 batches, 58.98
Epoch 8 learning rate: 0.01
Epoch 8 time: 356.50396370887756 seconds
Epoch 8 accuracy: 12.9%
Batch 25, Loss: 2.5471
Batch 50, Loss: 2.5363
Batch 75, Loss: 2.5258
Batch 100, Loss: 2.5157
Batch 125, Loss: 2.5058
Batch 150, Loss: 2.4962
Batch 175, Loss: 2.4868
Noise applied in 1098 out of 1728 batches, 63.54
Epoch 9 learning rate: 0.01
Epoch 9 time: 354.14928817749023 seconds
Epoch 9 accuracy: 12.86%
Batch 25, Loss: 2.4716
Batch 50, Loss: 2.4628
Batch 75, Loss: 2.4543
Batch 100, Loss: 2.4461
Batch 125, Loss: 2.4380
Batch 150, Loss: 2.4302
Batch 175, Loss: 2.4226
Noise applied in 1290 out of 1920 batches, 67.19
Epoch 10 learning rate: 0.01
Epoch 10 time: 353.8088483810425 seconds
Epoch 10 accuracy: 12.87%
Batch 25, Loss: 2.4100
Batch 50, Loss: 2.4028
Batch 75, Loss: 2.3956
Batch 100, Loss: 2.3887
Batch 125, Loss: 2.3819
Batch 150, Loss: 2.3753
Batch 175, Loss: 2.3688
Noise applied in 1482 out of 2112 batches, 70.17
Epoch 11 learning rate: 0.01
Epoch 11 time: 354.0830509662628 seconds
Epoch 11 accuracy: 12.94%
Batch 25, Loss: 2.3581
Batch 50, Loss: 2.3520
Batch 75, Loss: 2.3460
Batch 100, Loss: 2.3400
Batch 125, Loss: 2.3342
Batch 150, Loss: 2.3286
Batch 175, Loss: 2.3230
Noise applied in 1674 out of 2304 batches, 72.66
Epoch 12 learning rate: 0.01
Epoch 12 time: 354.36535596847534 seconds
Epoch 12 accuracy: 12.94%
Batch 25, Loss: 2.3138
Batch 50, Loss: 2.3085
Batch 75, Loss: 2.3031
Batch 100, Loss: 2.2979
Batch 125, Loss: 2.2928
Batch 150, Loss: 2.2877
Batch 175, Loss: 2.2828
Noise applied in 1866 out of 2496 batches, 74.76
Epoch 13 learning rate: 0.01
Epoch 13 time: 354.50587129592896 seconds
Epoch 13 accuracy: 12.85%
Batch 25, Loss: 2.2746
Batch 50, Loss: 2.2699
Batch 75, Loss: 2.2652
Batch 100, Loss: 2.2607
Batch 125, Loss: 2.2562
Batch 150, Loss: 2.2518
Batch 175, Loss: 2.2475
Noise applied in 2058 out of 2688 batches, 76.56
Epoch 14 learning rate: 0.01
Epoch 14 time: 353.48743176460266 seconds
Epoch 14 accuracy: 12.76%
Batch 25, Loss: 2.2403
Batch 50, Loss: 2.2361
Batch 75, Loss: 2.2320
Batch 100, Loss: 2.2279
Batch 125, Loss: 2.2239
Batch 150, Loss: 2.2200
Batch 175, Loss: 2.2161
Noise applied in 2250 out of 2880 batches, 78.12
Epoch 15 learning rate: 0.01
Epoch 15 time: 381.4287121295929 seconds
Epoch 15 accuracy: 12.67%
Batch 25, Loss: 2.2097
Batch 50, Loss: 2.2059
Batch 75, Loss: 2.2022
Batch 100, Loss: 2.1985
Batch 125, Loss: 2.1949
Batch 150, Loss: 2.1913
Batch 175, Loss: 2.1878
Noise applied in 2442 out of 3072 batches, 79.49
Epoch 16 learning rate: 0.01
Epoch 16 time: 382.9662718772888 seconds
Epoch 16 accuracy: 12.56%
Batch 25, Loss: 2.1819
Batch 50, Loss: 2.1785
Batch 75, Loss: 2.1752
Batch 100, Loss: 2.1718
Batch 125, Loss: 2.1685
Batch 150, Loss: 2.1652
Batch 175, Loss: 2.1619
Noise applied in 2634 out of 3264 batches, 80.70
Epoch 17 learning rate: 0.01
Epoch 17 time: 355.83918356895447 seconds
Epoch 17 accuracy: 12.49%
Batch 25, Loss: 2.1565
Batch 50, Loss: 2.1533
Batch 75, Loss: 2.1502
Batch 100, Loss: 2.1471
Batch 125, Loss: 2.1440
Batch 150, Loss: 2.1410
Batch 175, Loss: 2.1380
Noise applied in 2826 out of 3456 batches, 81.77
Epoch 18 learning rate: 0.01
Epoch 18 time: 405.1687126159668 seconds
Epoch 18 accuracy: 12.51%
Batch 25, Loss: 2.1330
Batch 50, Loss: 2.1300
Batch 75, Loss: 2.1271
Batch 100, Loss: 2.1241
Batch 125, Loss: 2.1212
Batch 150, Loss: 2.1183
Batch 175, Loss: 2.1154
Noise applied in 3018 out of 3648 batches, 82.73
Epoch 19 learning rate: 0.01
Epoch 19 time: 355.2653167247772 seconds
Epoch 19 accuracy: 12.53%
Batch 25, Loss: 2.1106
Batch 50, Loss: 2.1078
Batch 75, Loss: 2.1050
Batch 100, Loss: 2.1022
Batch 125, Loss: 2.0994
Batch 150, Loss: 2.0966
Batch 175, Loss: 2.0939
Noise applied in 3210 out of 3840 batches, 83.59
Epoch 20 learning rate: 0.01
Epoch 20 time: 360.7904441356659 seconds
Epoch 20 accuracy: 12.64%
rho:  0.04 , alpha:  0.3
Total training time: 6940.521781206131 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.4531
Norm of the Gradient: 2.4533666670e-01
Smallest Hessian Eigenvalue: -0.0664
Noise Threshold: 0.6
Noise Radius: 0.05
