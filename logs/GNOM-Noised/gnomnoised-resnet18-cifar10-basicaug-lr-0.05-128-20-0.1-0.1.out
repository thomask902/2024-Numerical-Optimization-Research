The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.05/batchsize-128/2024-08-18-15:59:51
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 587.9911
Batch 200, Loss: 36.6843
Batch 300, Loss: 24.0330
Noise applied in 0 out of 391 batches, 0.00
Epoch 1 learning rate: 0.05
Epoch 1 time: 130.469633102417 seconds
Epoch 1 accuracy: 9.85%
Batch 100, Loss: 21.1059
Batch 200, Loss: 19.8600
Batch 300, Loss: 19.5148
Noise applied in 0 out of 782 batches, 0.00
Epoch 2 learning rate: 0.05
Epoch 2 time: 120.40366387367249 seconds
Epoch 2 accuracy: 9.77%
Batch 100, Loss: 17.8562
Batch 200, Loss: 17.2914
Batch 300, Loss: 16.6590
Noise applied in 0 out of 1173 batches, 0.00
Epoch 3 learning rate: 0.05
Epoch 3 time: 120.1972508430481 seconds
Epoch 3 accuracy: 9.92%
Batch 100, Loss: 15.3124
Batch 200, Loss: 14.4970
Batch 300, Loss: 13.5793
Noise applied in 0 out of 1564 batches, 0.00
Epoch 4 learning rate: 0.05
Epoch 4 time: 120.35209083557129 seconds
Epoch 4 accuracy: 9.94%
Batch 100, Loss: 11.7759
Batch 200, Loss: 11.1318
Batch 300, Loss: 10.8197
Noise applied in 0 out of 1955 batches, 0.00
Epoch 5 learning rate: 0.05
Epoch 5 time: 119.65394926071167 seconds
Epoch 5 accuracy: 9.77%
Batch 100, Loss: 9.9380
Batch 200, Loss: 9.6650
Batch 300, Loss: 9.3031
Noise applied in 0 out of 2346 batches, 0.00
Epoch 6 learning rate: 0.05
Epoch 6 time: 120.28593969345093 seconds
Epoch 6 accuracy: 10.13%
Batch 100, Loss: 8.4571
Batch 200, Loss: 8.1855
Batch 300, Loss: 7.7818
Noise applied in 0 out of 2737 batches, 0.00
Epoch 7 learning rate: 0.05
Epoch 7 time: 120.17128109931946 seconds
Epoch 7 accuracy: 9.91%
Batch 100, Loss: 7.0752
Batch 200, Loss: 6.9748
Batch 300, Loss: 6.4866
Noise applied in 0 out of 3128 batches, 0.00
Epoch 8 learning rate: 0.05
Epoch 8 time: 119.95763993263245 seconds
Epoch 8 accuracy: 10.57%
Batch 100, Loss: 5.9095
Batch 200, Loss: 5.7052
Batch 300, Loss: 5.3744
Noise applied in 0 out of 3519 batches, 0.00
Epoch 9 learning rate: 0.05
Epoch 9 time: 120.1649284362793 seconds
Epoch 9 accuracy: 11.1%
Batch 100, Loss: 4.9759
Batch 200, Loss: 4.7263
Batch 300, Loss: 4.5434
Noise applied in 0 out of 3910 batches, 0.00
Epoch 10 learning rate: 0.05
Epoch 10 time: 120.29835414886475 seconds
Epoch 10 accuracy: 11.41%
Batch 100, Loss: 4.2619
Batch 200, Loss: 4.0642
Batch 300, Loss: 3.9640
Noise applied in 0 out of 4301 batches, 0.00
Epoch 11 learning rate: 0.05
Epoch 11 time: 120.28676843643188 seconds
Epoch 11 accuracy: 11.65%
Batch 100, Loss: 3.7182
Batch 200, Loss: 3.6017
Batch 300, Loss: 3.4834
Noise applied in 0 out of 4692 batches, 0.00
Epoch 12 learning rate: 0.05
Epoch 12 time: 119.75238728523254 seconds
Epoch 12 accuracy: 11.95%
Batch 100, Loss: 3.2887
Batch 200, Loss: 3.2237
Batch 300, Loss: 3.1705
Noise applied in 0 out of 5083 batches, 0.00
Epoch 13 learning rate: 0.05
Epoch 13 time: 119.59153628349304 seconds
Epoch 13 accuracy: 12.23%
Batch 100, Loss: 3.0045
Batch 200, Loss: 2.9899
Batch 300, Loss: 2.8953
Noise applied in 0 out of 5474 batches, 0.00
Epoch 14 learning rate: 0.05
Epoch 14 time: 120.21541571617126 seconds
Epoch 14 accuracy: 12.54%
Batch 100, Loss: 2.7705
Batch 200, Loss: 2.7260
Batch 300, Loss: 2.7075
Noise applied in 0 out of 5865 batches, 0.00
Epoch 15 learning rate: 0.05
Epoch 15 time: 120.07461214065552 seconds
Epoch 15 accuracy: 12.68%
Batch 100, Loss: 2.6055
Batch 200, Loss: 2.5792
Batch 300, Loss: 2.5099
Noise applied in 0 out of 6256 batches, 0.00
Epoch 16 learning rate: 0.05
Epoch 16 time: 120.17694449424744 seconds
Epoch 16 accuracy: 12.63%
Batch 100, Loss: 2.4229
Batch 200, Loss: 2.4030
Batch 300, Loss: 2.3953
Noise applied in 0 out of 6647 batches, 0.00
Epoch 17 learning rate: 0.05
Epoch 17 time: 120.03912305831909 seconds
Epoch 17 accuracy: 12.57%
Batch 100, Loss: 2.3173
Batch 200, Loss: 2.3012
Batch 300, Loss: 2.2701
Noise applied in 0 out of 7038 batches, 0.00
Epoch 18 learning rate: 0.05
Epoch 18 time: 119.9822883605957 seconds
Epoch 18 accuracy: 12.47%
Batch 100, Loss: 2.2207
Batch 200, Loss: 2.1951
Batch 300, Loss: 2.2087
Noise applied in 0 out of 7429 batches, 0.00
Epoch 19 learning rate: 0.05
Epoch 19 time: 119.66716122627258 seconds
Epoch 19 accuracy: 12.11%
Batch 100, Loss: 2.1477
Batch 200, Loss: 2.1147
Batch 300, Loss: 2.1188
Noise applied in 2 out of 7820 batches, 0.03
Epoch 20 learning rate: 0.05
Epoch 20 time: 121.25675392150879 seconds
Epoch 20 accuracy: 11.7%
rho:  0.04 , alpha:  0.3
Total training time: 2413.024397611618 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.5334
Norm of the Gradient: 1.7366462946e-01
Smallest Hessian Eigenvalue: -0.0697
Noise Threshold: 0.1
Noise Radius: 0.1
