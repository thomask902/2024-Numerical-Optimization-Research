The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.1/batchsize-128/2024-08-18-15:59:51
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 14596.7991
Batch 200, Loss: 3124.9709
Batch 300, Loss: 1296.4513
Noise applied in 0 out of 391 batches, 0.00
Epoch 1 learning rate: 0.1
Epoch 1 time: 132.72590351104736 seconds
Epoch 1 accuracy: 11.72%
Batch 100, Loss: 10770.7088
Batch 200, Loss: 12127.7719
Batch 300, Loss: 1497.3811
Noise applied in 0 out of 782 batches, 0.00
Epoch 2 learning rate: 0.1
Epoch 2 time: 119.77662181854248 seconds
Epoch 2 accuracy: 11.52%
Batch 100, Loss: 597.9950
Batch 200, Loss: 451.7169
Batch 300, Loss: 403.6931
Noise applied in 0 out of 1173 batches, 0.00
Epoch 3 learning rate: 0.1
Epoch 3 time: 119.70669031143188 seconds
Epoch 3 accuracy: 11.67%
Batch 100, Loss: 284.2946
Batch 200, Loss: 195.1116
Batch 300, Loss: 212.1325
Noise applied in 0 out of 1564 batches, 0.00
Epoch 4 learning rate: 0.1
Epoch 4 time: 119.69014024734497 seconds
Epoch 4 accuracy: 11.72%
Batch 100, Loss: 175.3806
Batch 200, Loss: 155.1988
Batch 300, Loss: 141.1628
Noise applied in 0 out of 1955 batches, 0.00
Epoch 5 learning rate: 0.1
Epoch 5 time: 119.60924363136292 seconds
Epoch 5 accuracy: 12.3%
Batch 100, Loss: 87151.6612
Batch 200, Loss: 46858.4555
Batch 300, Loss: 8174.2686
Noise applied in 0 out of 2346 batches, 0.00
Epoch 6 learning rate: 0.1
Epoch 6 time: 119.62493896484375 seconds
Epoch 6 accuracy: 12.09%
Batch 100, Loss: 3181.0903
Batch 200, Loss: 2207.6557
Batch 300, Loss: 2476.5466
Noise applied in 0 out of 2737 batches, 0.00
Epoch 7 learning rate: 0.1
Epoch 7 time: 119.64537739753723 seconds
Epoch 7 accuracy: 10.78%
Batch 100, Loss: 904.9537
Batch 200, Loss: 698.9177
Batch 300, Loss: 641.2291
Noise applied in 0 out of 3128 batches, 0.00
Epoch 8 learning rate: 0.1
Epoch 8 time: 119.54645681381226 seconds
Epoch 8 accuracy: 11.09%
Batch 100, Loss: 414.5898
Batch 200, Loss: 360.9514
Batch 300, Loss: 314.5927
Noise applied in 0 out of 3519 batches, 0.00
Epoch 9 learning rate: 0.1
Epoch 9 time: 119.57179522514343 seconds
Epoch 9 accuracy: 11.42%
Batch 100, Loss: 224.7941
Batch 200, Loss: 190.3322
Batch 300, Loss: 164.8894
Noise applied in 0 out of 3910 batches, 0.00
Epoch 10 learning rate: 0.1
Epoch 10 time: 119.53023099899292 seconds
Epoch 10 accuracy: 11.12%
Batch 100, Loss: 123.7003
Batch 200, Loss: 115.2481
Batch 300, Loss: 106.9158
Noise applied in 0 out of 4301 batches, 0.00
Epoch 11 learning rate: 0.1
Epoch 11 time: 119.59219312667847 seconds
Epoch 11 accuracy: 11.04%
Batch 100, Loss: 80.4752
Batch 200, Loss: 74.6323
Batch 300, Loss: 68.5445
Noise applied in 0 out of 4692 batches, 0.00
Epoch 12 learning rate: 0.1
Epoch 12 time: 119.61111903190613 seconds
Epoch 12 accuracy: 11.34%
Batch 100, Loss: 57.0649
Batch 200, Loss: 48.4651
Batch 300, Loss: 44.7092
Noise applied in 0 out of 5083 batches, 0.00
Epoch 13 learning rate: 0.1
Epoch 13 time: 119.54916000366211 seconds
Epoch 13 accuracy: 11.33%
Batch 100, Loss: 36.8663
Batch 200, Loss: 31.9276
Batch 300, Loss: 29.8258
Noise applied in 0 out of 5474 batches, 0.00
Epoch 14 learning rate: 0.1
Epoch 14 time: 119.49563002586365 seconds
Epoch 14 accuracy: 11.93%
Batch 100, Loss: 25.3554
Batch 200, Loss: 23.3784
Batch 300, Loss: 21.3335
Noise applied in 0 out of 5865 batches, 0.00
Epoch 15 learning rate: 0.1
Epoch 15 time: 119.51955103874207 seconds
Epoch 15 accuracy: 11.79%
Batch 100, Loss: 18.4897
Batch 200, Loss: 17.0992
Batch 300, Loss: 16.0001
Noise applied in 0 out of 6256 batches, 0.00
Epoch 16 learning rate: 0.1
Epoch 16 time: 119.49642252922058 seconds
Epoch 16 accuracy: 11.63%
Batch 100, Loss: 13.4588
Batch 200, Loss: 12.9835
Batch 300, Loss: 11.7582
Noise applied in 0 out of 6647 batches, 0.00
Epoch 17 learning rate: 0.1
Epoch 17 time: 119.54431676864624 seconds
Epoch 17 accuracy: 11.75%
Batch 100, Loss: 10.1034
Batch 200, Loss: 9.4161
Batch 300, Loss: 8.3157
Noise applied in 0 out of 7038 batches, 0.00
Epoch 18 learning rate: 0.1
Epoch 18 time: 119.54102516174316 seconds
Epoch 18 accuracy: 11.69%
Batch 100, Loss: 6.5966
Batch 200, Loss: 6.1163
Batch 300, Loss: 5.5788
Noise applied in 0 out of 7429 batches, 0.00
Epoch 19 learning rate: 0.1
Epoch 19 time: 119.5494441986084 seconds
Epoch 19 accuracy: 11.54%
Batch 100, Loss: 4.6056
Batch 200, Loss: 4.2516
Batch 300, Loss: 3.8059
Noise applied in 0 out of 7820 batches, 0.00
Epoch 20 learning rate: 0.1
Epoch 20 time: 119.62550401687622 seconds
Epoch 20 accuracy: 11.59%
rho:  0.04 , alpha:  0.3
Total training time: 2404.9693727493286 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.5864
Norm of the Gradient: 3.0786061287e-01
Smallest Hessian Eigenvalue: -0.0834
Noise Threshold: 0.1
Noise Radius: 0.1
