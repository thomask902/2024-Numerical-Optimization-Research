The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.1/batchsize-128/2024-08-18-16:02:55
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 225.3650
Batch 200, Loss: 25.8015
Batch 300, Loss: 13.2452
Noise applied in 0 out of 391 batches, 0.00
Epoch 1 learning rate: 0.1
Epoch 1 time: 136.82173013687134 seconds
Epoch 1 accuracy: 11.06%
Batch 100, Loss: 10.1632
Batch 200, Loss: 9.5628
Batch 300, Loss: 8.7757
Noise applied in 0 out of 782 batches, 0.00
Epoch 2 learning rate: 0.1
Epoch 2 time: 119.74062299728394 seconds
Epoch 2 accuracy: 11.05%
Batch 100, Loss: 7.6356
Batch 200, Loss: 6.9405
Batch 300, Loss: 6.2328
Noise applied in 0 out of 1173 batches, 0.00
Epoch 3 learning rate: 0.1
Epoch 3 time: 119.65475845336914 seconds
Epoch 3 accuracy: 10.9%
Batch 100, Loss: 5.0512
Batch 200, Loss: 4.5075
Batch 300, Loss: 4.0155
Noise applied in 0 out of 1564 batches, 0.00
Epoch 4 learning rate: 0.1
Epoch 4 time: 119.63880753517151 seconds
Epoch 4 accuracy: 12.25%
Batch 100, Loss: 3.3910
Batch 200, Loss: 2.9784
Batch 300, Loss: 2.6363
Noise applied in 0 out of 1955 batches, 0.00
Epoch 5 learning rate: 0.1
Epoch 5 time: 119.640780210495 seconds
Epoch 5 accuracy: 11.27%
Batch 100, Loss: 2.2827
Batch 200, Loss: 2.1657
Batch 300, Loss: 2.0542
Noise applied in 0 out of 2346 batches, 0.00
Epoch 6 learning rate: 0.1
Epoch 6 time: 119.5737612247467 seconds
Epoch 6 accuracy: 10.91%
Batch 100, Loss: 1.8650
Batch 200, Loss: 1.7958
Batch 300, Loss: 1.7713
Noise applied in 45 out of 2737 batches, 1.64
Epoch 7 learning rate: 0.1
Epoch 7 time: 146.717675447464 seconds
Epoch 7 accuracy: 11.66%
Batch 100, Loss: 1.7609
Batch 200, Loss: 1.7555
Batch 300, Loss: 1.7547
Noise applied in 216 out of 3128 batches, 6.91
Epoch 8 learning rate: 0.1
Epoch 8 time: 217.92568922042847 seconds
Epoch 8 accuracy: 12.11%
Batch 100, Loss: 1.7523
Batch 200, Loss: 1.7533
Batch 300, Loss: 1.7536
Noise applied in 432 out of 3519 batches, 12.28
Epoch 9 learning rate: 0.1
Epoch 9 time: 241.16134333610535 seconds
Epoch 9 accuracy: 11.51%
Batch 100, Loss: 1.7504
Batch 200, Loss: 1.7519
Batch 300, Loss: 1.7524
Noise applied in 697 out of 3910 batches, 17.83
Epoch 10 learning rate: 0.1
Epoch 10 time: 264.1525149345398 seconds
Epoch 10 accuracy: 11.68%
Batch 100, Loss: 1.7506
Batch 200, Loss: 1.7511
Batch 300, Loss: 1.7507
Noise applied in 970 out of 4301 batches, 22.55
Epoch 11 learning rate: 0.1
Epoch 11 time: 268.0082154273987 seconds
Epoch 11 accuracy: 12.13%
Batch 100, Loss: 1.7509
Batch 200, Loss: 1.7508
Batch 300, Loss: 1.7519
Noise applied in 1237 out of 4692 batches, 26.36
Epoch 12 learning rate: 0.1
Epoch 12 time: 265.4749276638031 seconds
Epoch 12 accuracy: 13.22%
Batch 100, Loss: 1.7528
Batch 200, Loss: 1.7514
Batch 300, Loss: 1.7516
Noise applied in 1505 out of 5083 batches, 29.61
Epoch 13 learning rate: 0.1
Epoch 13 time: 267.40726947784424 seconds
Epoch 13 accuracy: 11.94%
Batch 100, Loss: 1.7513
Batch 200, Loss: 1.7510
Batch 300, Loss: 1.7517
Noise applied in 1769 out of 5474 batches, 32.32
Epoch 14 learning rate: 0.1
Epoch 14 time: 264.45584058761597 seconds
Epoch 14 accuracy: 12.45%
Batch 100, Loss: 1.7528
Batch 200, Loss: 1.7513
Batch 300, Loss: 1.7523
Noise applied in 2062 out of 5865 batches, 35.16
Epoch 15 learning rate: 0.1
Epoch 15 time: 276.81783151626587 seconds
Epoch 15 accuracy: 13.18%
Batch 100, Loss: 1.7512
Batch 200, Loss: 1.7534
Batch 300, Loss: 1.7523
Noise applied in 2351 out of 6256 batches, 37.58
Epoch 16 learning rate: 0.1
Epoch 16 time: 275.6451163291931 seconds
Epoch 16 accuracy: 10.98%
Batch 100, Loss: 1.7524
Batch 200, Loss: 1.7524
Batch 300, Loss: 1.7521
Noise applied in 2647 out of 6647 batches, 39.82
Epoch 17 learning rate: 0.1
Epoch 17 time: 279.00092911720276 seconds
Epoch 17 accuracy: 12.89%
Batch 100, Loss: 1.7533
Batch 200, Loss: 1.7528
Batch 300, Loss: 1.7525
Noise applied in 2949 out of 7038 batches, 41.90
Epoch 18 learning rate: 0.1
Epoch 18 time: 281.1069140434265 seconds
Epoch 18 accuracy: 11.06%
Batch 100, Loss: 1.7539
Batch 200, Loss: 1.7530
Batch 300, Loss: 1.7530
Noise applied in 3263 out of 7429 batches, 43.92
Epoch 19 learning rate: 0.1
Epoch 19 time: 286.5582423210144 seconds
Epoch 19 accuracy: 13.03%
Batch 100, Loss: 1.7538
Batch 200, Loss: 1.7539
Batch 300, Loss: 1.7542
Noise applied in 3573 out of 7820 batches, 45.69
Epoch 20 learning rate: 0.1
Epoch 20 time: 285.8359053134918 seconds
Epoch 20 accuracy: 10.21%
rho:  0.04 , alpha:  0.3
Total training time: 4355.3549888134 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.3219
Norm of the Gradient: 9.0678229928e-02
Smallest Hessian Eigenvalue: -0.1180
Noise Threshold: 0.1
Noise Radius: 0.5
