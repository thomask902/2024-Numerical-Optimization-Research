The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.05/batchsize-128/2024-08-18-16:02:55
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 4272.0320
Batch 200, Loss: 376.5064
Batch 300, Loss: 42.2130
Noise applied in 0 out of 391 batches, 0.00
Epoch 1 learning rate: 0.05
Epoch 1 time: 134.85733270645142 seconds
Epoch 1 accuracy: 11.4%
Batch 100, Loss: 18.6005
Batch 200, Loss: 15.7306
Batch 300, Loss: 14.1492
Noise applied in 0 out of 782 batches, 0.00
Epoch 2 learning rate: 0.05
Epoch 2 time: 119.79007458686829 seconds
Epoch 2 accuracy: 10.84%
Batch 100, Loss: 12.7584
Batch 200, Loss: 11.5771
Batch 300, Loss: 11.0250
Noise applied in 0 out of 1173 batches, 0.00
Epoch 3 learning rate: 0.05
Epoch 3 time: 119.74638032913208 seconds
Epoch 3 accuracy: 11.33%
Batch 100, Loss: 9.1311
Batch 200, Loss: 8.4825
Batch 300, Loss: 8.1747
Noise applied in 0 out of 1564 batches, 0.00
Epoch 4 learning rate: 0.05
Epoch 4 time: 119.70984220504761 seconds
Epoch 4 accuracy: 11.21%
Batch 100, Loss: 7.7358
Batch 200, Loss: 7.3062
Batch 300, Loss: 6.9457
Noise applied in 0 out of 1955 batches, 0.00
Epoch 5 learning rate: 0.05
Epoch 5 time: 119.71059560775757 seconds
Epoch 5 accuracy: 11.35%
Batch 100, Loss: 6.4284
Batch 200, Loss: 6.0113
Batch 300, Loss: 5.5844
Noise applied in 0 out of 2346 batches, 0.00
Epoch 6 learning rate: 0.05
Epoch 6 time: 119.6437635421753 seconds
Epoch 6 accuracy: 10.63%
Batch 100, Loss: 4.3910
Batch 200, Loss: 4.1196
Batch 300, Loss: 3.7404
Noise applied in 0 out of 2737 batches, 0.00
Epoch 7 learning rate: 0.05
Epoch 7 time: 119.6688334941864 seconds
Epoch 7 accuracy: 12.56%
Batch 100, Loss: 3.4303
Batch 200, Loss: 3.2094
Batch 300, Loss: 3.1343
Noise applied in 0 out of 3128 batches, 0.00
Epoch 8 learning rate: 0.05
Epoch 8 time: 119.64097690582275 seconds
Epoch 8 accuracy: 13.68%
Batch 100, Loss: 2.8632
Batch 200, Loss: 2.7811
Batch 300, Loss: 2.6907
Noise applied in 0 out of 3519 batches, 0.00
Epoch 9 learning rate: 0.05
Epoch 9 time: 119.66749882698059 seconds
Epoch 9 accuracy: 10.92%
Batch 100, Loss: 2.5749
Batch 200, Loss: 2.4461
Batch 300, Loss: 2.4525
Noise applied in 0 out of 3910 batches, 0.00
Epoch 10 learning rate: 0.05
Epoch 10 time: 119.63764953613281 seconds
Epoch 10 accuracy: 12.09%
Batch 100, Loss: 2.3030
Batch 200, Loss: 2.2937
Batch 300, Loss: 2.2590
Noise applied in 0 out of 4301 batches, 0.00
Epoch 11 learning rate: 0.05
Epoch 11 time: 119.6422827243805 seconds
Epoch 11 accuracy: 10.37%
Batch 100, Loss: 2.1736
Batch 200, Loss: 2.1445
Batch 300, Loss: 2.1371
Noise applied in 0 out of 4692 batches, 0.00
Epoch 12 learning rate: 0.05
Epoch 12 time: 119.5958514213562 seconds
Epoch 12 accuracy: 10.04%
Batch 100, Loss: 2.0709
Batch 200, Loss: 2.0527
Batch 300, Loss: 2.0112
Noise applied in 0 out of 5083 batches, 0.00
Epoch 13 learning rate: 0.05
Epoch 13 time: 119.59368562698364 seconds
Epoch 13 accuracy: 10.08%
Batch 100, Loss: 1.9811
Batch 200, Loss: 1.9721
Batch 300, Loss: 1.9433
Noise applied in 2 out of 5474 batches, 0.04
Epoch 14 learning rate: 0.05
Epoch 14 time: 120.86794400215149 seconds
Epoch 14 accuracy: 10.15%
Batch 100, Loss: 1.9321
Batch 200, Loss: 1.9175
Batch 300, Loss: 1.9013
Noise applied in 13 out of 5865 batches, 0.22
Epoch 15 learning rate: 0.05
Epoch 15 time: 126.24808621406555 seconds
Epoch 15 accuracy: 11.41%
Batch 100, Loss: 1.8892
Batch 200, Loss: 1.8725
Batch 300, Loss: 1.8581
Noise applied in 32 out of 6256 batches, 0.51
Epoch 16 learning rate: 0.05
Epoch 16 time: 131.21823000907898 seconds
Epoch 16 accuracy: 11.52%
Batch 100, Loss: 1.8491
Batch 200, Loss: 1.8336
Batch 300, Loss: 1.8122
Noise applied in 61 out of 6647 batches, 0.92
Epoch 17 learning rate: 0.05
Epoch 17 time: 137.31451082229614 seconds
Epoch 17 accuracy: 12.03%
Batch 100, Loss: 1.7987
Batch 200, Loss: 1.7861
Batch 300, Loss: 1.7786
Noise applied in 150 out of 7038 batches, 2.13
Epoch 18 learning rate: 0.05
Epoch 18 time: 172.5417778491974 seconds
Epoch 18 accuracy: 10.5%
Batch 100, Loss: 1.7645
Batch 200, Loss: 1.7592
Batch 300, Loss: 1.7590
Noise applied in 335 out of 7429 batches, 4.51
Epoch 19 learning rate: 0.05
Epoch 19 time: 225.03286981582642 seconds
Epoch 19 accuracy: 10.42%
Batch 100, Loss: 1.7555
Batch 200, Loss: 1.7549
Batch 300, Loss: 1.7558
Noise applied in 562 out of 7820 batches, 7.19
Epoch 20 learning rate: 0.05
Epoch 20 time: 247.85845398902893 seconds
Epoch 20 accuracy: 10.68%
rho:  0.04 , alpha:  0.3
Total training time: 2732.004336118698 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.3813
Norm of the Gradient: 7.4761480093e-02
Smallest Hessian Eigenvalue: -0.4850
Noise Threshold: 0.1
Noise Radius: 0.5
