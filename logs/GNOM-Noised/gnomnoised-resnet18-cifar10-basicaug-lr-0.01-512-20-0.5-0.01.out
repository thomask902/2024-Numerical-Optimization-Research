The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-512/2024-08-18-17:02:18
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Noise applied in 0 out of 98 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 134.04448199272156 seconds
Epoch 1 accuracy: 10.29%
Noise applied in 0 out of 196 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 106.38201832771301 seconds
Epoch 2 accuracy: 10.37%
Noise applied in 0 out of 294 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 106.36553764343262 seconds
Epoch 3 accuracy: 10.78%
Noise applied in 0 out of 392 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 106.31131196022034 seconds
Epoch 4 accuracy: 10.92%
Noise applied in 1 out of 490 batches, 0.20
Epoch 5 learning rate: 0.01
Epoch 5 time: 107.01559448242188 seconds
Epoch 5 accuracy: 11.02%
Noise applied in 27 out of 588 batches, 4.59
Epoch 6 learning rate: 0.01
Epoch 6 time: 123.77959942817688 seconds
Epoch 6 accuracy: 10.94%
Noise applied in 106 out of 686 batches, 15.45
Epoch 7 learning rate: 0.01
Epoch 7 time: 159.23328828811646 seconds
Epoch 7 accuracy: 11.28%
Noise applied in 200 out of 784 batches, 25.51
Epoch 8 learning rate: 0.01
Epoch 8 time: 169.07715678215027 seconds
Epoch 8 accuracy: 11.72%
Noise applied in 298 out of 882 batches, 33.79
Epoch 9 learning rate: 0.01
Epoch 9 time: 171.90823197364807 seconds
Epoch 9 accuracy: 11.64%
Noise applied in 396 out of 980 batches, 40.41
Epoch 10 learning rate: 0.01
Epoch 10 time: 171.82958674430847 seconds
Epoch 10 accuracy: 11.71%
Noise applied in 494 out of 1078 batches, 45.83
Epoch 11 learning rate: 0.01
Epoch 11 time: 171.8505940437317 seconds
Epoch 11 accuracy: 10.4%
Noise applied in 592 out of 1176 batches, 50.34
Epoch 12 learning rate: 0.01
Epoch 12 time: 172.17703700065613 seconds
Epoch 12 accuracy: 9.82%
Noise applied in 690 out of 1274 batches, 54.16
Epoch 13 learning rate: 0.01
Epoch 13 time: 172.20451426506042 seconds
Epoch 13 accuracy: 9.79%
Noise applied in 788 out of 1372 batches, 57.43
Epoch 14 learning rate: 0.01
Epoch 14 time: 171.85885858535767 seconds
Epoch 14 accuracy: 10.07%
Noise applied in 886 out of 1470 batches, 60.27
Epoch 15 learning rate: 0.01
Epoch 15 time: 171.97727966308594 seconds
Epoch 15 accuracy: 10.14%
Noise applied in 984 out of 1568 batches, 62.76
Epoch 16 learning rate: 0.01
Epoch 16 time: 171.81566905975342 seconds
Epoch 16 accuracy: 9.46%
Noise applied in 1082 out of 1666 batches, 64.95
Epoch 17 learning rate: 0.01
Epoch 17 time: 172.21774244308472 seconds
Epoch 17 accuracy: 9.43%
Noise applied in 1180 out of 1764 batches, 66.89
Epoch 18 learning rate: 0.01
Epoch 18 time: 171.71579122543335 seconds
Epoch 18 accuracy: 10.17%
Noise applied in 1278 out of 1862 batches, 68.64
Epoch 19 learning rate: 0.01
Epoch 19 time: 172.11566829681396 seconds
Epoch 19 accuracy: 9.57%
Noise applied in 1376 out of 1960 batches, 70.20
Epoch 20 learning rate: 0.01
Epoch 20 time: 172.2154986858368 seconds
Epoch 20 accuracy: 9.78%
rho:  0.04 , alpha:  0.3
Total training time: 3076.1117186546326 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 3.7226
Norm of the Gradient: 2.4021266401e-01
Smallest Hessian Eigenvalue: -0.0854
Noise Threshold: 0.5
Noise Radius: 0.01
/local/var/spool/slurmd/job24194564/slurm_script: line 14: --noise-radius: command not found
