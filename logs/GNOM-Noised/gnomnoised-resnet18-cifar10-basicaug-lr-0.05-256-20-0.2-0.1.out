The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.05/batchsize-256/2024-08-18-16:49:45
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 166.8234
Noise applied in 0 out of 196 batches, 0.00
Epoch 1 learning rate: 0.05
Epoch 1 time: 128.99792504310608 seconds
Epoch 1 accuracy: 10.31%
Batch 100, Loss: 15.3889
Noise applied in 0 out of 392 batches, 0.00
Epoch 2 learning rate: 0.05
Epoch 2 time: 111.65909671783447 seconds
Epoch 2 accuracy: 11.64%
Batch 100, Loss: 10.5004
Noise applied in 0 out of 588 batches, 0.00
Epoch 3 learning rate: 0.05
Epoch 3 time: 111.60718512535095 seconds
Epoch 3 accuracy: 11.75%
Batch 100, Loss: 8.2553
Noise applied in 0 out of 784 batches, 0.00
Epoch 4 learning rate: 0.05
Epoch 4 time: 111.59753155708313 seconds
Epoch 4 accuracy: 11.75%
Batch 100, Loss: 7.1169
Noise applied in 0 out of 980 batches, 0.00
Epoch 5 learning rate: 0.05
Epoch 5 time: 111.64173221588135 seconds
Epoch 5 accuracy: 11.79%
Batch 100, Loss: 6.1506
Noise applied in 0 out of 1176 batches, 0.00
Epoch 6 learning rate: 0.05
Epoch 6 time: 111.64352512359619 seconds
Epoch 6 accuracy: 12.07%
Batch 100, Loss: 5.2426
Noise applied in 0 out of 1372 batches, 0.00
Epoch 7 learning rate: 0.05
Epoch 7 time: 111.585782289505 seconds
Epoch 7 accuracy: 12.01%
Batch 100, Loss: 4.5291
Noise applied in 0 out of 1568 batches, 0.00
Epoch 8 learning rate: 0.05
Epoch 8 time: 111.54713749885559 seconds
Epoch 8 accuracy: 11.87%
Batch 100, Loss: 4.0262
Noise applied in 0 out of 1764 batches, 0.00
Epoch 9 learning rate: 0.05
Epoch 9 time: 111.55014514923096 seconds
Epoch 9 accuracy: 11.76%
Batch 100, Loss: 3.5818
Noise applied in 0 out of 1960 batches, 0.00
Epoch 10 learning rate: 0.05
Epoch 10 time: 111.6225814819336 seconds
Epoch 10 accuracy: 11.32%
Batch 100, Loss: 3.2464
Noise applied in 0 out of 2156 batches, 0.00
Epoch 11 learning rate: 0.05
Epoch 11 time: 111.51310396194458 seconds
Epoch 11 accuracy: 10.74%
Batch 100, Loss: 2.9408
Noise applied in 0 out of 2352 batches, 0.00
Epoch 12 learning rate: 0.05
Epoch 12 time: 111.56790590286255 seconds
Epoch 12 accuracy: 10.63%
Batch 100, Loss: 2.6690
Noise applied in 0 out of 2548 batches, 0.00
Epoch 13 learning rate: 0.05
Epoch 13 time: 111.64639925956726 seconds
Epoch 13 accuracy: 10.42%
Batch 100, Loss: 2.4455
Noise applied in 0 out of 2744 batches, 0.00
Epoch 14 learning rate: 0.05
Epoch 14 time: 111.55479526519775 seconds
Epoch 14 accuracy: 10.5%
Batch 100, Loss: 2.2184
Noise applied in 26 out of 2940 batches, 0.88
Epoch 15 learning rate: 0.05
Epoch 15 time: 127.85775256156921 seconds
Epoch 15 accuracy: 10.55%
Batch 100, Loss: 2.0473
Noise applied in 125 out of 3136 batches, 3.99
Epoch 16 learning rate: 0.05
Epoch 16 time: 170.8466784954071 seconds
Epoch 16 accuracy: 11.34%
Batch 100, Loss: 1.9457
Noise applied in 274 out of 3332 batches, 8.22
Epoch 17 learning rate: 0.05
Epoch 17 time: 197.17270612716675 seconds
Epoch 17 accuracy: 12.17%
Batch 100, Loss: 1.8803
Noise applied in 443 out of 3528 batches, 12.56
Epoch 18 learning rate: 0.05
Epoch 18 time: 207.71976566314697 seconds
Epoch 18 accuracy: 14.78%
Batch 100, Loss: 1.8275
Noise applied in 623 out of 3724 batches, 16.73
Epoch 19 learning rate: 0.05
Epoch 19 time: 213.6326494216919 seconds
Epoch 19 accuracy: 14.95%
Batch 100, Loss: 1.8130
Noise applied in 806 out of 3920 batches, 20.56
Epoch 20 learning rate: 0.05
Epoch 20 time: 215.13703322410583 seconds
Epoch 20 accuracy: 14.9%
rho:  0.04 , alpha:  0.3
Total training time: 2712.1179854869843 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.9358
Norm of the Gradient: 9.0541735291e-02
Smallest Hessian Eigenvalue: -0.0242
Noise Threshold: 0.2
Noise Radius: 0.1
