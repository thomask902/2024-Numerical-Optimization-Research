The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:216: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM/basicaug/lr-0.05/batchsize-128/2024-08-04-19:30:43
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 158.5983
Batch 20, Loss: 184.2693
Batch 30, Loss: 76.9567
Batch 40, Loss: 31.2335
Batch 50, Loss: 17.1552
Batch 60, Loss: 11.8749
Batch 70, Loss: 8.8462
Batch 80, Loss: 7.4202
Batch 90, Loss: 6.5702
Batch 100, Loss: 6.0862
Batch 110, Loss: 5.8882
Batch 120, Loss: 5.3442
Batch 130, Loss: 5.3269
Batch 140, Loss: 5.2512
Batch 150, Loss: 5.0422
Batch 160, Loss: 5.0590
Batch 170, Loss: 4.7826
Batch 180, Loss: 4.8439
Batch 190, Loss: 4.9438
Batch 200, Loss: 4.6172
Batch 210, Loss: 4.5227
Batch 220, Loss: 4.3825
Batch 230, Loss: 4.3211
Batch 240, Loss: 4.2767
Batch 250, Loss: 4.0687
Batch 260, Loss: 4.1033
Batch 270, Loss: 4.0157
Batch 280, Loss: 4.0317
Batch 290, Loss: 3.7185
Batch 300, Loss: 3.8767
Batch 310, Loss: 3.5704
Batch 320, Loss: 3.5005
Batch 330, Loss: 3.5768
Batch 340, Loss: 3.3869
Batch 350, Loss: 3.2619
Batch 360, Loss: 3.2662
Batch 370, Loss: 3.0649
Batch 380, Loss: 3.0969
Batch 390, Loss: 3.0786
Epoch 1 learning rate: 0.05
Epoch 1 time: 127.70490574836731 seconds
Epoch 1 accuracy: 10.17%
Batch 10, Loss: 2.9600
Batch 20, Loss: 3.0070
Batch 30, Loss: 2.8616
Batch 40, Loss: 2.9663
Batch 50, Loss: 2.7523
Batch 60, Loss: 2.8939
Batch 70, Loss: 2.7885
Batch 80, Loss: 2.9342
Batch 90, Loss: 2.8374
Batch 100, Loss: 2.7738
Batch 110, Loss: 2.6868
Batch 120, Loss: 2.7853
Batch 130, Loss: 2.6964
Batch 140, Loss: 2.7558
Batch 150, Loss: 2.6961
Batch 160, Loss: 2.6862
Batch 170, Loss: 2.6950
Batch 180, Loss: 2.6878
Batch 190, Loss: 2.6170
Batch 200, Loss: 2.6881
Batch 210, Loss: 2.5612
Batch 220, Loss: 2.6642
Batch 230, Loss: 2.5422
Batch 240, Loss: 2.5712
Batch 250, Loss: 2.6576
Batch 260, Loss: 2.6030
Batch 270, Loss: 2.5983
Batch 280, Loss: 2.5068
Batch 290, Loss: 2.4657
Batch 300, Loss: 2.5222
Batch 310, Loss: 2.4420
Batch 320, Loss: 2.4460
Batch 330, Loss: 2.4435
Batch 340, Loss: 2.4401
Batch 350, Loss: 2.3738
Batch 360, Loss: 2.4145
Batch 370, Loss: 2.3422
Batch 380, Loss: 2.4922
Batch 390, Loss: 2.3839
Epoch 2 learning rate: 0.05
Epoch 2 time: 119.0426504611969 seconds
Epoch 2 accuracy: 10.88%
Batch 10, Loss: 2.3657
Batch 20, Loss: 2.4480
Batch 30, Loss: 2.3778
Batch 40, Loss: 2.4192
Batch 50, Loss: 2.3771
Batch 60, Loss: 2.3143
Batch 70, Loss: 2.3458
Batch 80, Loss: 2.3351
Batch 90, Loss: 2.2656
Batch 100, Loss: 2.2644
Batch 110, Loss: 2.3010
Batch 120, Loss: 2.2643
Batch 130, Loss: 2.3166
Batch 140, Loss: 2.2489
Batch 150, Loss: 2.3070
Batch 160, Loss: 2.2710
Batch 170, Loss: 2.2365
Batch 180, Loss: 2.2385
Batch 190, Loss: 2.1901
Batch 200, Loss: 2.2557
Batch 210, Loss: 2.2317
Batch 220, Loss: 2.2603
Batch 230, Loss: 2.2609
Batch 240, Loss: 2.2448
Batch 250, Loss: 2.2234
Batch 260, Loss: 2.1836
Batch 270, Loss: 2.1958
Batch 280, Loss: 2.1882
Batch 290, Loss: 2.1466
Batch 300, Loss: 2.1248
Batch 310, Loss: 2.1155
Batch 320, Loss: 2.1505
Batch 330, Loss: 2.0447
Batch 340, Loss: 2.1164
Batch 350, Loss: 2.1004
Batch 360, Loss: 2.1606
Batch 370, Loss: 2.0890
Batch 380, Loss: 2.1204
Batch 390, Loss: 2.0721
Epoch 3 learning rate: 0.05
Epoch 3 time: 119.04328680038452 seconds
Epoch 3 accuracy: 11.6%
Batch 10, Loss: 2.0417
Batch 20, Loss: 2.0444
Batch 30, Loss: 2.0506
Batch 40, Loss: 2.0200
Batch 50, Loss: 2.0320
Batch 60, Loss: 2.0256
Batch 70, Loss: 2.0741
Batch 80, Loss: 2.0525
Batch 90, Loss: 1.9981
Batch 100, Loss: 2.0664
Batch 110, Loss: 2.0774
Batch 120, Loss: 2.1012
Batch 130, Loss: 2.1411
Batch 140, Loss: 2.0758
Batch 150, Loss: 2.0440
Batch 160, Loss: 2.0018
Batch 170, Loss: 2.0065
Batch 180, Loss: 1.9836
Batch 190, Loss: 2.0125
Batch 200, Loss: 1.9626
Batch 210, Loss: 1.9879
Batch 220, Loss: 2.0068
Batch 230, Loss: 1.9965
Batch 240, Loss: 2.0009
Batch 250, Loss: 2.0011
Batch 260, Loss: 1.9943
Batch 270, Loss: 2.0065
Batch 280, Loss: 1.9759
Batch 290, Loss: 2.0067
Batch 300, Loss: 1.9519
Batch 310, Loss: 2.0024
Batch 320, Loss: 1.9721
Batch 330, Loss: 1.9466
Batch 340, Loss: 1.9844
Batch 350, Loss: 1.9482
Batch 360, Loss: 1.9431
Batch 370, Loss: 1.9483
Batch 380, Loss: 1.9961
Batch 390, Loss: 1.9701
Epoch 4 learning rate: 0.05
Epoch 4 time: 118.97145080566406 seconds
Epoch 4 accuracy: 11.06%
Batch 10, Loss: 1.9802
Batch 20, Loss: 1.9417
Batch 30, Loss: 1.9540
Batch 40, Loss: 1.9578
Batch 50, Loss: 1.9132
Batch 60, Loss: 1.9408
Batch 70, Loss: 1.9507
Batch 80, Loss: 1.9676
Batch 90, Loss: 1.9133
Batch 100, Loss: 1.9060
Batch 110, Loss: 1.9663
Batch 120, Loss: 1.9211
Batch 130, Loss: 1.9384
Batch 140, Loss: 1.9515
Batch 150, Loss: 1.9274
Batch 160, Loss: 1.8855
Batch 170, Loss: 1.9005
Batch 180, Loss: 1.9024
Batch 190, Loss: 1.9649
Batch 200, Loss: 1.8971
Batch 210, Loss: 1.9094
Batch 220, Loss: 1.9170
Batch 230, Loss: 1.9113
Batch 240, Loss: 1.9022
Batch 250, Loss: 1.8991
Batch 260, Loss: 1.8857
Batch 270, Loss: 1.8854
Batch 280, Loss: 1.8607
Batch 290, Loss: 1.8873
Batch 300, Loss: 1.9362
Batch 310, Loss: 1.8734
Batch 320, Loss: 1.8788
Batch 330, Loss: 1.8766
Batch 340, Loss: 1.8891
Batch 350, Loss: 1.8783
Batch 360, Loss: 1.8973
Batch 370, Loss: 1.8893
Batch 380, Loss: 1.8756
Batch 390, Loss: 1.8621
Epoch 5 learning rate: 0.05
Epoch 5 time: 118.98401522636414 seconds
Epoch 5 accuracy: 12.49%
Batch 10, Loss: 1.8510
Batch 20, Loss: 1.8717
Batch 30, Loss: 1.8629
Batch 40, Loss: 1.8650
Batch 50, Loss: 1.9004
Batch 60, Loss: 1.8410
Batch 70, Loss: 1.8588
Batch 80, Loss: 1.8387
Batch 90, Loss: 1.8625
Batch 100, Loss: 1.8543
Batch 110, Loss: 1.8433
Batch 120, Loss: 1.8509
Batch 130, Loss: 1.8463
Batch 140, Loss: 1.8515
Batch 150, Loss: 1.8164
Batch 160, Loss: 1.8578
Batch 170, Loss: 1.8482
Batch 180, Loss: 1.8176
Batch 190, Loss: 1.8584
Batch 200, Loss: 1.8651
Batch 210, Loss: 1.8497
Batch 220, Loss: 1.8345
Batch 230, Loss: 1.8331
Batch 240, Loss: 1.8242
Batch 250, Loss: 1.8169
Batch 260, Loss: 1.8267
Batch 270, Loss: 1.8028
Batch 280, Loss: 1.8121
Batch 290, Loss: 1.8142
Batch 300, Loss: 1.8049
Batch 310, Loss: 1.7976
Batch 320, Loss: 1.8288
Batch 330, Loss: 1.8075
Batch 340, Loss: 1.8402
Batch 350, Loss: 1.8190
Batch 360, Loss: 1.7939
Batch 370, Loss: 1.8161
Batch 380, Loss: 1.8102
Batch 390, Loss: 1.8010
Epoch 6 learning rate: 0.05
Epoch 6 time: 118.92346358299255 seconds
Epoch 6 accuracy: 13.8%
Batch 10, Loss: 1.8116
Batch 20, Loss: 1.7907
Batch 30, Loss: 1.8260
Batch 40, Loss: 1.7976
Batch 50, Loss: 1.7768
Batch 60, Loss: 1.7939
Batch 70, Loss: 1.7900
Batch 80, Loss: 1.7810
Batch 90, Loss: 1.7830
Batch 100, Loss: 1.7935
Batch 110, Loss: 1.7701
Batch 120, Loss: 1.7953
Batch 130, Loss: 1.7859
Batch 140, Loss: 1.7815
Batch 150, Loss: 1.7831
Batch 160, Loss: 1.7630
Batch 170, Loss: 1.7873
Batch 180, Loss: 1.7819
Batch 190, Loss: 1.7724
Batch 200, Loss: 1.7940
Batch 210, Loss: 1.7675
Batch 220, Loss: 1.7798
Batch 230, Loss: 1.7683
Batch 240, Loss: 1.7661
Batch 250, Loss: 1.7825
Batch 260, Loss: 1.7663
Batch 270, Loss: 1.7737
Batch 280, Loss: 1.7769
Batch 290, Loss: 1.7782
Batch 300, Loss: 1.7709
Batch 310, Loss: 1.7564
Batch 320, Loss: 1.7660
Batch 330, Loss: 1.7692
Batch 340, Loss: 1.7656
Batch 350, Loss: 1.7732
Batch 360, Loss: 1.7761
Batch 370, Loss: 1.7675
Batch 380, Loss: 1.7848
Batch 390, Loss: 1.7745
Epoch 7 learning rate: 0.05
Epoch 7 time: 118.97829484939575 seconds
Epoch 7 accuracy: 14.72%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7586
Batch 30, Loss: 1.7665
Batch 40, Loss: 1.7619
Batch 50, Loss: 1.7629
Batch 60, Loss: 1.7615
Batch 70, Loss: 1.7537
Batch 80, Loss: 1.7653
Batch 90, Loss: 1.7450
Batch 100, Loss: 1.7503
Batch 110, Loss: 1.7615
Batch 120, Loss: 1.7519
Batch 130, Loss: 1.7775
Batch 140, Loss: 1.7561
Batch 150, Loss: 1.7676
Batch 160, Loss: 1.7452
Batch 170, Loss: 1.7657
Batch 180, Loss: 1.7395
Batch 190, Loss: 1.7434
Batch 200, Loss: 1.7424
Batch 210, Loss: 1.7500
Batch 220, Loss: 1.7468
Batch 230, Loss: 1.7569
Batch 240, Loss: 1.7659
Batch 250, Loss: 1.7444
Batch 260, Loss: 1.7443
Batch 270, Loss: 1.7295
Batch 280, Loss: 1.7470
Batch 290, Loss: 1.7529
Batch 300, Loss: 1.7467
Batch 310, Loss: 1.7299
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7503
Batch 340, Loss: 1.7394
Batch 350, Loss: 1.7520
Batch 360, Loss: 1.7394
Batch 370, Loss: 1.7603
Batch 380, Loss: 1.7435
Batch 390, Loss: 1.7529
Epoch 8 learning rate: 0.05
Epoch 8 time: 118.83659625053406 seconds
Epoch 8 accuracy: 14.18%
Batch 10, Loss: 1.7439
Batch 20, Loss: 1.7493
Batch 30, Loss: 1.7481
Batch 40, Loss: 1.7426
Batch 50, Loss: 1.7386
Batch 60, Loss: 1.7379
Batch 70, Loss: 1.7642
Batch 80, Loss: 1.7525
Batch 90, Loss: 1.7378
Batch 100, Loss: 1.7413
Batch 110, Loss: 1.7394
Batch 120, Loss: 1.7372
Batch 130, Loss: 1.7406
Batch 140, Loss: 1.7398
Batch 150, Loss: 1.7444
Batch 160, Loss: 1.7486
Batch 170, Loss: 1.7368
Batch 180, Loss: 1.7549
Batch 190, Loss: 1.7401
Batch 200, Loss: 1.7391
Batch 210, Loss: 1.7442
Batch 220, Loss: 1.7404
Batch 230, Loss: 1.7434
Batch 240, Loss: 1.7465
Batch 250, Loss: 1.7397
Batch 260, Loss: 1.7436
Batch 270, Loss: 1.7454
Batch 280, Loss: 1.7435
Batch 290, Loss: 1.7327
Batch 300, Loss: 1.7437
Batch 310, Loss: 1.7450
Batch 320, Loss: 1.7417
Batch 330, Loss: 1.7507
Batch 340, Loss: 1.7491
Batch 350, Loss: 1.7464
Batch 360, Loss: 1.7419
Batch 370, Loss: 1.7512
Batch 380, Loss: 1.7430
Batch 390, Loss: 1.7429
Epoch 9 learning rate: 0.05
Epoch 9 time: 118.92057991027832 seconds
Epoch 9 accuracy: 15.44%
Batch 10, Loss: 1.7372
Batch 20, Loss: 1.7288
Batch 30, Loss: 1.7365
Batch 40, Loss: 1.7429
Batch 50, Loss: 1.7373
Batch 60, Loss: 1.7451
Batch 70, Loss: 1.7478
Batch 80, Loss: 1.7430
Batch 90, Loss: 1.7371
Batch 100, Loss: 1.7469
Batch 110, Loss: 1.7434
Batch 120, Loss: 1.7485
Batch 130, Loss: 1.7438
Batch 140, Loss: 1.7294
Batch 150, Loss: 1.7417
Batch 160, Loss: 1.7405
Batch 170, Loss: 1.7382
Batch 180, Loss: 1.7508
Batch 190, Loss: 1.7372
Batch 200, Loss: 1.7401
Batch 210, Loss: 1.7444
Batch 220, Loss: 1.7354
Batch 230, Loss: 1.7411
Batch 240, Loss: 1.7437
Batch 250, Loss: 1.7401
Batch 260, Loss: 1.7357
Batch 270, Loss: 1.7266
Batch 280, Loss: 1.7410
Batch 290, Loss: 1.7478
Batch 300, Loss: 1.7441
Batch 310, Loss: 1.7485
Batch 320, Loss: 1.7376
Batch 330, Loss: 1.7438
Batch 340, Loss: 1.7387
Batch 350, Loss: 1.7450
Batch 360, Loss: 1.7414
Batch 370, Loss: 1.7489
Batch 380, Loss: 1.7373
Batch 390, Loss: 1.7339
Epoch 10 learning rate: 0.05
Epoch 10 time: 118.80493378639221 seconds
Epoch 10 accuracy: 15.4%
Batch 10, Loss: 1.7427
Batch 20, Loss: 1.7362
Batch 30, Loss: 1.7424
Batch 40, Loss: 1.7421
Batch 50, Loss: 1.7364
Batch 60, Loss: 1.7420
Batch 70, Loss: 1.7378
Batch 80, Loss: 1.7439
Batch 90, Loss: 1.7326
Batch 100, Loss: 1.7360
Batch 110, Loss: 1.7509
Batch 120, Loss: 1.7410
Batch 130, Loss: 1.7389
Batch 140, Loss: 1.7475
Batch 150, Loss: 1.7393
Batch 160, Loss: 1.7418
Batch 170, Loss: 1.7473
Batch 180, Loss: 1.7397
Batch 190, Loss: 1.7320
Batch 200, Loss: 1.7354
Batch 210, Loss: 1.7398
Batch 220, Loss: 1.7395
Batch 230, Loss: 1.7426
Batch 240, Loss: 1.7359
Batch 250, Loss: 1.7403
Batch 260, Loss: 1.7401
Batch 270, Loss: 1.7442
Batch 280, Loss: 1.7506
Batch 290, Loss: 1.7362
Batch 300, Loss: 1.7318
Batch 310, Loss: 1.7341
Batch 320, Loss: 1.7351
Batch 330, Loss: 1.7402
Batch 340, Loss: 1.7377
Batch 350, Loss: 1.7398
Batch 360, Loss: 1.7458
Batch 370, Loss: 1.7411
Batch 380, Loss: 1.7467
Batch 390, Loss: 1.7353
Epoch 11 learning rate: 0.05
Epoch 11 time: 118.85670566558838 seconds
Epoch 11 accuracy: 15.26%
Batch 10, Loss: 1.7347
Batch 20, Loss: 1.7428
Batch 30, Loss: 1.7415
Batch 40, Loss: 1.7385
Batch 50, Loss: 1.7484
Batch 60, Loss: 1.7397
Batch 70, Loss: 1.7382
Batch 80, Loss: 1.7426
Batch 90, Loss: 1.7445
Batch 100, Loss: 1.7378
Batch 110, Loss: 1.7387
Batch 120, Loss: 1.7340
Batch 130, Loss: 1.7390
Batch 140, Loss: 1.7369
Batch 150, Loss: 1.7356
Batch 160, Loss: 1.7381
Batch 170, Loss: 1.7322
Batch 180, Loss: 1.7358
Batch 190, Loss: 1.7420
Batch 200, Loss: 1.7364
Batch 210, Loss: 1.7401
Batch 220, Loss: 1.7385
Batch 230, Loss: 1.7402
Batch 240, Loss: 1.7334
Batch 250, Loss: 1.7372
Batch 260, Loss: 1.7511
Batch 270, Loss: 1.7419
Batch 280, Loss: 1.7406
Batch 290, Loss: 1.7421
Batch 300, Loss: 1.7460
Batch 310, Loss: 1.7362
Batch 320, Loss: 1.7460
Batch 330, Loss: 1.7470
Batch 340, Loss: 1.7388
Batch 350, Loss: 1.7444
Batch 360, Loss: 1.7345
Batch 370, Loss: 1.7413
Batch 380, Loss: 1.7362
Batch 390, Loss: 1.7375
Epoch 12 learning rate: 0.05
Epoch 12 time: 118.80694842338562 seconds
Epoch 12 accuracy: 15.18%
Batch 10, Loss: 1.7308
Batch 20, Loss: 1.7460
Batch 30, Loss: 1.7416
Batch 40, Loss: 1.7452
Batch 50, Loss: 1.7431
Batch 60, Loss: 1.7385
Batch 70, Loss: 1.7365
Batch 80, Loss: 1.7380
Batch 90, Loss: 1.7398
Batch 100, Loss: 1.7439
Batch 110, Loss: 1.7344
Batch 120, Loss: 1.7356
Batch 130, Loss: 1.7407
Batch 140, Loss: 1.7388
Batch 150, Loss: 1.7387
Batch 160, Loss: 1.7371
Batch 170, Loss: 1.7390
Batch 180, Loss: 1.7411
Batch 190, Loss: 1.7497
Batch 200, Loss: 1.7428
Batch 210, Loss: 1.7415
Batch 220, Loss: 1.7396
Batch 230, Loss: 1.7342
Batch 240, Loss: 1.7433
Batch 250, Loss: 1.7325
Batch 260, Loss: 1.7440
Batch 270, Loss: 1.7403
Batch 280, Loss: 1.7400
Batch 290, Loss: 1.7368
Batch 300, Loss: 1.7459
Batch 310, Loss: 1.7363
Batch 320, Loss: 1.7428
Batch 330, Loss: 1.7383
Batch 340, Loss: 1.7465
Batch 350, Loss: 1.7421
Batch 360, Loss: 1.7388
Batch 370, Loss: 1.7455
Batch 380, Loss: 1.7431
Batch 390, Loss: 1.7431
Epoch 13 learning rate: 0.05
Epoch 13 time: 118.78163933753967 seconds
Epoch 13 accuracy: 15.1%
Batch 10, Loss: 1.7429
Batch 20, Loss: 1.7415
Batch 30, Loss: 1.7402
Batch 40, Loss: 1.7436
Batch 50, Loss: 1.7445
Batch 60, Loss: 1.7392
Batch 70, Loss: 1.7441
Batch 80, Loss: 1.7401
Batch 90, Loss: 1.7417
Batch 100, Loss: 1.7471
Batch 110, Loss: 1.7404
Batch 120, Loss: 1.7358
Batch 130, Loss: 1.7424
Batch 140, Loss: 1.7511
Batch 150, Loss: 1.7399
Batch 160, Loss: 1.7482
Batch 170, Loss: 1.7422
Batch 180, Loss: 1.7373
Batch 190, Loss: 1.7416
Batch 200, Loss: 1.7426
Batch 210, Loss: 1.7381
Batch 220, Loss: 1.7469
Batch 230, Loss: 1.7471
Batch 240, Loss: 1.7423
Batch 250, Loss: 1.7457
Batch 260, Loss: 1.7425
Batch 270, Loss: 1.7408
Batch 280, Loss: 1.7417
Batch 290, Loss: 1.7362
Batch 300, Loss: 1.7393
Batch 310, Loss: 1.7424
Batch 320, Loss: 1.7384
Batch 330, Loss: 1.7438
Batch 340, Loss: 1.7428
Batch 350, Loss: 1.7422
Batch 360, Loss: 1.7410
Batch 370, Loss: 1.7475
Batch 380, Loss: 1.7394
Batch 390, Loss: 1.7411
Epoch 14 learning rate: 0.05
Epoch 14 time: 118.7302176952362 seconds
Epoch 14 accuracy: 14.41%
Batch 10, Loss: 1.7442
Batch 20, Loss: 1.7458
Batch 30, Loss: 1.7476
Batch 40, Loss: 1.7437
Batch 50, Loss: 1.7391
Batch 60, Loss: 1.7459
Batch 70, Loss: 1.7422
Batch 80, Loss: 1.7463
Batch 90, Loss: 1.7444
Batch 100, Loss: 1.7427
Batch 110, Loss: 1.7463
Batch 120, Loss: 1.7407
Batch 130, Loss: 1.7410
Batch 140, Loss: 1.7397
Batch 150, Loss: 1.7380
Batch 160, Loss: 1.7440
Batch 170, Loss: 1.7486
Batch 180, Loss: 1.7466
Batch 190, Loss: 1.7451
Batch 200, Loss: 1.7430
Batch 210, Loss: 1.7493
Batch 220, Loss: 1.7419
Batch 230, Loss: 1.7427
Batch 240, Loss: 1.7409
Batch 250, Loss: 1.7424
Batch 260, Loss: 1.7397
Batch 270, Loss: 1.7388
Batch 280, Loss: 1.7429
Batch 290, Loss: 1.7453
Batch 300, Loss: 1.7438
Batch 310, Loss: 1.7452
Batch 320, Loss: 1.7435
Batch 330, Loss: 1.7453
Batch 340, Loss: 1.7394
Batch 350, Loss: 1.7405
Batch 360, Loss: 1.7465
Batch 370, Loss: 1.7453
Batch 380, Loss: 1.7423
Batch 390, Loss: 1.7470
Epoch 15 learning rate: 0.05
Epoch 15 time: 118.75907349586487 seconds
Epoch 15 accuracy: 15.11%
Batch 10, Loss: 1.7410
Batch 20, Loss: 1.7437
Batch 30, Loss: 1.7441
Batch 40, Loss: 1.7403
Batch 50, Loss: 1.7445
Batch 60, Loss: 1.7416
Batch 70, Loss: 1.7443
Batch 80, Loss: 1.7496
Batch 90, Loss: 1.7398
Batch 100, Loss: 1.7446
Batch 110, Loss: 1.7459
Batch 120, Loss: 1.7432
Batch 130, Loss: 1.7447
Batch 140, Loss: 1.7428
Batch 150, Loss: 1.7501
Batch 160, Loss: 1.7422
Batch 170, Loss: 1.7429
Batch 180, Loss: 1.7459
Batch 190, Loss: 1.7435
Batch 200, Loss: 1.7450
Batch 210, Loss: 1.7405
Batch 220, Loss: 1.7491
Batch 230, Loss: 1.7397
Batch 240, Loss: 1.7440
Batch 250, Loss: 1.7486
Batch 260, Loss: 1.7463
Batch 270, Loss: 1.7496
Batch 280, Loss: 1.7484
Batch 290, Loss: 1.7497
Batch 300, Loss: 1.7467
Batch 310, Loss: 1.7458
Batch 320, Loss: 1.7489
Batch 330, Loss: 1.7433
Batch 340, Loss: 1.7425
Batch 350, Loss: 1.7429
Batch 360, Loss: 1.7494
Batch 370, Loss: 1.7453
Batch 380, Loss: 1.7488
Batch 390, Loss: 1.7465
Epoch 16 learning rate: 0.05
Epoch 16 time: 118.82347559928894 seconds
Epoch 16 accuracy: 15.29%
Batch 10, Loss: 1.7489
Batch 20, Loss: 1.7472
Batch 30, Loss: 1.7478
Batch 40, Loss: 1.7495
Batch 50, Loss: 1.7450
Batch 60, Loss: 1.7453
Batch 70, Loss: 1.7447
Batch 80, Loss: 1.7483
Batch 90, Loss: 1.7438
Batch 100, Loss: 1.7455
Batch 110, Loss: 1.7458
Batch 120, Loss: 1.7473
Batch 130, Loss: 1.7446
Batch 140, Loss: 1.7488
Batch 150, Loss: 1.7481
Batch 160, Loss: 1.7473
Batch 170, Loss: 1.7476
Batch 180, Loss: 1.7468
Batch 190, Loss: 1.7497
Batch 200, Loss: 1.7454
Batch 210, Loss: 1.7473
Batch 220, Loss: 1.7430
Batch 230, Loss: 1.7452
Batch 240, Loss: 1.7473
Batch 250, Loss: 1.7473
Batch 260, Loss: 1.7484
Batch 270, Loss: 1.7481
Batch 280, Loss: 1.7449
Batch 290, Loss: 1.7457
Batch 300, Loss: 1.7472
Batch 310, Loss: 1.7485
Batch 320, Loss: 1.7460
Batch 330, Loss: 1.7504
Batch 340, Loss: 1.7489
Batch 350, Loss: 1.7480
Batch 360, Loss: 1.7495
Batch 370, Loss: 1.7455
Batch 380, Loss: 1.7483
Batch 390, Loss: 1.7465
Epoch 17 learning rate: 0.05
Epoch 17 time: 118.81664443016052 seconds
Epoch 17 accuracy: 14.8%
Batch 10, Loss: 1.7483
Batch 20, Loss: 1.7484
Batch 30, Loss: 1.7478
Batch 40, Loss: 1.7471
Batch 50, Loss: 1.7469
Batch 60, Loss: 1.7512
Batch 70, Loss: 1.7474
Batch 80, Loss: 1.7486
Batch 90, Loss: 1.7494
Batch 100, Loss: 1.7486
Batch 110, Loss: 1.7472
Batch 120, Loss: 1.7444
Batch 130, Loss: 1.7477
Batch 140, Loss: 1.7483
Batch 150, Loss: 1.7494
Batch 160, Loss: 1.7463
Batch 170, Loss: 1.7471
Batch 180, Loss: 1.7473
Batch 190, Loss: 1.7504
Batch 200, Loss: 1.7484
Batch 210, Loss: 1.7510
Batch 220, Loss: 1.7497
Batch 230, Loss: 1.7498
Batch 240, Loss: 1.7478
Batch 250, Loss: 1.7503
Batch 260, Loss: 1.7492
Batch 270, Loss: 1.7517
Batch 280, Loss: 1.7482
Batch 290, Loss: 1.7495
Batch 300, Loss: 1.7518
Batch 310, Loss: 1.7506
Batch 320, Loss: 1.7494
Batch 330, Loss: 1.7515
Batch 340, Loss: 1.7490
Batch 350, Loss: 1.7499
Batch 360, Loss: 1.7483
Batch 370, Loss: 1.7496
Batch 380, Loss: 1.7497
Batch 390, Loss: 1.7494
Epoch 18 learning rate: 0.05
Epoch 18 time: 118.79024934768677 seconds
Epoch 18 accuracy: 14.93%
Batch 10, Loss: 1.7493
Batch 20, Loss: 1.7504
Batch 30, Loss: 1.7489
Batch 40, Loss: 1.7486
Batch 50, Loss: 1.7501
Batch 60, Loss: 1.7513
Batch 70, Loss: 1.7499
Batch 80, Loss: 1.7499
Batch 90, Loss: 1.7500
Batch 100, Loss: 1.7502
Batch 110, Loss: 1.7474
Batch 120, Loss: 1.7493
Batch 130, Loss: 1.7485
Batch 140, Loss: 1.7506
Batch 150, Loss: 1.7485
Batch 160, Loss: 1.7490
Batch 170, Loss: 1.7507
Batch 180, Loss: 1.7479
Batch 190, Loss: 1.7484
Batch 200, Loss: 1.7515
Batch 210, Loss: 1.7506
Batch 220, Loss: 1.7492
Batch 230, Loss: 1.7497
Batch 240, Loss: 1.7501
Batch 250, Loss: 1.7535
Batch 260, Loss: 1.7492
Batch 270, Loss: 1.7516
Batch 280, Loss: 1.7494
Batch 290, Loss: 1.7525
Batch 300, Loss: 1.7516
Batch 310, Loss: 1.7521
Batch 320, Loss: 1.7521
Batch 330, Loss: 1.7522
Batch 340, Loss: 1.7500
Batch 350, Loss: 1.7522
Batch 360, Loss: 1.7505
Batch 370, Loss: 1.7517
Batch 380, Loss: 1.7493
Batch 390, Loss: 1.7515
Epoch 19 learning rate: 0.05
Epoch 19 time: 118.73643279075623 seconds
Epoch 19 accuracy: 14.82%
Batch 10, Loss: 1.7513
Batch 20, Loss: 1.7514
Batch 30, Loss: 1.7513
Batch 40, Loss: 1.7500
Batch 50, Loss: 1.7516
Batch 60, Loss: 1.7517
Batch 70, Loss: 1.7494
Batch 80, Loss: 1.7534
Batch 90, Loss: 1.7497
Batch 100, Loss: 1.7530
Batch 110, Loss: 1.7509
Batch 120, Loss: 1.7511
Batch 130, Loss: 1.7508
Batch 140, Loss: 1.7501
Batch 150, Loss: 1.7527
Batch 160, Loss: 1.7529
Batch 170, Loss: 1.7523
Batch 180, Loss: 1.7538
Batch 190, Loss: 1.7521
Batch 200, Loss: 1.7541
Batch 210, Loss: 1.7515
Batch 220, Loss: 1.7528
Batch 230, Loss: 1.7533
Batch 240, Loss: 1.7516
Batch 250, Loss: 1.7510
Batch 260, Loss: 1.7516
Batch 270, Loss: 1.7501
Batch 280, Loss: 1.7517
Batch 290, Loss: 1.7509
Batch 300, Loss: 1.7528
Batch 310, Loss: 1.7532
Batch 320, Loss: 1.7500
Batch 330, Loss: 1.7527
Batch 340, Loss: 1.7535
Batch 350, Loss: 1.7517
Batch 360, Loss: 1.7521
Batch 370, Loss: 1.7520
Batch 380, Loss: 1.7533
Batch 390, Loss: 1.7536
Epoch 20 learning rate: 0.05
Epoch 20 time: 118.76680874824524 seconds
Epoch 20 accuracy: 14.63%
Batch 10, Loss: 1.7546
Batch 20, Loss: 1.7543
Batch 30, Loss: 1.7512
Batch 40, Loss: 1.7536
Batch 50, Loss: 1.7533
Batch 60, Loss: 1.7517
Batch 70, Loss: 1.7531
Batch 80, Loss: 1.7529
Batch 90, Loss: 1.7541
Batch 100, Loss: 1.7514
Batch 110, Loss: 1.7527
Batch 120, Loss: 1.7535
Batch 130, Loss: 1.7522
Batch 140, Loss: 1.7529
Batch 150, Loss: 1.7511
Batch 160, Loss: 1.7526
Batch 170, Loss: 1.7537
Batch 180, Loss: 1.7533
Batch 190, Loss: 1.7515
Batch 200, Loss: 1.7516
Batch 210, Loss: 1.7541
Batch 220, Loss: 1.7542
Batch 230, Loss: 1.7551
Batch 240, Loss: 1.7539
Batch 250, Loss: 1.7551
Batch 260, Loss: 1.7537
Batch 270, Loss: 1.7533
Batch 280, Loss: 1.7534
Batch 290, Loss: 1.7545
Batch 300, Loss: 1.7534
Batch 310, Loss: 1.7532
Batch 320, Loss: 1.7549
Batch 330, Loss: 1.7542
Batch 340, Loss: 1.7541
Batch 350, Loss: 1.7545
Batch 360, Loss: 1.7538
Batch 370, Loss: 1.7533
Batch 380, Loss: 1.7531
Batch 390, Loss: 1.7546
Epoch 21 learning rate: 0.05
Epoch 21 time: 118.79909873008728 seconds
Epoch 21 accuracy: 13.86%
Batch 10, Loss: 1.7543
Batch 20, Loss: 1.7537
Batch 30, Loss: 1.7547
Batch 40, Loss: 1.7535
Batch 50, Loss: 1.7540
Batch 60, Loss: 1.7536
Batch 70, Loss: 1.7533
Batch 80, Loss: 1.7547
Batch 90, Loss: 1.7552
Batch 100, Loss: 1.7530
Batch 110, Loss: 1.7538
Batch 120, Loss: 1.7555
Batch 130, Loss: 1.7550
Batch 140, Loss: 1.7548
Batch 150, Loss: 1.7534
Batch 160, Loss: 1.7541
Batch 170, Loss: 1.7538
Batch 180, Loss: 1.7535
Batch 190, Loss: 1.7539
Batch 200, Loss: 1.7532
Batch 210, Loss: 1.7545
Batch 220, Loss: 1.7546
Batch 230, Loss: 1.7539
Batch 240, Loss: 1.7545
Batch 250, Loss: 1.7549
Batch 260, Loss: 1.7544
Batch 270, Loss: 1.7542
Batch 280, Loss: 1.7551
Batch 290, Loss: 1.7556
Batch 300, Loss: 1.7539
Batch 310, Loss: 1.7547
Batch 320, Loss: 1.7553
Batch 330, Loss: 1.7551
Batch 340, Loss: 1.7556
Batch 350, Loss: 1.7546
Batch 360, Loss: 1.7549
Batch 370, Loss: 1.7544
Batch 380, Loss: 1.7555
Batch 390, Loss: 1.7557
Epoch 22 learning rate: 0.05
Epoch 22 time: 118.74187874794006 seconds
Epoch 22 accuracy: 12.53%
Batch 10, Loss: 1.7544
Batch 20, Loss: 1.7542
Batch 30, Loss: 1.7550
Batch 40, Loss: 1.7546
Batch 50, Loss: 1.7550
Batch 60, Loss: 1.7544
Batch 70, Loss: 1.7561
Batch 80, Loss: 1.7555
Batch 90, Loss: 1.7546
Batch 100, Loss: 1.7550
Batch 110, Loss: 1.7556
Batch 120, Loss: 1.7569
Batch 130, Loss: 1.7547
Batch 140, Loss: 1.7551
Batch 150, Loss: 1.7563
Batch 160, Loss: 1.7550
Batch 170, Loss: 1.7555
Batch 180, Loss: 1.7548
Batch 190, Loss: 1.7555
Batch 200, Loss: 1.7554
Batch 210, Loss: 1.7555
Batch 220, Loss: 1.7548
Batch 230, Loss: 1.7554
Batch 240, Loss: 1.7547
Batch 250, Loss: 1.7555
Batch 260, Loss: 1.7552
Batch 270, Loss: 1.7553
Batch 280, Loss: 1.7546
Batch 290, Loss: 1.7552
Batch 300, Loss: 1.7553
Batch 310, Loss: 1.7555
Batch 320, Loss: 1.7560
Batch 330, Loss: 1.7557
Batch 340, Loss: 1.7549
Batch 350, Loss: 1.7560
Batch 360, Loss: 1.7553
Batch 370, Loss: 1.7555
Batch 380, Loss: 1.7560
Batch 390, Loss: 1.7570
Epoch 23 learning rate: 0.05
Epoch 23 time: 118.85241413116455 seconds
Epoch 23 accuracy: 14.76%
Batch 10, Loss: 1.7560
Batch 20, Loss: 1.7554
Batch 30, Loss: 1.7560
Batch 40, Loss: 1.7553
Batch 50, Loss: 1.7556
Batch 60, Loss: 1.7559
Batch 70, Loss: 1.7566
Batch 80, Loss: 1.7552
Batch 90, Loss: 1.7556
Batch 100, Loss: 1.7562
Batch 110, Loss: 1.7558
Batch 120, Loss: 1.7559
Batch 130, Loss: 1.7555
Batch 140, Loss: 1.7552
Batch 150, Loss: 1.7561
Batch 160, Loss: 1.7559
Batch 170, Loss: 1.7558
Batch 180, Loss: 1.7557
Batch 190, Loss: 1.7560
Batch 200, Loss: 1.7564
Batch 210, Loss: 1.7561
Batch 220, Loss: 1.7557
Batch 230, Loss: 1.7563
Batch 240, Loss: 1.7558
Batch 250, Loss: 1.7559
Batch 260, Loss: 1.7565
Batch 270, Loss: 1.7562
Batch 280, Loss: 1.7559
Batch 290, Loss: 1.7554
Batch 300, Loss: 1.7563
Batch 310, Loss: 1.7566
Batch 320, Loss: 1.7564
Batch 330, Loss: 1.7563
Batch 340, Loss: 1.7560
Batch 350, Loss: 1.7567
Batch 360, Loss: 1.7559
Batch 370, Loss: 1.7564
Batch 380, Loss: 1.7553
Batch 390, Loss: 1.7568
Epoch 24 learning rate: 0.05
Epoch 24 time: 118.83736300468445 seconds
Epoch 24 accuracy: 12.2%
Batch 10, Loss: 1.7561
Batch 20, Loss: 1.7564
Batch 30, Loss: 1.7567
Batch 40, Loss: 1.7563
Batch 50, Loss: 1.7560
Batch 60, Loss: 1.7570
Batch 70, Loss: 1.7559
Batch 80, Loss: 1.7563
Batch 90, Loss: 1.7567
Batch 100, Loss: 1.7565
Batch 110, Loss: 1.7564
Batch 120, Loss: 1.7561
Batch 130, Loss: 1.7558
Batch 140, Loss: 1.7557
Batch 150, Loss: 1.7565
Batch 160, Loss: 1.7565
Batch 170, Loss: 1.7565
Batch 180, Loss: 1.7555
Batch 190, Loss: 1.7561
Batch 200, Loss: 1.7572
Batch 210, Loss: 1.7564
Batch 220, Loss: 1.7558
Batch 230, Loss: 1.7566
Batch 240, Loss: 1.7567
Batch 250, Loss: 1.7568
Batch 260, Loss: 1.7567
Batch 270, Loss: 1.7565
Batch 280, Loss: 1.7565
Batch 290, Loss: 1.7568
Batch 300, Loss: 1.7569
Batch 310, Loss: 1.7573
Batch 320, Loss: 1.7560
Batch 330, Loss: 1.7567
Batch 340, Loss: 1.7571
Batch 350, Loss: 1.7563
Batch 360, Loss: 1.7565
Batch 370, Loss: 1.7564
Batch 380, Loss: 1.7573
Batch 390, Loss: 1.7569
Epoch 25 learning rate: 0.05
Epoch 25 time: 118.85388660430908 seconds
Epoch 25 accuracy: 12.62%
Batch 10, Loss: 1.7562
Batch 20, Loss: 1.7568
Batch 30, Loss: 1.7565
Batch 40, Loss: 1.7568
Batch 50, Loss: 1.7565
Batch 60, Loss: 1.7566
Batch 70, Loss: 1.7563
Batch 80, Loss: 1.7567
Batch 90, Loss: 1.7569
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7570
Batch 120, Loss: 1.7569
Batch 130, Loss: 1.7569
Batch 140, Loss: 1.7569
Batch 150, Loss: 1.7568
Batch 160, Loss: 1.7568
Batch 170, Loss: 1.7568
Batch 180, Loss: 1.7571
Batch 190, Loss: 1.7569
Batch 200, Loss: 1.7572
Batch 210, Loss: 1.7567
Batch 220, Loss: 1.7569
Batch 230, Loss: 1.7567
Batch 240, Loss: 1.7570
Batch 250, Loss: 1.7570
Batch 260, Loss: 1.7565
Batch 270, Loss: 1.7567
Batch 280, Loss: 1.7573
Batch 290, Loss: 1.7568
Batch 300, Loss: 1.7571
Batch 310, Loss: 1.7569
Batch 320, Loss: 1.7568
Batch 330, Loss: 1.7569
Batch 340, Loss: 1.7571
Batch 350, Loss: 1.7567
Batch 360, Loss: 1.7571
Batch 370, Loss: 1.7568
Batch 380, Loss: 1.7569
Batch 390, Loss: 1.7565
Epoch 26 learning rate: 0.05
Epoch 26 time: 118.82528829574585 seconds
Epoch 26 accuracy: 13.55%
Batch 10, Loss: 1.7570
Batch 20, Loss: 1.7572
Batch 30, Loss: 1.7569
Batch 40, Loss: 1.7564
Batch 50, Loss: 1.7569
Batch 60, Loss: 1.7575
Batch 70, Loss: 1.7571
Batch 80, Loss: 1.7569
Batch 90, Loss: 1.7571
Batch 100, Loss: 1.7569
Batch 110, Loss: 1.7574
Batch 120, Loss: 1.7571
Batch 130, Loss: 1.7571
Batch 140, Loss: 1.7572
Batch 150, Loss: 1.7566
Batch 160, Loss: 1.7571
Batch 170, Loss: 1.7571
Batch 180, Loss: 1.7570
Batch 190, Loss: 1.7568
Batch 200, Loss: 1.7569
Batch 210, Loss: 1.7572
Batch 220, Loss: 1.7574
Batch 230, Loss: 1.7571
Batch 240, Loss: 1.7573
Batch 250, Loss: 1.7571
Batch 260, Loss: 1.7568
Batch 270, Loss: 1.7573
Batch 280, Loss: 1.7573
Batch 290, Loss: 1.7577
Batch 300, Loss: 1.7576
Batch 310, Loss: 1.7574
Batch 320, Loss: 1.7572
Batch 330, Loss: 1.7575
Batch 340, Loss: 1.7572
Batch 350, Loss: 1.7569
Batch 360, Loss: 1.7573
Batch 370, Loss: 1.7569
Batch 380, Loss: 1.7571
Batch 390, Loss: 1.7572
Epoch 27 learning rate: 0.05
Epoch 27 time: 118.82102704048157 seconds
Epoch 27 accuracy: 14.73%
Batch 10, Loss: 1.7570
Batch 20, Loss: 1.7573
Batch 30, Loss: 1.7573
Batch 40, Loss: 1.7566
Batch 50, Loss: 1.7571
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7570
Batch 80, Loss: 1.7571
Batch 90, Loss: 1.7568
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7570
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7574
Batch 150, Loss: 1.7573
Batch 160, Loss: 1.7575
Batch 170, Loss: 1.7572
Batch 180, Loss: 1.7570
Batch 190, Loss: 1.7574
Batch 200, Loss: 1.7575
Batch 210, Loss: 1.7574
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7575
Batch 240, Loss: 1.7573
Batch 250, Loss: 1.7572
Batch 260, Loss: 1.7575
Batch 270, Loss: 1.7575
Batch 280, Loss: 1.7575
Batch 290, Loss: 1.7575
Batch 300, Loss: 1.7575
Batch 310, Loss: 1.7573
Batch 320, Loss: 1.7573
Batch 330, Loss: 1.7574
Batch 340, Loss: 1.7574
Batch 350, Loss: 1.7575
Batch 360, Loss: 1.7573
Batch 370, Loss: 1.7571
Batch 380, Loss: 1.7575
Batch 390, Loss: 1.7573
Epoch 28 learning rate: 0.05
Epoch 28 time: 118.89088988304138 seconds
Epoch 28 accuracy: 14.25%
Batch 10, Loss: 1.7570
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7574
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7575
Batch 60, Loss: 1.7572
Batch 70, Loss: 1.7574
Batch 80, Loss: 1.7572
Batch 90, Loss: 1.7572
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7575
Batch 120, Loss: 1.7575
Batch 130, Loss: 1.7572
Batch 140, Loss: 1.7576
Batch 150, Loss: 1.7574
Batch 160, Loss: 1.7570
Batch 170, Loss: 1.7576
Batch 180, Loss: 1.7576
Batch 190, Loss: 1.7573
Batch 200, Loss: 1.7576
Batch 210, Loss: 1.7576
Batch 220, Loss: 1.7576
Batch 230, Loss: 1.7574
Batch 240, Loss: 1.7576
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7574
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7573
Batch 300, Loss: 1.7577
Batch 310, Loss: 1.7574
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7575
Batch 340, Loss: 1.7573
Batch 350, Loss: 1.7576
Batch 360, Loss: 1.7575
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7575
Batch 390, Loss: 1.7578
Epoch 29 learning rate: 0.05
Epoch 29 time: 118.84044075012207 seconds
Epoch 29 accuracy: 11.94%
Batch 10, Loss: 1.7574
Batch 20, Loss: 1.7575
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7575
Batch 50, Loss: 1.7573
Batch 60, Loss: 1.7573
Batch 70, Loss: 1.7574
Batch 80, Loss: 1.7574
Batch 90, Loss: 1.7575
Batch 100, Loss: 1.7575
Batch 110, Loss: 1.7573
Batch 120, Loss: 1.7573
Batch 130, Loss: 1.7576
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7575
Batch 170, Loss: 1.7576
Batch 180, Loss: 1.7574
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7573
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7576
Batch 230, Loss: 1.7575
Batch 240, Loss: 1.7570
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7575
Batch 310, Loss: 1.7577
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7572
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7574
Batch 390, Loss: 1.7576
Epoch 30 learning rate: 0.05
Epoch 30 time: 118.86008143424988 seconds
Epoch 30 accuracy: 13.47%
Batch 10, Loss: 1.7576
Batch 20, Loss: 1.7576
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7576
Batch 60, Loss: 1.7574
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7572
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7580
Batch 130, Loss: 1.7576
Batch 140, Loss: 1.7574
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7574
Batch 170, Loss: 1.7576
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7579
Batch 200, Loss: 1.7574
Batch 210, Loss: 1.7576
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7574
Batch 240, Loss: 1.7577
Batch 250, Loss: 1.7576
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7574
Batch 330, Loss: 1.7577
Batch 340, Loss: 1.7576
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7577
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7579
Epoch 31 learning rate: 0.05
Epoch 31 time: 118.82530212402344 seconds
Epoch 31 accuracy: 10.7%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7576
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7577
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7576
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7576
Batch 110, Loss: 1.7575
Batch 120, Loss: 1.7575
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7575
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7577
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7574
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7577
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7576
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7576
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7577
Epoch 32 learning rate: 0.05
Epoch 32 time: 118.8804190158844 seconds
Epoch 32 accuracy: 10.11%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7575
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7580
Batch 100, Loss: 1.7576
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7576
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7576
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7577
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7579
Epoch 33 learning rate: 0.05
Epoch 33 time: 118.90016222000122 seconds
Epoch 33 accuracy: 9.8%
Batch 10, Loss: 1.7575
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7575
Batch 50, Loss: 1.7577
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7580
Batch 100, Loss: 1.7576
Batch 110, Loss: 1.7575
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7575
Batch 160, Loss: 1.7580
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7575
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7577
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7582
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7576
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7576
Batch 380, Loss: 1.7581
Batch 390, Loss: 1.7580
Epoch 34 learning rate: 0.05
Epoch 34 time: 118.84562826156616 seconds
Epoch 34 accuracy: 10.12%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7578
Batch 100, Loss: 1.7578
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7577
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7577
Batch 160, Loss: 1.7573
Batch 170, Loss: 1.7575
Batch 180, Loss: 1.7582
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7583
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7583
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7577
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7580
Batch 370, Loss: 1.7576
Batch 380, Loss: 1.7576
Batch 390, Loss: 1.7580
Epoch 35 learning rate: 0.05
Epoch 35 time: 118.86878371238708 seconds
Epoch 35 accuracy: 9.7%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7580
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7581
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7578
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7575
Batch 140, Loss: 1.7580
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7576
Batch 180, Loss: 1.7576
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7581
Batch 220, Loss: 1.7584
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7578
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7580
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7577
Epoch 36 learning rate: 0.05
Epoch 36 time: 118.80895924568176 seconds
Epoch 36 accuracy: 8.02%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7574
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7583
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7581
Batch 100, Loss: 1.7575
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7577
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7582
Batch 200, Loss: 1.7581
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7576
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7581
Batch 390, Loss: 1.7578
Epoch 37 learning rate: 0.05
Epoch 37 time: 118.84128379821777 seconds
Epoch 37 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7573
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7575
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7580
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7577
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7583
Batch 160, Loss: 1.7580
Batch 170, Loss: 1.7576
Batch 180, Loss: 1.7575
Batch 190, Loss: 1.7582
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7574
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7574
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7583
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7582
Batch 300, Loss: 1.7583
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7576
Batch 330, Loss: 1.7583
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7581
Batch 390, Loss: 1.7578
Epoch 38 learning rate: 0.05
Epoch 38 time: 118.7968065738678 seconds
Epoch 38 accuracy: 9.53%
Batch 10, Loss: 1.7580
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7577
Batch 60, Loss: 1.7581
Batch 70, Loss: 1.7576
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7583
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7575
Batch 130, Loss: 1.7582
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7580
Batch 180, Loss: 1.7576
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7575
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7576
Batch 310, Loss: 1.7582
Batch 320, Loss: 1.7578
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7579
Epoch 39 learning rate: 0.05
Epoch 39 time: 118.78554058074951 seconds
Epoch 39 accuracy: 10.0%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7581
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7582
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7574
Batch 80, Loss: 1.7576
Batch 90, Loss: 1.7581
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7574
Batch 130, Loss: 1.7582
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7580
Batch 170, Loss: 1.7583
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7577
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7578
Epoch 40 learning rate: 0.05
Epoch 40 time: 118.78015279769897 seconds
Epoch 40 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7574
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7581
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7581
Batch 100, Loss: 1.7580
Batch 110, Loss: 1.7578
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7581
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7576
Batch 170, Loss: 1.7575
Batch 180, Loss: 1.7582
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7577
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7579
Batch 260, Loss: 1.7575
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7576
Batch 300, Loss: 1.7577
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7582
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7578
Epoch 41 learning rate: 0.05
Epoch 41 time: 118.78259563446045 seconds
Epoch 41 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7580
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7580
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7582
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7581
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7583
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7579
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7583
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7579
Epoch 42 learning rate: 0.05
Epoch 42 time: 118.77902317047119 seconds
Epoch 42 accuracy: 10.0%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7576
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7582
Batch 130, Loss: 1.7575
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7573
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7582
Batch 220, Loss: 1.7576
Batch 230, Loss: 1.7579
Batch 240, Loss: 1.7581
Batch 250, Loss: 1.7576
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7580
Batch 280, Loss: 1.7582
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7584
Batch 320, Loss: 1.7581
Batch 330, Loss: 1.7581
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7580
Epoch 43 learning rate: 0.05
Epoch 43 time: 118.79766345024109 seconds
Epoch 43 accuracy: 10.0%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7579
Batch 30, Loss: 1.7580
Batch 40, Loss: 1.7581
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7576
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7575
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7573
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7581
Batch 160, Loss: 1.7582
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7581
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7578
Batch 230, Loss: 1.7582
Batch 240, Loss: 1.7582
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7577
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7581
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7580
Batch 370, Loss: 1.7582
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7577
Epoch 44 learning rate: 0.05
Epoch 44 time: 118.75552082061768 seconds
Epoch 44 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7579
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7576
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7580
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7578
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7579
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7580
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7576
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7581
Batch 330, Loss: 1.7580
Batch 340, Loss: 1.7580
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7575
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7581
Epoch 45 learning rate: 0.05
Epoch 45 time: 118.82548379898071 seconds
Epoch 45 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7576
Batch 90, Loss: 1.7580
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7574
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7575
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7581
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7577
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7583
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7583
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7580
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7579
Epoch 46 learning rate: 0.05
Epoch 46 time: 118.81993126869202 seconds
Epoch 46 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7576
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7580
Batch 50, Loss: 1.7580
Batch 60, Loss: 1.7581
Batch 70, Loss: 1.7580
Batch 80, Loss: 1.7576
Batch 90, Loss: 1.7581
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7578
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7582
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7580
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7579
Batch 390, Loss: 1.7578
Epoch 47 learning rate: 0.05
Epoch 47 time: 118.79895281791687 seconds
Epoch 47 accuracy: 10.0%
Batch 10, Loss: 1.7576
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7580
Batch 50, Loss: 1.7583
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7581
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7582
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7580
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7580
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7577
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7575
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7581
Batch 320, Loss: 1.7578
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7587
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7581
Batch 390, Loss: 1.7578
Epoch 48 learning rate: 0.05
Epoch 48 time: 118.80728244781494 seconds
Epoch 48 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7578
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7581
Batch 70, Loss: 1.7577
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7578
Batch 120, Loss: 1.7580
Batch 130, Loss: 1.7580
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7574
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7581
Batch 190, Loss: 1.7582
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7577
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7581
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7579
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7581
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7581
Batch 340, Loss: 1.7582
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7578
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7580
Epoch 49 learning rate: 0.05
Epoch 49 time: 118.73370599746704 seconds
Epoch 49 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7579
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7580
Batch 90, Loss: 1.7574
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7577
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7582
Batch 240, Loss: 1.7579
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7577
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7582
Batch 360, Loss: 1.7581
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7580
Epoch 50 learning rate: 0.05
Epoch 50 time: 118.78215646743774 seconds
Epoch 50 accuracy: 10.0%
rho:  0.04 , alpha:  0.3
Total training time: 5950.657633304596 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
The top Hessian eigenvalue of this model is 0.1001
Norm of the Gradient: 7.1133963764e-02
