The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 44113.3828
Batch 20, Loss: 66405.8613
Batch 30, Loss: 32693.9885
Batch 40, Loss: 13869.4256
Batch 50, Loss: 4706.1116
Batch 60, Loss: 2475.0182
Batch 70, Loss: 1780.2371
Batch 80, Loss: 1324.6835
Batch 90, Loss: 1013.1957
Batch 100, Loss: 939.5922
Batch 110, Loss: 753.1762
Batch 120, Loss: 686.2732
Batch 130, Loss: 593.3764
Batch 140, Loss: 724.5808
Batch 150, Loss: 618.8993
Batch 160, Loss: 389.2665
Batch 170, Loss: 413.3417
Batch 180, Loss: 378.9684
Batch 190, Loss: 432.1982
Batch 200, Loss: 360.1983
Batch 210, Loss: 379.3405
Batch 220, Loss: 357.3563
Batch 230, Loss: 344.2258
Batch 240, Loss: 341.7635
Batch 250, Loss: 202.9606
Batch 260, Loss: 231.6790
Batch 270, Loss: 214.9320
Batch 280, Loss: 285.8347
Batch 290, Loss: 216.4449
Batch 300, Loss: 316.8623
Batch 310, Loss: 203.0907
Batch 320, Loss: 186.3483
Batch 330, Loss: 228.7748
Batch 340, Loss: 154.7045
Batch 350, Loss: 139.8357
Batch 360, Loss: 131.3484
Batch 370, Loss: 114.8632
Batch 380, Loss: 163.2431
Batch 390, Loss: 147.7695
Epoch 1 learning rate: 0.05
Epoch 1 time: 128.54159998893738 seconds
Epoch 1 accuracy: 8.58%
Batch 10, Loss: 105.2097
Batch 20, Loss: 117.0396
Batch 30, Loss: 163.8571
Batch 40, Loss: 107.6245
Batch 50, Loss: 135.5119
Batch 60, Loss: 110.3276
Batch 70, Loss: 124.5456
Batch 80, Loss: 67.7635
Batch 90, Loss: 87.1477
Batch 100, Loss: 67.0038
Batch 110, Loss: 88.0135
Batch 120, Loss: 83.0209
Batch 130, Loss: 66.1045
Batch 140, Loss: 112.8262
Batch 150, Loss: 98.1188
Batch 160, Loss: 61.6106
Batch 170, Loss: 111.0775
Batch 180, Loss: 73.2064
Batch 190, Loss: 83.0588
Batch 200, Loss: 93.3720
Batch 210, Loss: 68.0056
Batch 220, Loss: 79.5213
Batch 230, Loss: 72.7286
Batch 240, Loss: 61.5084
Batch 250, Loss: 50.3112
Batch 260, Loss: 56.9057
Batch 270, Loss: 70.9291
Batch 280, Loss: 59.2992
Batch 290, Loss: 62.6561
Batch 300, Loss: 61.4627
Batch 310, Loss: 60.5044
Batch 320, Loss: 48.5955
Batch 330, Loss: 52.5159
Batch 340, Loss: 43.8987
Batch 350, Loss: 36.6424
Batch 360, Loss: 38.6986
Batch 370, Loss: 35.2544
Batch 380, Loss: 32.0914
Batch 390, Loss: 36.8202
Epoch 2 learning rate: 0.05
Epoch 2 time: 119.59073305130005 seconds
Epoch 2 accuracy: 10.01%
Batch 10, Loss: 30.1682
Batch 20, Loss: 41.9642
Batch 30, Loss: 41.1774
Batch 40, Loss: 55.5440
Batch 50, Loss: 34.9650
Batch 60, Loss: 38.8665
Batch 70, Loss: 36.2272
Batch 80, Loss: 36.2027
Batch 90, Loss: 30.4440
Batch 100, Loss: 34.9925
Batch 110, Loss: 33.5065
Batch 120, Loss: 34.3660
Batch 130, Loss: 46.5747
Batch 140, Loss: 73.6424
Batch 150, Loss: 54.1374
Batch 160, Loss: 56.5092
Batch 170, Loss: 42.9947
Batch 180, Loss: 36.2013
Batch 190, Loss: 39.6102
Batch 200, Loss: 38.7791
Batch 210, Loss: 51.7644
Batch 220, Loss: 41.7136
Batch 230, Loss: 59.1807
Batch 240, Loss: 29.0530
Batch 250, Loss: 32.0597
Batch 260, Loss: 33.6503
Batch 270, Loss: 28.1758
Batch 280, Loss: 33.8969
Batch 290, Loss: 27.5859
Batch 300, Loss: 33.0947
Batch 310, Loss: 30.5462
Batch 320, Loss: 28.5584
Batch 330, Loss: 36.2705
Batch 340, Loss: 35.6171
Batch 350, Loss: 28.9998
Batch 360, Loss: 33.4699
Batch 370, Loss: 40.2111
Batch 380, Loss: 22.6415
Batch 390, Loss: 40.5435
Epoch 3 learning rate: 0.05
Epoch 3 time: 119.55766248703003 seconds
Epoch 3 accuracy: 10.67%
Batch 10, Loss: 27.3454
Batch 20, Loss: 22.6754
Batch 30, Loss: 34.5051
Batch 40, Loss: 31.5383
Batch 50, Loss: 24.8440
Batch 60, Loss: 25.9279
Batch 70, Loss: 29.5939
Batch 80, Loss: 21.5000
Batch 90, Loss: 39.5648
Batch 100, Loss: 33.4532
Batch 110, Loss: 20.9449
Batch 120, Loss: 22.4576
Batch 130, Loss: 23.0936
Batch 140, Loss: 22.3841
Batch 150, Loss: 24.6957
Batch 160, Loss: 24.9555
Batch 170, Loss: 22.3156
Batch 180, Loss: 22.6404
Batch 190, Loss: 21.7706
Batch 200, Loss: 24.2923
Batch 210, Loss: 26.0930
Batch 220, Loss: 20.7080
Batch 230, Loss: 22.2647
Batch 240, Loss: 24.8361
Batch 250, Loss: 24.8430
Batch 260, Loss: 25.2448
Batch 270, Loss: 20.2594
Batch 280, Loss: 26.7388
Batch 290, Loss: 33.4250
Batch 300, Loss: 21.7876
Batch 310, Loss: 19.2394
Batch 320, Loss: 25.1670
Batch 330, Loss: 21.3776
Batch 340, Loss: 17.2999
Batch 350, Loss: 21.5959
Batch 360, Loss: 18.3550
Batch 370, Loss: 19.5045
Batch 380, Loss: 24.6669
Batch 390, Loss: 19.6518
Epoch 4 learning rate: 0.05
Epoch 4 time: 119.75301480293274 seconds
Epoch 4 accuracy: 10.42%
Batch 10, Loss: 20.7677
Batch 20, Loss: 20.6754
Batch 30, Loss: 19.5499
Batch 40, Loss: 18.6418
Batch 50, Loss: 17.4108
Batch 60, Loss: 22.5092
Batch 70, Loss: 20.1472
Batch 80, Loss: 19.7487
Batch 90, Loss: 17.5821
Batch 100, Loss: 21.6966
Batch 110, Loss: 18.3997
Batch 120, Loss: 16.8462
Batch 130, Loss: 21.5534
Batch 140, Loss: 18.2641
Batch 150, Loss: 20.8801
Batch 160, Loss: 20.9331
Batch 170, Loss: 20.4954
Batch 180, Loss: 18.0496
Batch 190, Loss: 18.5273
Batch 200, Loss: 21.5689
Batch 210, Loss: 17.4165
Batch 220, Loss: 18.1692
Batch 230, Loss: 20.6983
Batch 240, Loss: 20.3769
Batch 250, Loss: 20.4437
Batch 260, Loss: 17.3961
Batch 270, Loss: 15.9973
Batch 280, Loss: 17.7109
Batch 290, Loss: 19.5862
Batch 300, Loss: 14.6716
Batch 310, Loss: 15.1061
Batch 320, Loss: 15.3349
Batch 330, Loss: 17.1362
Batch 340, Loss: 17.9391
Batch 350, Loss: 17.2374
Batch 360, Loss: 13.3556
Batch 370, Loss: 16.9967
Batch 380, Loss: 14.8834
Batch 390, Loss: 15.5579
Epoch 5 learning rate: 0.05
Epoch 5 time: 119.71790552139282 seconds
Epoch 5 accuracy: 10.36%
Batch 10, Loss: 18.4368
Batch 20, Loss: 16.9806
Batch 30, Loss: 16.5162
Batch 40, Loss: 20.8193
Batch 50, Loss: 15.9917
Batch 60, Loss: 19.7187
Batch 70, Loss: 15.9156
Batch 80, Loss: 18.6696
Batch 90, Loss: 16.5495
Batch 100, Loss: 20.2037
Batch 110, Loss: 18.0573
Batch 120, Loss: 15.6761
Batch 130, Loss: 15.4024
Batch 140, Loss: 14.0211
Batch 150, Loss: 18.1092
Batch 160, Loss: 13.5467
Batch 170, Loss: 15.2476
Batch 180, Loss: 15.7628
Batch 190, Loss: 13.4615
Batch 200, Loss: 14.1754
Batch 210, Loss: 18.8516
Batch 220, Loss: 14.2058
Batch 230, Loss: 14.3289
Batch 240, Loss: 17.8395
Batch 250, Loss: 16.6959
Batch 260, Loss: 15.2761
Batch 270, Loss: 15.5085
Batch 280, Loss: 12.8055
Batch 290, Loss: 14.3156
Batch 300, Loss: 16.4770
Batch 310, Loss: 12.5008
Batch 320, Loss: 12.9060
Batch 330, Loss: 13.4146
Batch 340, Loss: 12.9681
Batch 350, Loss: 12.4702
Batch 360, Loss: 13.2969
Batch 370, Loss: 14.2852
Batch 380, Loss: 14.3908
Batch 390, Loss: 13.3449
Epoch 6 learning rate: 0.05
Epoch 6 time: 119.48816084861755 seconds
Epoch 6 accuracy: 10.48%
Batch 10, Loss: 12.8602
Batch 20, Loss: 17.1303
Batch 30, Loss: 13.3801
Batch 40, Loss: 12.2245
Batch 50, Loss: 12.4146
Batch 60, Loss: 11.4396
Batch 70, Loss: 16.1264
Batch 80, Loss: 13.8337
Batch 90, Loss: 13.7600
Batch 100, Loss: 13.0454
Batch 110, Loss: 14.9690
Batch 120, Loss: 11.7020
Batch 130, Loss: 13.4334
Batch 140, Loss: 13.7696
Batch 150, Loss: 12.6931
Batch 160, Loss: 11.8593
Batch 170, Loss: 12.2102
Batch 180, Loss: 11.0498
Batch 190, Loss: 10.9607
Batch 200, Loss: 14.0837
Batch 210, Loss: 13.2138
Batch 220, Loss: 16.9799
Batch 230, Loss: 13.5626
Batch 240, Loss: 11.6623
Batch 250, Loss: 11.9196
Batch 260, Loss: 11.3866
Batch 270, Loss: 11.8674
Batch 280, Loss: 10.7604
Batch 290, Loss: 13.6757
Batch 300, Loss: 11.1863
Batch 310, Loss: 11.1862
Batch 320, Loss: 11.4971
Batch 330, Loss: 11.2640
Batch 340, Loss: 10.5920
Batch 350, Loss: 10.4999
Batch 360, Loss: 10.9217
Batch 370, Loss: 11.0353
Batch 380, Loss: 11.8475
Batch 390, Loss: 10.9901
Epoch 7 learning rate: 0.05
Epoch 7 time: 119.77444434165955 seconds
Epoch 7 accuracy: 10.56%
Batch 10, Loss: 10.9010
Batch 20, Loss: 9.8207
Batch 30, Loss: 13.2781
Batch 40, Loss: 11.4348
Batch 50, Loss: 11.9453
Batch 60, Loss: 9.5588
Batch 70, Loss: 11.7601
Batch 80, Loss: 10.6167
Batch 90, Loss: 9.8957
Batch 100, Loss: 9.9152
Batch 110, Loss: 10.2298
Batch 120, Loss: 10.2745
Batch 130, Loss: 10.1162
Batch 140, Loss: 10.4243
Batch 150, Loss: 9.7941
Batch 160, Loss: 9.6328
Batch 170, Loss: 9.4546
Batch 180, Loss: 10.3244
Batch 190, Loss: 11.9262
Batch 200, Loss: 10.4887
Batch 210, Loss: 10.3657
Batch 220, Loss: 11.1608
Batch 230, Loss: 9.4659
Batch 240, Loss: 9.9438
Batch 250, Loss: 9.6911
Batch 260, Loss: 9.1634
Batch 270, Loss: 9.4771
Batch 280, Loss: 11.5429
Batch 290, Loss: 10.8111
Batch 300, Loss: 9.8116
Batch 310, Loss: 9.1068
Batch 320, Loss: 10.3477
Batch 330, Loss: 9.2200
Batch 340, Loss: 8.8676
Batch 350, Loss: 9.8700
Batch 360, Loss: 9.3693
Batch 370, Loss: 8.5690
Batch 380, Loss: 12.4585
Batch 390, Loss: 8.4945
Epoch 8 learning rate: 0.05
Epoch 8 time: 119.51615953445435 seconds
Epoch 8 accuracy: 10.72%
Batch 10, Loss: 9.0327
Batch 20, Loss: 8.1680
Batch 30, Loss: 8.4226
Batch 40, Loss: 9.3911
Batch 50, Loss: 8.9436
Batch 60, Loss: 7.8386
Batch 70, Loss: 8.7738
Batch 80, Loss: 7.6320
Batch 90, Loss: 8.8603
Batch 100, Loss: 8.7452
Batch 110, Loss: 8.3718
Batch 120, Loss: 8.0926
Batch 130, Loss: 7.0978
Batch 140, Loss: 7.2876
Batch 150, Loss: 7.0290
Batch 160, Loss: 7.4530
Batch 170, Loss: 8.2489
Batch 180, Loss: 7.3477
Batch 190, Loss: 7.1542
Batch 200, Loss: 6.9097
Batch 210, Loss: 8.8258
Batch 220, Loss: 6.1835
Batch 230, Loss: 5.6317
Batch 240, Loss: 5.7996
Batch 250, Loss: 5.1798
Batch 260, Loss: 5.0129
Batch 270, Loss: 5.4939
Batch 280, Loss: 5.9401
Batch 290, Loss: 5.2259
Batch 300, Loss: 4.8045
Batch 310, Loss: 4.7582
Batch 320, Loss: 5.5675
Batch 330, Loss: 4.9858
Batch 340, Loss: 4.2010
Batch 350, Loss: 4.4742
Batch 360, Loss: 5.5453
Batch 370, Loss: 5.0352
Batch 380, Loss: 5.3361
Batch 390, Loss: 4.9876
Epoch 9 learning rate: 0.05
Epoch 9 time: 119.54787731170654 seconds
Epoch 9 accuracy: 12.81%
Batch 10, Loss: 4.8782
Batch 20, Loss: 4.1387
Batch 30, Loss: 4.3627
Batch 40, Loss: 4.9198
Batch 50, Loss: 4.1201
Batch 60, Loss: 4.8773
Batch 70, Loss: 4.5425
Batch 80, Loss: 3.9586
Batch 90, Loss: 5.0990
Batch 100, Loss: 5.0963
Batch 110, Loss: 6.5714
Batch 120, Loss: 4.8639
Batch 130, Loss: 4.8592
Batch 140, Loss: 5.0688
Batch 150, Loss: 4.9700
Batch 160, Loss: 5.2961
Batch 170, Loss: 4.8234
Batch 180, Loss: 4.1819
Batch 190, Loss: 4.1776
Batch 200, Loss: 4.0060
Batch 210, Loss: 3.8894
Batch 220, Loss: 4.0903
Batch 230, Loss: 4.1046
Batch 240, Loss: 3.8858
Batch 250, Loss: 4.1569
Batch 260, Loss: 3.9665
Batch 270, Loss: 4.5954
Batch 280, Loss: 5.0508
Batch 290, Loss: 3.7840
Batch 300, Loss: 3.9392
Batch 310, Loss: 4.9202
Batch 320, Loss: 3.7412
Batch 330, Loss: 3.7189
Batch 340, Loss: 3.6470
Batch 350, Loss: 3.9535
Batch 360, Loss: 4.0743
Batch 370, Loss: 3.4874
Batch 380, Loss: 3.3994
Batch 390, Loss: 3.7441
Epoch 10 learning rate: 0.05
Epoch 10 time: 119.5813398361206 seconds
Epoch 10 accuracy: 12.76%
Batch 10, Loss: 3.3124
Batch 20, Loss: 3.7334
Batch 30, Loss: 5.2340
Batch 40, Loss: 3.2320
Batch 50, Loss: 3.8485
Batch 60, Loss: 3.1601
Batch 70, Loss: 3.2758
Batch 80, Loss: 3.5579
Batch 90, Loss: 3.5017
Batch 100, Loss: 3.7999
Batch 110, Loss: 3.2043
Batch 120, Loss: 3.4807
Batch 130, Loss: 3.1198
Batch 140, Loss: 3.2260
Batch 150, Loss: 3.1003
Batch 160, Loss: 3.1751
Batch 170, Loss: 2.9757
Batch 180, Loss: 3.8346
Batch 190, Loss: 3.9991
Batch 200, Loss: 3.5232
Batch 210, Loss: 3.0920
Batch 220, Loss: 3.2741
Batch 230, Loss: 3.1876
Batch 240, Loss: 3.1702
Batch 250, Loss: 3.7078
Batch 260, Loss: 3.2003
Batch 270, Loss: 3.0120
Batch 280, Loss: 3.0168
Batch 290, Loss: 2.8871
Batch 300, Loss: 3.1674
Batch 310, Loss: 2.7007
Batch 320, Loss: 2.7986
Batch 330, Loss: 2.8731
Batch 340, Loss: 2.7936
Batch 350, Loss: 2.7954
Batch 360, Loss: 2.6988
Batch 370, Loss: 2.6574
Batch 380, Loss: 2.7142
Batch 390, Loss: 2.9356
Epoch 11 learning rate: 0.05
Epoch 11 time: 119.30940842628479 seconds
Epoch 11 accuracy: 11.47%
Batch 10, Loss: 3.3190
Batch 20, Loss: 2.9700
Batch 30, Loss: 3.5921
Batch 40, Loss: 2.7326
Batch 50, Loss: 2.7424
Batch 60, Loss: 2.6726
Batch 70, Loss: 2.8098
Batch 80, Loss: 2.6361
Batch 90, Loss: 2.5856
Batch 100, Loss: 2.9027
Batch 110, Loss: 2.6455
Batch 120, Loss: 2.6526
Batch 130, Loss: 2.6558
Batch 140, Loss: 3.2173
Batch 150, Loss: 2.4540
Batch 160, Loss: 2.6718
Batch 170, Loss: 3.0890
Batch 180, Loss: 2.7665
Batch 190, Loss: 2.4754
Batch 200, Loss: 2.3215
Batch 210, Loss: 2.4721
Batch 220, Loss: 2.4357
Batch 230, Loss: 2.4436
Batch 240, Loss: 2.8556
Batch 250, Loss: 2.6451
Batch 260, Loss: 2.4487
Batch 270, Loss: 2.4790
Batch 280, Loss: 2.3985
Batch 290, Loss: 2.3335
Batch 300, Loss: 2.3735
Batch 310, Loss: 2.4448
Batch 320, Loss: 2.4445
Batch 330, Loss: 2.3489
Batch 340, Loss: 2.3865
Batch 350, Loss: 2.7094
Batch 360, Loss: 2.4068
Batch 370, Loss: 2.3507
Batch 380, Loss: 2.3027
Batch 390, Loss: 2.8332
Epoch 12 learning rate: 0.05
Epoch 12 time: 119.3470687866211 seconds
Epoch 12 accuracy: 12.6%
Batch 10, Loss: 2.4978
Batch 20, Loss: 2.4093
Batch 30, Loss: 2.5542
Batch 40, Loss: 2.2786
Batch 50, Loss: 2.4068
Batch 60, Loss: 2.3172
Batch 70, Loss: 2.2714
Batch 80, Loss: 2.4912
Batch 90, Loss: 2.4778
Batch 100, Loss: 2.3251
Batch 110, Loss: 2.2323
Batch 120, Loss: 2.2899
Batch 130, Loss: 2.3027
Batch 140, Loss: 2.3147
Batch 150, Loss: 2.3863
Batch 160, Loss: 2.2379
Batch 170, Loss: 2.4919
Batch 180, Loss: 2.3972
Batch 190, Loss: 2.3390
Batch 200, Loss: 2.1805
Batch 210, Loss: 2.4619
Batch 220, Loss: 2.2825
Batch 230, Loss: 2.1577
Batch 240, Loss: 2.1965
Batch 250, Loss: 2.1880
Batch 260, Loss: 2.3128
Batch 270, Loss: 2.2287
Batch 280, Loss: 2.1392
Batch 290, Loss: 2.2471
Batch 300, Loss: 2.1726
Batch 310, Loss: 2.4639
Batch 320, Loss: 2.0970
Batch 330, Loss: 2.0984
Batch 340, Loss: 2.1417
Batch 350, Loss: 2.2202
Batch 360, Loss: 2.2480
Batch 370, Loss: 2.4128
Batch 380, Loss: 2.2414
Batch 390, Loss: 2.1822
Epoch 13 learning rate: 0.05
Epoch 13 time: 119.46134305000305 seconds
Epoch 13 accuracy: 12.7%
Batch 10, Loss: 2.3267
Batch 20, Loss: 2.1493
Batch 30, Loss: 2.2643
Batch 40, Loss: 2.1240
Batch 50, Loss: 2.2657
Batch 60, Loss: 2.0821
Batch 70, Loss: 2.3704
Batch 80, Loss: 2.1746
Batch 90, Loss: 2.2275
Batch 100, Loss: 2.2305
Batch 110, Loss: 2.1065
Batch 120, Loss: 2.2801
Batch 130, Loss: 2.1812
Batch 140, Loss: 2.3221
Batch 150, Loss: 2.2387
Batch 160, Loss: 2.2354
Batch 170, Loss: 2.0977
Batch 180, Loss: 2.1464
Batch 190, Loss: 2.1957
Batch 200, Loss: 2.1622
Batch 210, Loss: 2.1396
Batch 220, Loss: 2.0865
Batch 230, Loss: 2.1482
Batch 240, Loss: 2.0665
Batch 250, Loss: 2.1003
Batch 260, Loss: 2.0973
Batch 270, Loss: 2.0832
Batch 280, Loss: 2.1035
Batch 290, Loss: 2.1146
Batch 300, Loss: 2.4280
Batch 310, Loss: 2.2187
Batch 320, Loss: 2.1074
Batch 330, Loss: 2.0627
Batch 340, Loss: 2.1151
Batch 350, Loss: 2.1059
Batch 360, Loss: 2.0489
Batch 370, Loss: 2.3719
Batch 380, Loss: 2.0875
Batch 390, Loss: 2.0461
Epoch 14 learning rate: 0.05
Epoch 14 time: 119.42723202705383 seconds
Epoch 14 accuracy: 14.25%
Batch 10, Loss: 2.0168
Batch 20, Loss: 2.1765
Batch 30, Loss: 2.1157
Batch 40, Loss: 2.1483
Batch 50, Loss: 2.1347
Batch 60, Loss: 2.0280
Batch 70, Loss: 2.1693
Batch 80, Loss: 2.1024
Batch 90, Loss: 2.3850
Batch 100, Loss: 2.1139
Batch 110, Loss: 2.0577
Batch 120, Loss: 2.2098
Batch 130, Loss: 2.1241
Batch 140, Loss: 2.1884
Batch 150, Loss: 2.1404
Batch 160, Loss: 2.0995
Batch 170, Loss: 1.9878
Batch 180, Loss: 2.2280
Batch 190, Loss: 2.0638
Batch 200, Loss: 2.0609
Batch 210, Loss: 2.0692
Batch 220, Loss: 2.0863
Batch 230, Loss: 1.9951
Batch 240, Loss: 2.0968
Batch 250, Loss: 1.9993
Batch 260, Loss: 2.1038
Batch 270, Loss: 2.0306
Batch 280, Loss: 2.0079
Batch 290, Loss: 2.0191
Batch 300, Loss: 2.0223
Batch 310, Loss: 2.1091
Batch 320, Loss: 2.0100
Batch 330, Loss: 2.0143
Batch 340, Loss: 2.0463
Batch 350, Loss: 2.0138
Batch 360, Loss: 2.0041
Batch 370, Loss: 2.0077
Batch 380, Loss: 1.9999
Batch 390, Loss: 1.9577
Epoch 15 learning rate: 0.05
Epoch 15 time: 119.37940621376038 seconds
Epoch 15 accuracy: 15.3%
Batch 10, Loss: 2.1657
Batch 20, Loss: 2.0432
Batch 30, Loss: 2.0467
Batch 40, Loss: 2.0320
Batch 50, Loss: 1.9964
Batch 60, Loss: 2.1825
Batch 70, Loss: 1.9559
Batch 80, Loss: 2.0104
Batch 90, Loss: 1.9795
Batch 100, Loss: 1.9609
Batch 110, Loss: 2.0278
Batch 120, Loss: 1.9711
Batch 130, Loss: 1.9745
Batch 140, Loss: 1.9903
Batch 150, Loss: 1.9543
Batch 160, Loss: 1.9599
Batch 170, Loss: 2.0911
Batch 180, Loss: 1.9720
Batch 190, Loss: 1.9966
Batch 200, Loss: 2.0865
Batch 210, Loss: 2.0093
Batch 220, Loss: 2.0165
Batch 230, Loss: 1.9814
Batch 240, Loss: 1.9657
Batch 250, Loss: 1.9456
Batch 260, Loss: 1.9224
Batch 270, Loss: 2.0161
Batch 280, Loss: 1.9194
Batch 290, Loss: 2.1809
Batch 300, Loss: 1.9883
Batch 310, Loss: 1.9841
Batch 320, Loss: 2.0047
Batch 330, Loss: 1.9742
Batch 340, Loss: 1.9681
Batch 350, Loss: 1.9305
Batch 360, Loss: 1.9091
Batch 370, Loss: 1.9730
Batch 380, Loss: 1.9881
Batch 390, Loss: 1.9047
Epoch 16 learning rate: 0.05
Epoch 16 time: 119.17427515983582 seconds
Epoch 16 accuracy: 15.99%
Batch 10, Loss: 1.9410
Batch 20, Loss: 1.9493
Batch 30, Loss: 2.1471
Batch 40, Loss: 1.9592
Batch 50, Loss: 1.9213
Batch 60, Loss: 1.8972
Batch 70, Loss: 1.9350
Batch 80, Loss: 1.8953
Batch 90, Loss: 1.9258
Batch 100, Loss: 1.9304
Batch 110, Loss: 1.9313
Batch 120, Loss: 1.8890
Batch 130, Loss: 2.1044
Batch 140, Loss: 1.9400
Batch 150, Loss: 1.8838
Batch 160, Loss: 1.9198
Batch 170, Loss: 1.9560
Batch 180, Loss: 1.8784
Batch 190, Loss: 1.8847
Batch 200, Loss: 1.9881
Batch 210, Loss: 2.0431
Batch 220, Loss: 1.9183
Batch 230, Loss: 1.9635
Batch 240, Loss: 1.8949
Batch 250, Loss: 1.8706
Batch 260, Loss: 1.9062
Batch 270, Loss: 1.9268
Batch 280, Loss: 1.8609
Batch 290, Loss: 1.8894
Batch 300, Loss: 1.9319
Batch 310, Loss: 1.9718
Batch 320, Loss: 1.9349
Batch 330, Loss: 1.9145
Batch 340, Loss: 1.9567
Batch 350, Loss: 1.9338
Batch 360, Loss: 1.9313
Batch 370, Loss: 1.8816
Batch 380, Loss: 1.9169
Batch 390, Loss: 1.9179
Epoch 17 learning rate: 0.05
Epoch 17 time: 119.23740696907043 seconds
Epoch 17 accuracy: 17.91%
Batch 10, Loss: 1.9025
Batch 20, Loss: 1.8747
Batch 30, Loss: 1.8837
Batch 40, Loss: 1.8735
Batch 50, Loss: 1.8599
Batch 60, Loss: 1.8657
Batch 70, Loss: 1.9121
Batch 80, Loss: 1.8909
Batch 90, Loss: 1.8215
Batch 100, Loss: 1.8600
Batch 110, Loss: 1.8816
Batch 120, Loss: 1.8655
Batch 130, Loss: 1.8743
Batch 140, Loss: 1.8825
Batch 150, Loss: 1.8405
Batch 160, Loss: 1.8694
Batch 170, Loss: 1.8919
Batch 180, Loss: 1.8545
Batch 190, Loss: 1.9943
Batch 200, Loss: 1.8662
Batch 210, Loss: 1.8625
Batch 220, Loss: 1.8375
Batch 230, Loss: 1.8637
Batch 240, Loss: 1.8346
Batch 250, Loss: 1.8335
Batch 260, Loss: 1.9946
Batch 270, Loss: 1.8366
Batch 280, Loss: 1.8505
Batch 290, Loss: 1.8411
Batch 300, Loss: 1.9044
Batch 310, Loss: 1.8840
Batch 320, Loss: 1.8672
Batch 330, Loss: 1.8042
Batch 340, Loss: 1.8234
Batch 350, Loss: 1.8242
Batch 360, Loss: 1.8956
Batch 370, Loss: 1.8149
Batch 380, Loss: 1.8048
Batch 390, Loss: 1.8336
Epoch 18 learning rate: 0.05
Epoch 18 time: 119.49658250808716 seconds
Epoch 18 accuracy: 17.15%
Batch 10, Loss: 1.8333
Batch 20, Loss: 1.8920
Batch 30, Loss: 1.8594
Batch 40, Loss: 1.8256
Batch 50, Loss: 1.8582
Batch 60, Loss: 1.8154
Batch 70, Loss: 1.7974
Batch 80, Loss: 1.7892
Batch 90, Loss: 1.8076
Batch 100, Loss: 1.7951
Batch 110, Loss: 1.8282
Batch 120, Loss: 1.8155
Batch 130, Loss: 1.8083
Batch 140, Loss: 1.7715
Batch 150, Loss: 1.8252
Batch 160, Loss: 1.8149
Batch 170, Loss: 1.8134
Batch 180, Loss: 1.7764
Batch 190, Loss: 1.7966
Batch 200, Loss: 1.7731
Batch 210, Loss: 1.8065
Batch 220, Loss: 1.7751
Batch 230, Loss: 1.7735
Batch 240, Loss: 1.8052
Batch 250, Loss: 1.7774
Batch 260, Loss: 1.7882
Batch 270, Loss: 1.9897
Batch 280, Loss: 1.8224
Batch 290, Loss: 1.7832
Batch 300, Loss: 1.7843
Batch 310, Loss: 1.7797
Batch 320, Loss: 1.7518
Batch 330, Loss: 1.7685
Batch 340, Loss: 1.7652
Batch 350, Loss: 1.7827
Batch 360, Loss: 1.7638
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7656
Batch 390, Loss: 1.7769
Epoch 19 learning rate: 0.05
Epoch 19 time: 119.55313539505005 seconds
Epoch 19 accuracy: 18.66%
Batch 10, Loss: 1.7637
Batch 20, Loss: 1.7420
Batch 30, Loss: 1.7832
Batch 40, Loss: 1.7761
Batch 50, Loss: 1.7583
Batch 60, Loss: 1.7553
Batch 70, Loss: 1.8266
Batch 80, Loss: 1.7414
Batch 90, Loss: 1.7453
Batch 100, Loss: 1.7667
Batch 110, Loss: 1.7521
Batch 120, Loss: 1.7525
Batch 130, Loss: 1.8704
Batch 140, Loss: 1.7553
Batch 150, Loss: 1.7534
Batch 160, Loss: 1.7725
Batch 170, Loss: 1.7724
Batch 180, Loss: 1.7657
Batch 190, Loss: 1.7506
Batch 200, Loss: 1.7429
Batch 210, Loss: 1.7680
Batch 220, Loss: 1.7256
Batch 230, Loss: 1.7877
Batch 240, Loss: 1.7279
Batch 250, Loss: 1.7347
Batch 260, Loss: 1.7218
Batch 270, Loss: 1.8126
Batch 280, Loss: 1.7381
Batch 290, Loss: 1.7294
Batch 300, Loss: 1.7409
Batch 310, Loss: 1.7382
Batch 320, Loss: 1.7415
Batch 330, Loss: 1.7377
Batch 340, Loss: 1.7376
Batch 350, Loss: 1.7466
Batch 360, Loss: 1.7361
Batch 370, Loss: 1.7385
Batch 380, Loss: 1.7355
Batch 390, Loss: 1.7604
Epoch 20 learning rate: 0.05
Epoch 20 time: 119.4945285320282 seconds
Epoch 20 accuracy: 18.06%
Total training time: 2398.976304292679 seconds
