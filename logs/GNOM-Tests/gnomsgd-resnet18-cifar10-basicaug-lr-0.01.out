The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 257.3641
Batch 20, Loss: 684.5684
Batch 30, Loss: 2169.8641
Batch 40, Loss: 1732.2903
Batch 50, Loss: 1208.3269
Batch 60, Loss: 933.3290
Batch 70, Loss: 627.9860
Batch 80, Loss: 465.3499
Batch 90, Loss: 367.6954
Batch 100, Loss: 302.3298
Batch 110, Loss: 281.4842
Batch 120, Loss: 210.3424
Batch 130, Loss: 183.0326
Batch 140, Loss: 137.1711
Batch 150, Loss: 143.7447
Batch 160, Loss: 101.6623
Batch 170, Loss: 86.7561
Batch 180, Loss: 92.4940
Batch 190, Loss: 71.6915
Batch 200, Loss: 54.0860
Batch 210, Loss: 58.7598
Batch 220, Loss: 52.1759
Batch 230, Loss: 43.7965
Batch 240, Loss: 36.3768
Batch 250, Loss: 36.8616
Batch 260, Loss: 26.8876
Batch 270, Loss: 25.1812
Batch 280, Loss: 25.2507
Batch 290, Loss: 25.0836
Batch 300, Loss: 20.8234
Batch 310, Loss: 23.2408
Batch 320, Loss: 19.6629
Batch 330, Loss: 18.2579
Batch 340, Loss: 17.9564
Batch 350, Loss: 17.4159
Batch 360, Loss: 19.8426
Batch 370, Loss: 16.0459
Batch 380, Loss: 16.0021
Batch 390, Loss: 18.7305
Epoch 1 learning rate: 0.01
Epoch 1 time: 128.01357054710388 seconds
Epoch 1 accuracy: 10.16%
Batch 10, Loss: 13.9622
Batch 20, Loss: 17.4123
Batch 30, Loss: 14.7072
Batch 40, Loss: 15.3227
Batch 50, Loss: 14.0972
Batch 60, Loss: 14.8532
Batch 70, Loss: 13.0860
Batch 80, Loss: 12.9966
Batch 90, Loss: 13.7192
Batch 100, Loss: 13.0636
Batch 110, Loss: 12.8846
Batch 120, Loss: 13.2723
Batch 130, Loss: 13.6859
Batch 140, Loss: 12.4255
Batch 150, Loss: 12.2733
Batch 160, Loss: 15.0176
Batch 170, Loss: 11.5947
Batch 180, Loss: 11.0854
Batch 190, Loss: 12.8665
Batch 200, Loss: 10.9680
Batch 210, Loss: 11.2752
Batch 220, Loss: 11.3393
Batch 230, Loss: 11.3749
Batch 240, Loss: 11.1344
Batch 250, Loss: 11.5277
Batch 260, Loss: 10.6750
Batch 270, Loss: 10.4508
Batch 280, Loss: 10.6613
Batch 290, Loss: 11.4274
Batch 300, Loss: 11.7366
Batch 310, Loss: 10.8290
Batch 320, Loss: 11.0450
Batch 330, Loss: 10.3684
Batch 340, Loss: 12.0514
Batch 350, Loss: 11.0977
Batch 360, Loss: 11.6825
Batch 370, Loss: 10.7820
Batch 380, Loss: 10.6768
Batch 390, Loss: 10.2311
Epoch 2 learning rate: 0.01
Epoch 2 time: 119.21131300926208 seconds
Epoch 2 accuracy: 10.3%
Batch 10, Loss: 11.4317
Batch 20, Loss: 9.8847
Batch 30, Loss: 10.5573
Batch 40, Loss: 11.0429
Batch 50, Loss: 10.0472
Batch 60, Loss: 10.9882
Batch 70, Loss: 10.6546
Batch 80, Loss: 10.0877
Batch 90, Loss: 9.9383
Batch 100, Loss: 11.1452
Batch 110, Loss: 9.8879
Batch 120, Loss: 10.1771
Batch 130, Loss: 10.1925
Batch 140, Loss: 11.2690
Batch 150, Loss: 10.8660
Batch 160, Loss: 9.7148
Batch 170, Loss: 9.9455
Batch 180, Loss: 9.7287
Batch 190, Loss: 10.2010
Batch 200, Loss: 9.5239
Batch 210, Loss: 10.1285
Batch 220, Loss: 10.0362
Batch 230, Loss: 10.4610
Batch 240, Loss: 10.0094
Batch 250, Loss: 9.8262
Batch 260, Loss: 10.1045
Batch 270, Loss: 10.0924
Batch 280, Loss: 10.0324
Batch 290, Loss: 9.2722
Batch 300, Loss: 10.1626
Batch 310, Loss: 9.6911
Batch 320, Loss: 9.9440
Batch 330, Loss: 9.7043
Batch 340, Loss: 9.9555
Batch 350, Loss: 9.5950
Batch 360, Loss: 9.5133
Batch 370, Loss: 9.4302
Batch 380, Loss: 9.4827
Batch 390, Loss: 10.0723
Epoch 3 learning rate: 0.01
Epoch 3 time: 119.24660968780518 seconds
Epoch 3 accuracy: 10.24%
Batch 10, Loss: 9.2609
Batch 20, Loss: 9.4310
Batch 30, Loss: 9.8459
Batch 40, Loss: 10.2607
Batch 50, Loss: 9.6280
Batch 60, Loss: 9.1665
Batch 70, Loss: 9.5591
Batch 80, Loss: 9.8363
Batch 90, Loss: 9.7608
Batch 100, Loss: 9.6252
Batch 110, Loss: 9.4729
Batch 120, Loss: 9.4417
Batch 130, Loss: 9.3881
Batch 140, Loss: 9.0622
Batch 150, Loss: 9.4269
Batch 160, Loss: 9.6475
Batch 170, Loss: 9.6203
Batch 180, Loss: 9.1194
Batch 190, Loss: 9.3020
Batch 200, Loss: 9.2952
Batch 210, Loss: 9.0929
Batch 220, Loss: 9.1952
Batch 230, Loss: 9.0663
Batch 240, Loss: 9.1546
Batch 250, Loss: 9.4215
Batch 260, Loss: 8.9629
Batch 270, Loss: 9.2926
Batch 280, Loss: 9.6706
Batch 290, Loss: 9.7945
Batch 300, Loss: 9.7470
Batch 310, Loss: 9.1312
Batch 320, Loss: 9.3987
Batch 330, Loss: 10.2619
Batch 340, Loss: 9.1653
Batch 350, Loss: 9.1815
Batch 360, Loss: 9.0633
Batch 370, Loss: 9.3366
Batch 380, Loss: 8.9691
Batch 390, Loss: 9.4198
Epoch 4 learning rate: 0.01
Epoch 4 time: 119.03985452651978 seconds
Epoch 4 accuracy: 10.2%
Batch 10, Loss: 9.2098
Batch 20, Loss: 9.8224
Batch 30, Loss: 9.0038
Batch 40, Loss: 9.3004
Batch 50, Loss: 9.0887
Batch 60, Loss: 9.2166
Batch 70, Loss: 8.7599
Batch 80, Loss: 9.2560
Batch 90, Loss: 8.9582
Batch 100, Loss: 8.8963
Batch 110, Loss: 8.9049
Batch 120, Loss: 8.9404
Batch 130, Loss: 9.4114
Batch 140, Loss: 8.8372
Batch 150, Loss: 8.9220
Batch 160, Loss: 8.7539
Batch 170, Loss: 9.6674
Batch 180, Loss: 9.0287
Batch 190, Loss: 9.0191
Batch 200, Loss: 9.0950
Batch 210, Loss: 9.2965
Batch 220, Loss: 8.8086
Batch 230, Loss: 9.5269
Batch 240, Loss: 9.1800
Batch 250, Loss: 9.0245
Batch 260, Loss: 8.8149
Batch 270, Loss: 9.1536
Batch 280, Loss: 8.7048
Batch 290, Loss: 8.9283
Batch 300, Loss: 9.0537
Batch 310, Loss: 9.1505
Batch 320, Loss: 9.0229
Batch 330, Loss: 8.8775
Batch 340, Loss: 8.9653
Batch 350, Loss: 9.1225
Batch 360, Loss: 9.0299
Batch 370, Loss: 9.1203
Batch 380, Loss: 8.8561
Batch 390, Loss: 8.7664
Epoch 5 learning rate: 0.01
Epoch 5 time: 119.29406666755676 seconds
Epoch 5 accuracy: 10.17%
Batch 10, Loss: 8.8329
Batch 20, Loss: 9.1874
Batch 30, Loss: 8.6503
Batch 40, Loss: 8.9419
Batch 50, Loss: 9.3976
Batch 60, Loss: 8.8184
Batch 70, Loss: 8.5778
Batch 80, Loss: 8.8104
Batch 90, Loss: 8.6885
Batch 100, Loss: 8.5783
Batch 110, Loss: 8.8373
Batch 120, Loss: 8.8265
Batch 130, Loss: 8.8148
Batch 140, Loss: 8.8380
Batch 150, Loss: 8.6180
Batch 160, Loss: 8.6857
Batch 170, Loss: 8.6631
Batch 180, Loss: 8.6751
Batch 190, Loss: 8.9445
Batch 200, Loss: 8.5687
Batch 210, Loss: 8.6936
Batch 220, Loss: 8.8477
Batch 230, Loss: 8.7796
Batch 240, Loss: 8.7052
Batch 250, Loss: 8.5888
Batch 260, Loss: 8.7062
Batch 270, Loss: 8.9137
Batch 280, Loss: 8.5089
Batch 290, Loss: 8.5049
Batch 300, Loss: 8.5267
Batch 310, Loss: 8.5523
Batch 320, Loss: 8.3779
Batch 330, Loss: 8.6816
Batch 340, Loss: 8.9421
Batch 350, Loss: 8.5326
Batch 360, Loss: 8.4641
Batch 370, Loss: 8.7798
Batch 380, Loss: 8.4875
Batch 390, Loss: 8.7418
Epoch 6 learning rate: 0.01
Epoch 6 time: 119.1837387084961 seconds
Epoch 6 accuracy: 10.22%
Batch 10, Loss: 8.5374
Batch 20, Loss: 8.5529
Batch 30, Loss: 9.0524
Batch 40, Loss: 8.3416
Batch 50, Loss: 8.6935
Batch 60, Loss: 8.5671
Batch 70, Loss: 8.8670
Batch 80, Loss: 8.4271
Batch 90, Loss: 8.4376
Batch 100, Loss: 8.5388
Batch 110, Loss: 8.3827
Batch 120, Loss: 8.5145
Batch 130, Loss: 8.4285
Batch 140, Loss: 8.6864
Batch 150, Loss: 8.1802
Batch 160, Loss: 8.4697
Batch 170, Loss: 8.5400
Batch 180, Loss: 8.2954
Batch 190, Loss: 8.6228
Batch 200, Loss: 8.3713
Batch 210, Loss: 8.1699
Batch 220, Loss: 8.3651
Batch 230, Loss: 8.5152
Batch 240, Loss: 8.6428
Batch 250, Loss: 8.4478
Batch 260, Loss: 8.2419
Batch 270, Loss: 8.2376
Batch 280, Loss: 8.2901
Batch 290, Loss: 8.6876
Batch 300, Loss: 8.3920
Batch 310, Loss: 8.2950
Batch 320, Loss: 8.2940
Batch 330, Loss: 8.5346
Batch 340, Loss: 8.2940
Batch 350, Loss: 8.2423
Batch 360, Loss: 8.3883
Batch 370, Loss: 8.2334
Batch 380, Loss: 8.1410
Batch 390, Loss: 8.5310
Epoch 7 learning rate: 0.01
Epoch 7 time: 119.05125045776367 seconds
Epoch 7 accuracy: 10.21%
Batch 10, Loss: 8.3340
Batch 20, Loss: 8.1119
Batch 30, Loss: 7.9696
Batch 40, Loss: 8.4604
Batch 50, Loss: 8.2782
Batch 60, Loss: 8.1550
Batch 70, Loss: 8.3549
Batch 80, Loss: 8.3762
Batch 90, Loss: 8.2344
Batch 100, Loss: 8.1147
Batch 110, Loss: 8.2438
Batch 120, Loss: 8.1642
Batch 130, Loss: 8.1945
Batch 140, Loss: 8.1406
Batch 150, Loss: 8.3572
Batch 160, Loss: 8.1879
Batch 170, Loss: 8.4002
Batch 180, Loss: 8.2768
Batch 190, Loss: 8.5280
Batch 200, Loss: 8.2152
Batch 210, Loss: 8.0541
Batch 220, Loss: 8.0418
Batch 230, Loss: 8.2512
Batch 240, Loss: 8.0529
Batch 250, Loss: 8.1792
Batch 260, Loss: 8.1374
Batch 270, Loss: 7.9708
Batch 280, Loss: 8.3992
Batch 290, Loss: 7.9618
Batch 300, Loss: 8.2780
Batch 310, Loss: 7.9271
Batch 320, Loss: 8.3356
Batch 330, Loss: 7.9661
Batch 340, Loss: 8.0368
Batch 350, Loss: 8.3163
Batch 360, Loss: 8.0672
Batch 370, Loss: 7.9546
Batch 380, Loss: 8.1166
Batch 390, Loss: 7.7932
Epoch 8 learning rate: 0.01
Epoch 8 time: 119.02603650093079 seconds
Epoch 8 accuracy: 10.22%
Batch 10, Loss: 7.9624
Batch 20, Loss: 7.9760
Batch 30, Loss: 8.0492
Batch 40, Loss: 8.6663
Batch 50, Loss: 8.0152
Batch 60, Loss: 8.0844
Batch 70, Loss: 7.9172
Batch 80, Loss: 7.8889
Batch 90, Loss: 7.8272
Batch 100, Loss: 7.9324
Batch 110, Loss: 8.0763
Batch 120, Loss: 7.8732
Batch 130, Loss: 8.0106
Batch 140, Loss: 7.7183
Batch 150, Loss: 8.0143
Batch 160, Loss: 8.0265
Batch 170, Loss: 7.8818
Batch 180, Loss: 7.7933
Batch 190, Loss: 7.8030
Batch 200, Loss: 8.2196
Batch 210, Loss: 7.9838
Batch 220, Loss: 7.7199
Batch 230, Loss: 7.9138
Batch 240, Loss: 8.0504
Batch 250, Loss: 7.6561
Batch 260, Loss: 7.8672
Batch 270, Loss: 7.8443
Batch 280, Loss: 7.8043
Batch 290, Loss: 8.0056
Batch 300, Loss: 7.9838
Batch 310, Loss: 7.7803
Batch 320, Loss: 8.0718
Batch 330, Loss: 7.9938
Batch 340, Loss: 7.8056
Batch 350, Loss: 7.9730
Batch 360, Loss: 7.6908
Batch 370, Loss: 7.8105
Batch 380, Loss: 7.9618
Batch 390, Loss: 7.7489
Epoch 9 learning rate: 0.01
Epoch 9 time: 119.09516310691833 seconds
Epoch 9 accuracy: 10.17%
Batch 10, Loss: 7.6650
Batch 20, Loss: 7.9263
Batch 30, Loss: 7.7703
Batch 40, Loss: 7.9187
Batch 50, Loss: 7.7976
Batch 60, Loss: 7.9278
Batch 70, Loss: 7.5205
Batch 80, Loss: 7.9815
Batch 90, Loss: 7.7628
Batch 100, Loss: 7.7208
Batch 110, Loss: 8.1285
Batch 120, Loss: 7.6186
Batch 130, Loss: 7.6221
Batch 140, Loss: 7.5148
Batch 150, Loss: 7.9128
Batch 160, Loss: 7.6618
Batch 170, Loss: 7.5859
Batch 180, Loss: 7.6244
Batch 190, Loss: 7.6226
Batch 200, Loss: 7.6875
Batch 210, Loss: 7.6972
Batch 220, Loss: 7.6934
Batch 230, Loss: 7.5164
Batch 240, Loss: 7.6005
Batch 250, Loss: 7.7804
Batch 260, Loss: 7.4779
Batch 270, Loss: 7.4736
Batch 280, Loss: 7.5457
Batch 290, Loss: 7.7842
Batch 300, Loss: 7.5166
Batch 310, Loss: 7.4794
Batch 320, Loss: 7.5097
Batch 330, Loss: 7.5069
Batch 340, Loss: 7.5255
Batch 350, Loss: 7.5109
Batch 360, Loss: 7.3437
Batch 370, Loss: 7.5462
Batch 380, Loss: 7.5326
Batch 390, Loss: 7.3557
Epoch 10 learning rate: 0.01
Epoch 10 time: 119.01002168655396 seconds
Epoch 10 accuracy: 10.19%
Batch 10, Loss: 7.4363
Batch 20, Loss: 7.5166
Batch 30, Loss: 7.6142
Batch 40, Loss: 7.4459
Batch 50, Loss: 7.6634
Batch 60, Loss: 7.4633
Batch 70, Loss: 7.4349
Batch 80, Loss: 7.4266
Batch 90, Loss: 7.4325
Batch 100, Loss: 7.3930
Batch 110, Loss: 7.3988
Batch 120, Loss: 7.5772
Batch 130, Loss: 7.3347
Batch 140, Loss: 7.6026
Batch 150, Loss: 7.2131
Batch 160, Loss: 7.4025
Batch 170, Loss: 7.5943
Batch 180, Loss: 7.2158
Batch 190, Loss: 7.3285
Batch 200, Loss: 7.4091
Batch 210, Loss: 7.3240
Batch 220, Loss: 7.4107
Batch 230, Loss: 7.3601
Batch 240, Loss: 7.3417
Batch 250, Loss: 7.2405
Batch 260, Loss: 7.3309
Batch 270, Loss: 7.2422
Batch 280, Loss: 7.2795
Batch 290, Loss: 7.3658
Batch 300, Loss: 7.3666
Batch 310, Loss: 7.3527
Batch 320, Loss: 7.3405
Batch 330, Loss: 7.3824
Batch 340, Loss: 7.1376
Batch 350, Loss: 7.4483
Batch 360, Loss: 7.3186
Batch 370, Loss: 7.1602
Batch 380, Loss: 7.2578
Batch 390, Loss: 7.4943
Epoch 11 learning rate: 0.01
Epoch 11 time: 119.0913507938385 seconds
Epoch 11 accuracy: 10.21%
Batch 10, Loss: 7.1113
Batch 20, Loss: 7.5620
Batch 30, Loss: 7.4426
Batch 40, Loss: 7.3946
Batch 50, Loss: 7.1308
Batch 60, Loss: 7.2658
Batch 70, Loss: 7.0748
Batch 80, Loss: 7.2332
Batch 90, Loss: 7.0724
Batch 100, Loss: 7.2127
Batch 110, Loss: 7.0366
Batch 120, Loss: 7.3191
Batch 130, Loss: 7.1353
Batch 140, Loss: 7.0192
Batch 150, Loss: 7.0673
Batch 160, Loss: 7.3059
Batch 170, Loss: 7.0799
Batch 180, Loss: 7.2000
Batch 190, Loss: 7.4149
Batch 200, Loss: 6.9787
Batch 210, Loss: 7.0726
Batch 220, Loss: 7.2341
Batch 230, Loss: 7.0946
Batch 240, Loss: 7.0630
Batch 250, Loss: 6.9442
Batch 260, Loss: 7.0788
Batch 270, Loss: 6.9431
Batch 280, Loss: 6.9301
Batch 290, Loss: 6.9095
Batch 300, Loss: 6.9549
Batch 310, Loss: 7.0636
Batch 320, Loss: 7.1916
Batch 330, Loss: 6.9881
Batch 340, Loss: 6.9748
Batch 350, Loss: 6.9874
Batch 360, Loss: 6.7532
Batch 370, Loss: 6.8996
Batch 380, Loss: 6.9216
Batch 390, Loss: 6.8313
Epoch 12 learning rate: 0.01
Epoch 12 time: 119.11828255653381 seconds
Epoch 12 accuracy: 10.19%
Batch 10, Loss: 6.8953
Batch 20, Loss: 6.9777
Batch 30, Loss: 7.1274
Batch 40, Loss: 6.8745
Batch 50, Loss: 7.2408
Batch 60, Loss: 6.8562
Batch 70, Loss: 6.9051
Batch 80, Loss: 6.8716
Batch 90, Loss: 6.7491
Batch 100, Loss: 6.8494
Batch 110, Loss: 6.8015
Batch 120, Loss: 6.8140
Batch 130, Loss: 6.5885
Batch 140, Loss: 6.8739
Batch 150, Loss: 6.7103
Batch 160, Loss: 6.7035
Batch 170, Loss: 6.8685
Batch 180, Loss: 6.7169
Batch 190, Loss: 6.9303
Batch 200, Loss: 6.5932
Batch 210, Loss: 6.8904
Batch 220, Loss: 6.8895
Batch 230, Loss: 6.7988
Batch 240, Loss: 6.6458
Batch 250, Loss: 6.7686
Batch 260, Loss: 6.5952
Batch 270, Loss: 6.6378
Batch 280, Loss: 6.8125
Batch 290, Loss: 6.5671
Batch 300, Loss: 7.0983
Batch 310, Loss: 6.3739
Batch 320, Loss: 6.5928
Batch 330, Loss: 6.6871
Batch 340, Loss: 6.7118
Batch 350, Loss: 6.5296
Batch 360, Loss: 6.6611
Batch 370, Loss: 6.7492
Batch 380, Loss: 6.3765
Batch 390, Loss: 6.5090
Epoch 13 learning rate: 0.01
Epoch 13 time: 119.10345983505249 seconds
Epoch 13 accuracy: 10.16%
Batch 10, Loss: 6.6461
Batch 20, Loss: 6.4730
Batch 30, Loss: 6.7765
Batch 40, Loss: 6.4746
Batch 50, Loss: 6.6111
Batch 60, Loss: 6.3981
Batch 70, Loss: 6.4995
Batch 80, Loss: 6.3158
Batch 90, Loss: 6.4835
Batch 100, Loss: 6.6534
Batch 110, Loss: 6.3996
Batch 120, Loss: 6.2329
Batch 130, Loss: 6.4735
Batch 140, Loss: 6.2789
Batch 150, Loss: 6.4609
Batch 160, Loss: 6.7988
Batch 170, Loss: 6.4100
Batch 180, Loss: 6.3522
Batch 190, Loss: 6.6439
Batch 200, Loss: 6.5913
Batch 210, Loss: 6.3459
Batch 220, Loss: 6.3538
Batch 230, Loss: 6.3065
Batch 240, Loss: 6.2287
Batch 250, Loss: 6.1392
Batch 260, Loss: 6.1685
Batch 270, Loss: 6.1203
Batch 280, Loss: 6.4422
Batch 290, Loss: 6.1872
Batch 300, Loss: 6.1650
Batch 310, Loss: 6.1621
Batch 320, Loss: 6.3691
Batch 330, Loss: 6.2775
Batch 340, Loss: 6.4811
Batch 350, Loss: 6.2616
Batch 360, Loss: 6.2778
Batch 370, Loss: 6.1670
Batch 380, Loss: 6.0625
Batch 390, Loss: 6.0009
Epoch 14 learning rate: 0.01
Epoch 14 time: 119.15001893043518 seconds
Epoch 14 accuracy: 10.15%
Batch 10, Loss: 6.0261
Batch 20, Loss: 6.6298
Batch 30, Loss: 6.2468
Batch 40, Loss: 6.0119
Batch 50, Loss: 5.9991
Batch 60, Loss: 5.9694
Batch 70, Loss: 6.0194
Batch 80, Loss: 5.9722
Batch 90, Loss: 6.0469
Batch 100, Loss: 6.0519
Batch 110, Loss: 5.7890
Batch 120, Loss: 5.9334
Batch 130, Loss: 5.8392
Batch 140, Loss: 5.8781
Batch 150, Loss: 5.8222
Batch 160, Loss: 5.8150
Batch 170, Loss: 5.8043
Batch 180, Loss: 5.8121
Batch 190, Loss: 5.7365
Batch 200, Loss: 5.7466
Batch 210, Loss: 5.7218
Batch 220, Loss: 5.7630
Batch 230, Loss: 5.6342
Batch 240, Loss: 5.5964
Batch 250, Loss: 5.6751
Batch 260, Loss: 5.7772
Batch 270, Loss: 5.4653
Batch 280, Loss: 5.7195
Batch 290, Loss: 5.6698
Batch 300, Loss: 5.7886
Batch 310, Loss: 5.6289
Batch 320, Loss: 5.8618
Batch 330, Loss: 5.4032
Batch 340, Loss: 5.6120
Batch 350, Loss: 5.4054
Batch 360, Loss: 5.3439
Batch 370, Loss: 5.5849
Batch 380, Loss: 5.3709
Batch 390, Loss: 5.3922
Epoch 15 learning rate: 0.01
Epoch 15 time: 119.05090618133545 seconds
Epoch 15 accuracy: 10.25%
Batch 10, Loss: 5.2535
Batch 20, Loss: 5.1801
Batch 30, Loss: 5.2499
Batch 40, Loss: 5.1884
Batch 50, Loss: 5.3072
Batch 60, Loss: 5.2650
Batch 70, Loss: 5.0206
Batch 80, Loss: 5.2261
Batch 90, Loss: 5.3754
Batch 100, Loss: 5.1517
Batch 110, Loss: 5.0843
Batch 120, Loss: 5.1853
Batch 130, Loss: 5.1959
Batch 140, Loss: 5.0512
Batch 150, Loss: 5.2656
Batch 160, Loss: 5.0740
Batch 170, Loss: 5.0398
Batch 180, Loss: 4.9047
Batch 190, Loss: 4.7092
Batch 200, Loss: 5.1014
Batch 210, Loss: 4.9800
Batch 220, Loss: 4.9379
Batch 230, Loss: 4.7860
Batch 240, Loss: 4.7461
Batch 250, Loss: 4.8232
Batch 260, Loss: 4.6929
Batch 270, Loss: 4.5458
Batch 280, Loss: 4.7629
Batch 290, Loss: 4.7350
Batch 300, Loss: 4.5314
Batch 310, Loss: 4.7880
Batch 320, Loss: 4.4404
Batch 330, Loss: 4.6579
Batch 340, Loss: 4.3658
Batch 350, Loss: 4.4984
Batch 360, Loss: 4.7030
Batch 370, Loss: 4.4550
Batch 380, Loss: 4.3690
Batch 390, Loss: 4.5767
Epoch 16 learning rate: 0.01
Epoch 16 time: 118.88248062133789 seconds
Epoch 16 accuracy: 10.3%
Batch 10, Loss: 4.3189
Batch 20, Loss: 4.5111
Batch 30, Loss: 4.3705
Batch 40, Loss: 4.5438
Batch 50, Loss: 4.2672
Batch 60, Loss: 4.1567
Batch 70, Loss: 4.2795
Batch 80, Loss: 4.2916
Batch 90, Loss: 4.2854
Batch 100, Loss: 4.2284
Batch 110, Loss: 4.3092
Batch 120, Loss: 4.2878
Batch 130, Loss: 4.1916
Batch 140, Loss: 4.0543
Batch 150, Loss: 4.0199
Batch 160, Loss: 4.1585
Batch 170, Loss: 4.0540
Batch 180, Loss: 4.0028
Batch 190, Loss: 4.2627
Batch 200, Loss: 4.0060
Batch 210, Loss: 3.9057
Batch 220, Loss: 3.9347
Batch 230, Loss: 3.8953
Batch 240, Loss: 4.0446
Batch 250, Loss: 3.9067
Batch 260, Loss: 4.0527
Batch 270, Loss: 3.9475
Batch 280, Loss: 4.1509
Batch 290, Loss: 3.9850
Batch 300, Loss: 3.8148
Batch 310, Loss: 3.9383
Batch 320, Loss: 3.7944
Batch 330, Loss: 3.8820
Batch 340, Loss: 3.9860
Batch 350, Loss: 3.8341
Batch 360, Loss: 3.8432
Batch 370, Loss: 3.6890
Batch 380, Loss: 3.7631
Batch 390, Loss: 3.7437
Epoch 17 learning rate: 0.01
Epoch 17 time: 119.06386375427246 seconds
Epoch 17 accuracy: 10.31%
Batch 10, Loss: 3.7255
Batch 20, Loss: 3.7732
Batch 30, Loss: 3.5511
Batch 40, Loss: 3.7337
Batch 50, Loss: 3.6672
Batch 60, Loss: 3.8259
Batch 70, Loss: 3.6358
Batch 80, Loss: 3.6278
Batch 90, Loss: 3.7837
Batch 100, Loss: 3.5583
Batch 110, Loss: 3.7773
Batch 120, Loss: 3.6099
Batch 130, Loss: 3.5733
Batch 140, Loss: 3.5118
Batch 150, Loss: 3.5885
Batch 160, Loss: 3.4471
Batch 170, Loss: 3.4831
Batch 180, Loss: 3.4786
Batch 190, Loss: 3.5174
Batch 200, Loss: 3.4875
Batch 210, Loss: 3.6003
Batch 220, Loss: 3.4190
Batch 230, Loss: 3.5595
Batch 240, Loss: 3.4895
Batch 250, Loss: 3.3903
Batch 260, Loss: 3.6006
Batch 270, Loss: 3.4244
Batch 280, Loss: 3.4545
Batch 290, Loss: 3.4087
Batch 300, Loss: 3.3701
Batch 310, Loss: 3.5014
Batch 320, Loss: 3.5328
Batch 330, Loss: 3.5160
Batch 340, Loss: 3.3629
Batch 350, Loss: 3.4024
Batch 360, Loss: 3.3516
Batch 370, Loss: 3.3112
Batch 380, Loss: 3.4147
Batch 390, Loss: 3.4518
Epoch 18 learning rate: 0.01
Epoch 18 time: 118.94460964202881 seconds
Epoch 18 accuracy: 10.3%
Batch 10, Loss: 3.3348
Batch 20, Loss: 3.2635
Batch 30, Loss: 3.2297
Batch 40, Loss: 3.4738
Batch 50, Loss: 3.2984
Batch 60, Loss: 3.2151
Batch 70, Loss: 3.2630
Batch 80, Loss: 3.2758
Batch 90, Loss: 3.1728
Batch 100, Loss: 3.1850
Batch 110, Loss: 3.2356
Batch 120, Loss: 3.2445
Batch 130, Loss: 3.2454
Batch 140, Loss: 3.3918
Batch 150, Loss: 3.2426
Batch 160, Loss: 3.1438
Batch 170, Loss: 3.1564
Batch 180, Loss: 3.3115
Batch 190, Loss: 3.1370
Batch 200, Loss: 3.1612
Batch 210, Loss: 3.1608
Batch 220, Loss: 3.1396
Batch 230, Loss: 3.0866
Batch 240, Loss: 3.1102
Batch 250, Loss: 3.2737
Batch 260, Loss: 3.1528
Batch 270, Loss: 3.2150
Batch 280, Loss: 3.2424
Batch 290, Loss: 3.0920
Batch 300, Loss: 3.0968
Batch 310, Loss: 3.0987
Batch 320, Loss: 3.0874
Batch 330, Loss: 2.9977
Batch 340, Loss: 3.0952
Batch 350, Loss: 3.1388
Batch 360, Loss: 3.0454
Batch 370, Loss: 3.0047
Batch 380, Loss: 2.9630
Batch 390, Loss: 3.2530
Epoch 19 learning rate: 0.01
Epoch 19 time: 119.0124192237854 seconds
Epoch 19 accuracy: 10.22%
Batch 10, Loss: 3.0115
Batch 20, Loss: 3.1356
Batch 30, Loss: 3.1005
Batch 40, Loss: 3.0240
Batch 50, Loss: 3.0281
Batch 60, Loss: 2.9721
Batch 70, Loss: 2.9959
Batch 80, Loss: 2.9057
Batch 90, Loss: 2.9898
Batch 100, Loss: 2.9669
Batch 110, Loss: 2.9306
Batch 120, Loss: 2.9233
Batch 130, Loss: 2.9990
Batch 140, Loss: 2.8340
Batch 150, Loss: 2.9246
Batch 160, Loss: 2.8943
Batch 170, Loss: 2.9056
Batch 180, Loss: 2.9394
Batch 190, Loss: 2.9377
Batch 200, Loss: 2.9235
Batch 210, Loss: 2.8790
Batch 220, Loss: 2.7309
Batch 230, Loss: 2.8980
Batch 240, Loss: 2.8258
Batch 250, Loss: 2.8321
Batch 260, Loss: 2.8355
Batch 270, Loss: 2.7521
Batch 280, Loss: 2.8626
Batch 290, Loss: 2.8039
Batch 300, Loss: 2.7655
Batch 310, Loss: 2.8134
Batch 320, Loss: 2.7777
Batch 330, Loss: 2.7981
Batch 340, Loss: 2.7757
Batch 350, Loss: 2.8272
Batch 360, Loss: 2.7616
Batch 370, Loss: 2.8973
Batch 380, Loss: 2.7040
Batch 390, Loss: 2.6945
Epoch 20 learning rate: 0.01
Epoch 20 time: 119.00007605552673 seconds
Epoch 20 accuracy: 10.3%
Total training time: 2390.61270904541 seconds
