The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 934.1079
Batch 20, Loss: 2617.8316
Batch 30, Loss: 607.5703
Batch 40, Loss: 636.3175
Batch 50, Loss: 278.7038
Batch 60, Loss: 148.0725
Batch 70, Loss: 85.7214
Batch 80, Loss: 78.3144
Batch 90, Loss: 60.4692
Batch 100, Loss: 60.9850
Batch 110, Loss: 68.5005
Batch 120, Loss: 57.6194
Batch 130, Loss: 65.2846
Batch 140, Loss: 281.4771
Batch 150, Loss: 552.1223
Batch 160, Loss: 566.2563
Batch 170, Loss: 1520.5267
Batch 180, Loss: 2550.2492
Batch 190, Loss: 1079.8151
Batch 200, Loss: 519.5158
Batch 210, Loss: 282.6862
Batch 220, Loss: 314.2252
Batch 230, Loss: 130.4872
Batch 240, Loss: 105.6127
Batch 250, Loss: 105.8621
Batch 260, Loss: 93.9422
Batch 270, Loss: 79.4439
Batch 280, Loss: 67.0296
Batch 290, Loss: 76.9495
Batch 300, Loss: 68.1647
Batch 310, Loss: 53.5894
Batch 320, Loss: 63.9569
Batch 330, Loss: 58.6947
Batch 340, Loss: 59.3955
Batch 350, Loss: 56.9288
Batch 360, Loss: 55.9534
Batch 370, Loss: 56.3945
Batch 380, Loss: 48.0928
Batch 390, Loss: 48.4546
Epoch 1 learning rate: 0.2
Epoch 1 time: 127.53769135475159 seconds
Epoch 1 accuracy: 10.3%
Batch 10, Loss: 47.6774
Batch 20, Loss: 45.0722
Batch 30, Loss: 49.5841
Batch 40, Loss: 46.4541
Batch 50, Loss: 44.8549
Batch 60, Loss: 45.3894
Batch 70, Loss: 45.2374
Batch 80, Loss: 42.4227
Batch 90, Loss: 40.2328
Batch 100, Loss: 41.4130
Batch 110, Loss: 41.1028
Batch 120, Loss: 39.0627
Batch 130, Loss: 39.7843
Batch 140, Loss: 39.8455
Batch 150, Loss: 38.5399
Batch 160, Loss: 38.3834
Batch 170, Loss: 37.9751
Batch 180, Loss: 36.8372
Batch 190, Loss: 37.3709
Batch 200, Loss: 35.4677
Batch 210, Loss: 36.0340
Batch 220, Loss: 36.1968
Batch 230, Loss: 34.9771
Batch 240, Loss: 34.9550
Batch 250, Loss: 34.0056
Batch 260, Loss: 33.9141
Batch 270, Loss: 33.2325
Batch 280, Loss: 32.0867
Batch 290, Loss: 31.7119
Batch 300, Loss: 31.8606
Batch 310, Loss: 31.8990
Batch 320, Loss: 31.8444
Batch 330, Loss: 31.5134
Batch 340, Loss: 30.7738
Batch 350, Loss: 29.4368
Batch 360, Loss: 30.1520
Batch 370, Loss: 29.4732
Batch 380, Loss: 28.7075
Batch 390, Loss: 29.1803
Epoch 2 learning rate: 0.2
Epoch 2 time: 118.93225383758545 seconds
Epoch 2 accuracy: 10.18%
Batch 10, Loss: 29.0020
Batch 20, Loss: 28.2080
Batch 30, Loss: 27.6371
Batch 40, Loss: 27.4204
Batch 50, Loss: 26.8587
Batch 60, Loss: 26.9497
Batch 70, Loss: 27.0239
Batch 80, Loss: 26.2972
Batch 90, Loss: 26.2260
Batch 100, Loss: 25.3433
Batch 110, Loss: 25.3111
Batch 120, Loss: 25.7016
Batch 130, Loss: 24.7974
Batch 140, Loss: 24.9788
Batch 150, Loss: 24.4329
Batch 160, Loss: 24.2833
Batch 170, Loss: 23.9660
Batch 180, Loss: 23.3264
Batch 190, Loss: 23.0435
Batch 200, Loss: 22.7377
Batch 210, Loss: 23.0124
Batch 220, Loss: 22.8311
Batch 230, Loss: 22.5917
Batch 240, Loss: 22.1256
Batch 250, Loss: 21.6417
Batch 260, Loss: 21.3483
Batch 270, Loss: 21.7900
Batch 280, Loss: 21.2103
Batch 290, Loss: 20.7433
Batch 300, Loss: 20.8803
Batch 310, Loss: 20.7634
Batch 320, Loss: 20.5180
Batch 330, Loss: 20.2087
Batch 340, Loss: 19.8505
Batch 350, Loss: 19.9368
Batch 360, Loss: 19.5926
Batch 370, Loss: 19.1015
Batch 380, Loss: 18.8287
Batch 390, Loss: 18.8076
Epoch 3 learning rate: 0.2
Epoch 3 time: 118.89904856681824 seconds
Epoch 3 accuracy: 10.2%
Batch 10, Loss: 18.0956
Batch 20, Loss: 18.5228
Batch 30, Loss: 18.2077
Batch 40, Loss: 18.2726
Batch 50, Loss: 17.6796
Batch 60, Loss: 17.4998
Batch 70, Loss: 17.0262
Batch 80, Loss: 17.1139
Batch 90, Loss: 16.8436
Batch 100, Loss: 17.0908
Batch 110, Loss: 16.4991
Batch 120, Loss: 16.1713
Batch 130, Loss: 16.1294
Batch 140, Loss: 16.0026
Batch 150, Loss: 15.7430
Batch 160, Loss: 15.9732
Batch 170, Loss: 15.5330
Batch 180, Loss: 15.5308
Batch 190, Loss: 15.4583
Batch 200, Loss: 15.3300
Batch 210, Loss: 14.7475
Batch 220, Loss: 14.5752
Batch 230, Loss: 14.6537
Batch 240, Loss: 14.4184
Batch 250, Loss: 14.2770
Batch 260, Loss: 14.0644
Batch 270, Loss: 13.9680
Batch 280, Loss: 13.7749
Batch 290, Loss: 13.4400
Batch 300, Loss: 13.5851
Batch 310, Loss: 13.1988
Batch 320, Loss: 12.9414
Batch 330, Loss: 12.9731
Batch 340, Loss: 12.8271
Batch 350, Loss: 12.5818
Batch 360, Loss: 12.5362
Batch 370, Loss: 12.4184
Batch 380, Loss: 12.5112
Batch 390, Loss: 12.1542
Epoch 4 learning rate: 0.2
Epoch 4 time: 118.89654350280762 seconds
Epoch 4 accuracy: 10.23%
Batch 10, Loss: 11.8830
Batch 20, Loss: 11.7536
Batch 30, Loss: 11.6981
Batch 40, Loss: 11.6983
Batch 50, Loss: 11.4328
Batch 60, Loss: 11.2971
Batch 70, Loss: 10.9925
Batch 80, Loss: 10.9936
Batch 90, Loss: 10.7810
Batch 100, Loss: 10.8564
Batch 110, Loss: 10.2608
Batch 120, Loss: 10.4518
Batch 130, Loss: 10.2990
Batch 140, Loss: 10.0519
Batch 150, Loss: 10.1925
Batch 160, Loss: 9.8546
Batch 170, Loss: 10.0048
Batch 180, Loss: 9.6865
Batch 190, Loss: 9.4921
Batch 200, Loss: 9.4123
Batch 210, Loss: 9.1410
Batch 220, Loss: 9.1249
Batch 230, Loss: 9.0780
Batch 240, Loss: 8.8448
Batch 250, Loss: 8.6835
Batch 260, Loss: 8.5421
Batch 270, Loss: 8.4031
Batch 280, Loss: 8.1246
Batch 290, Loss: 8.4284
Batch 300, Loss: 8.0073
Batch 310, Loss: 8.0161
Batch 320, Loss: 7.6385
Batch 330, Loss: 7.7066
Batch 340, Loss: 7.4212
Batch 350, Loss: 7.1948
Batch 360, Loss: 7.1273
Batch 370, Loss: 7.0467
Batch 380, Loss: 6.7455
Batch 390, Loss: 6.6512
Epoch 5 learning rate: 0.2
Epoch 5 time: 118.87223172187805 seconds
Epoch 5 accuracy: 10.67%
Batch 10, Loss: 6.1785
Batch 20, Loss: 5.7675
Batch 30, Loss: 5.3420
Batch 40, Loss: 4.6494
Batch 50, Loss: 4.1481
Batch 60, Loss: 3.6706
Batch 70, Loss: 3.5422
Batch 80, Loss: 3.1679
Batch 90, Loss: 3.1589
Batch 100, Loss: 3.0312
Batch 110, Loss: 2.8095
Batch 120, Loss: 2.8005
Batch 130, Loss: 2.7010
Batch 140, Loss: 2.6192
Batch 150, Loss: 2.5786
Batch 160, Loss: 2.5061
Batch 170, Loss: 2.4293
Batch 180, Loss: 2.3225
Batch 190, Loss: 2.3521
Batch 200, Loss: 2.3509
Batch 210, Loss: 2.2567
Batch 220, Loss: 2.1778
Batch 230, Loss: 2.1659
Batch 240, Loss: 2.1530
Batch 250, Loss: 2.0437
Batch 260, Loss: 2.0618
Batch 270, Loss: 2.0371
Batch 280, Loss: 1.9875
Batch 290, Loss: 1.9969
Batch 300, Loss: 1.9288
Batch 310, Loss: 1.9444
Batch 320, Loss: 1.9039
Batch 330, Loss: 1.8735
Batch 340, Loss: 1.8559
Batch 350, Loss: 1.8781
Batch 360, Loss: 1.8299
Batch 370, Loss: 1.8095
Batch 380, Loss: 1.8146
Batch 390, Loss: 1.7938
Epoch 6 learning rate: 0.2
Epoch 6 time: 118.91217613220215 seconds
Epoch 6 accuracy: 10.66%
Batch 10, Loss: 1.7684
Batch 20, Loss: 1.8052
Batch 30, Loss: 1.8099
Batch 40, Loss: 1.7729
Batch 50, Loss: 1.7914
Batch 60, Loss: 1.7700
Batch 70, Loss: 1.7670
Batch 80, Loss: 1.7638
Batch 90, Loss: 1.7564
Batch 100, Loss: 1.7666
Batch 110, Loss: 1.7567
Batch 120, Loss: 1.7647
Batch 130, Loss: 1.7601
Batch 140, Loss: 1.7514
Batch 150, Loss: 1.7659
Batch 160, Loss: 1.7558
Batch 170, Loss: 1.7594
Batch 180, Loss: 1.7584
Batch 190, Loss: 1.7494
Batch 200, Loss: 1.7529
Batch 210, Loss: 1.7511
Batch 220, Loss: 1.7553
Batch 230, Loss: 1.7591
Batch 240, Loss: 1.7481
Batch 250, Loss: 1.7592
Batch 260, Loss: 1.7480
Batch 270, Loss: 1.7536
Batch 280, Loss: 1.7627
Batch 290, Loss: 1.7523
Batch 300, Loss: 1.7559
Batch 310, Loss: 1.7537
Batch 320, Loss: 1.7523
Batch 330, Loss: 1.7527
Batch 340, Loss: 1.7532
Batch 350, Loss: 1.7539
Batch 360, Loss: 1.7524
Batch 370, Loss: 1.7557
Batch 380, Loss: 1.7525
Batch 390, Loss: 1.7568
Epoch 7 learning rate: 0.2
Epoch 7 time: 118.843337059021 seconds
Epoch 7 accuracy: 11.18%
Batch 10, Loss: 1.7528
Batch 20, Loss: 1.7561
Batch 30, Loss: 1.7539
Batch 40, Loss: 1.7585
Batch 50, Loss: 1.7545
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7513
Batch 80, Loss: 1.7573
Batch 90, Loss: 1.7601
Batch 100, Loss: 1.7545
Batch 110, Loss: 1.7533
Batch 120, Loss: 1.7532
Batch 130, Loss: 1.7555
Batch 140, Loss: 1.7535
Batch 150, Loss: 1.7590
Batch 160, Loss: 1.7544
Batch 170, Loss: 1.7532
Batch 180, Loss: 1.7544
Batch 190, Loss: 1.7541
Batch 200, Loss: 1.7546
Batch 210, Loss: 1.7534
Batch 220, Loss: 1.7548
Batch 230, Loss: 1.7549
Batch 240, Loss: 1.7543
Batch 250, Loss: 1.7579
Batch 260, Loss: 1.7540
Batch 270, Loss: 1.7553
Batch 280, Loss: 1.7551
Batch 290, Loss: 1.7517
Batch 300, Loss: 1.7539
Batch 310, Loss: 1.7573
Batch 320, Loss: 1.7556
Batch 330, Loss: 1.7545
Batch 340, Loss: 1.7532
Batch 350, Loss: 1.7549
Batch 360, Loss: 1.7548
Batch 370, Loss: 1.7554
Batch 380, Loss: 1.7569
Batch 390, Loss: 1.7537
Epoch 8 learning rate: 0.2
Epoch 8 time: 118.7867021560669 seconds
Epoch 8 accuracy: 10.2%
Batch 10, Loss: 1.7560
Batch 20, Loss: 1.7553
Batch 30, Loss: 1.7540
Batch 40, Loss: 1.7528
Batch 50, Loss: 1.7552
Batch 60, Loss: 1.7549
Batch 70, Loss: 1.7559
Batch 80, Loss: 1.7566
Batch 90, Loss: 1.7552
Batch 100, Loss: 1.7560
Batch 110, Loss: 1.7551
Batch 120, Loss: 1.7571
Batch 130, Loss: 1.7556
Batch 140, Loss: 1.7539
Batch 150, Loss: 1.7543
Batch 160, Loss: 1.7567
Batch 170, Loss: 1.7560
Batch 180, Loss: 1.7536
Batch 190, Loss: 1.7566
Batch 200, Loss: 1.7566
Batch 210, Loss: 1.7550
Batch 220, Loss: 1.7551
Batch 230, Loss: 1.7563
Batch 240, Loss: 1.7545
Batch 250, Loss: 1.7546
Batch 260, Loss: 1.7570
Batch 270, Loss: 1.7552
Batch 280, Loss: 1.7566
Batch 290, Loss: 1.7557
Batch 300, Loss: 1.7556
Batch 310, Loss: 1.7565
Batch 320, Loss: 1.7560
Batch 330, Loss: 1.7554
Batch 340, Loss: 1.7557
Batch 350, Loss: 1.7557
Batch 360, Loss: 1.7551
Batch 370, Loss: 1.7564
Batch 380, Loss: 1.7560
Batch 390, Loss: 1.7571
Epoch 9 learning rate: 0.2
Epoch 9 time: 118.81145596504211 seconds
Epoch 9 accuracy: 10.15%
Batch 10, Loss: 1.7562
Batch 20, Loss: 1.7551
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7560
Batch 50, Loss: 1.7565
Batch 60, Loss: 1.7566
Batch 70, Loss: 1.7562
Batch 80, Loss: 1.7566
Batch 90, Loss: 1.7561
Batch 100, Loss: 1.7559
Batch 110, Loss: 1.7572
Batch 120, Loss: 1.7562
Batch 130, Loss: 1.7565
Batch 140, Loss: 1.7567
Batch 150, Loss: 1.7571
Batch 160, Loss: 1.7559
Batch 170, Loss: 1.7556
Batch 180, Loss: 1.7563
Batch 190, Loss: 1.7570
Batch 200, Loss: 1.7559
Batch 210, Loss: 1.7562
Batch 220, Loss: 1.7582
Batch 230, Loss: 1.7567
Batch 240, Loss: 1.7561
Batch 250, Loss: 1.7567
Batch 260, Loss: 1.7571
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7566
Batch 290, Loss: 1.7569
Batch 300, Loss: 1.7575
Batch 310, Loss: 1.7569
Batch 320, Loss: 1.7555
Batch 330, Loss: 1.7562
Batch 340, Loss: 1.7583
Batch 350, Loss: 1.7574
Batch 360, Loss: 1.7574
Batch 370, Loss: 1.7566
Batch 380, Loss: 1.7575
Batch 390, Loss: 1.7576
Epoch 10 learning rate: 0.2
Epoch 10 time: 118.80717301368713 seconds
Epoch 10 accuracy: 10.02%
Batch 10, Loss: 1.7568
Batch 20, Loss: 1.7568
Batch 30, Loss: 1.7568
Batch 40, Loss: 1.7570
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7574
Batch 70, Loss: 1.7571
Batch 80, Loss: 1.7564
Batch 90, Loss: 1.7570
Batch 100, Loss: 1.7568
Batch 110, Loss: 1.7575
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7573
Batch 140, Loss: 1.7573
Batch 150, Loss: 1.7570
Batch 160, Loss: 1.7576
Batch 170, Loss: 1.7573
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7569
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7566
Batch 230, Loss: 1.7584
Batch 240, Loss: 1.7575
Batch 250, Loss: 1.7573
Batch 260, Loss: 1.7571
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7576
Batch 300, Loss: 1.7578
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7576
Batch 330, Loss: 1.7568
Batch 340, Loss: 1.7591
Batch 350, Loss: 1.7579
Batch 360, Loss: 1.7572
Batch 370, Loss: 1.7578
Batch 380, Loss: 1.7568
Batch 390, Loss: 1.7583
Epoch 11 learning rate: 0.2
Epoch 11 time: 118.78487157821655 seconds
Epoch 11 accuracy: 9.99%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7573
Batch 30, Loss: 1.7572
Batch 40, Loss: 1.7583
Batch 50, Loss: 1.7573
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7578
Batch 80, Loss: 1.7576
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7569
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7583
Batch 140, Loss: 1.7574
Batch 150, Loss: 1.7576
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7573
Batch 190, Loss: 1.7584
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7583
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7576
Batch 260, Loss: 1.7569
Batch 270, Loss: 1.7582
Batch 280, Loss: 1.7585
Batch 290, Loss: 1.7585
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7576
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7582
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7577
Epoch 12 learning rate: 0.2
Epoch 12 time: 118.75835251808167 seconds
Epoch 12 accuracy: 10.71%
Batch 10, Loss: 1.7584
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7581
Batch 50, Loss: 1.7576
Batch 60, Loss: 1.7581
Batch 70, Loss: 1.7581
Batch 80, Loss: 1.7574
Batch 90, Loss: 1.7581
Batch 100, Loss: 1.7575
Batch 110, Loss: 1.7568
Batch 120, Loss: 1.7581
Batch 130, Loss: 1.7569
Batch 140, Loss: 1.7572
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7577
Batch 170, Loss: 1.7585
Batch 180, Loss: 1.7582
Batch 190, Loss: 1.7589
Batch 200, Loss: 1.7583
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7577
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7572
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7584
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7577
Batch 300, Loss: 1.7585
Batch 310, Loss: 1.7582
Batch 320, Loss: 1.7586
Batch 330, Loss: 1.7570
Batch 340, Loss: 1.7573
Batch 350, Loss: 1.7587
Batch 360, Loss: 1.7593
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7586
Batch 390, Loss: 1.7580
Epoch 13 learning rate: 0.2
Epoch 13 time: 118.7267906665802 seconds
Epoch 13 accuracy: 10.02%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7581
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7583
Batch 50, Loss: 1.7580
Batch 60, Loss: 1.7581
Batch 70, Loss: 1.7580
Batch 80, Loss: 1.7588
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7582
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7582
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7584
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7574
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7584
Batch 200, Loss: 1.7576
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7579
Batch 230, Loss: 1.7583
Batch 240, Loss: 1.7581
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7586
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7585
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7586
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7582
Batch 340, Loss: 1.7574
Batch 350, Loss: 1.7584
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7589
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7576
Epoch 14 learning rate: 0.2
Epoch 14 time: 118.78839230537415 seconds
Epoch 14 accuracy: 10.03%
Batch 10, Loss: 1.7580
Batch 20, Loss: 1.7581
Batch 30, Loss: 1.7583
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7581
Batch 60, Loss: 1.7586
Batch 70, Loss: 1.7582
Batch 80, Loss: 1.7585
Batch 90, Loss: 1.7582
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7582
Batch 120, Loss: 1.7576
Batch 130, Loss: 1.7584
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7583
Batch 160, Loss: 1.7580
Batch 170, Loss: 1.7585
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7581
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7575
Batch 230, Loss: 1.7591
Batch 240, Loss: 1.7573
Batch 250, Loss: 1.7588
Batch 260, Loss: 1.7582
Batch 270, Loss: 1.7570
Batch 280, Loss: 1.7583
Batch 290, Loss: 1.7582
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7584
Batch 320, Loss: 1.7580
Batch 330, Loss: 1.7582
Batch 340, Loss: 1.7579
Batch 350, Loss: 1.7589
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7576
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7587
Epoch 15 learning rate: 0.2
Epoch 15 time: 118.69306516647339 seconds
Epoch 15 accuracy: 10.0%
Batch 10, Loss: 1.7585
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7584
Batch 50, Loss: 1.7573
Batch 60, Loss: 1.7591
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7584
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7588
Batch 120, Loss: 1.7582
Batch 130, Loss: 1.7586
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7583
Batch 160, Loss: 1.7577
Batch 170, Loss: 1.7571
Batch 180, Loss: 1.7572
Batch 190, Loss: 1.7588
Batch 200, Loss: 1.7587
Batch 210, Loss: 1.7587
Batch 220, Loss: 1.7576
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7583
Batch 260, Loss: 1.7580
Batch 270, Loss: 1.7582
Batch 280, Loss: 1.7582
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7582
Batch 310, Loss: 1.7579
Batch 320, Loss: 1.7586
Batch 330, Loss: 1.7584
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7583
Batch 360, Loss: 1.7582
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7576
Batch 390, Loss: 1.7579
Epoch 16 learning rate: 0.2
Epoch 16 time: 118.74847555160522 seconds
Epoch 16 accuracy: 10.0%
Batch 10, Loss: 1.7573
Batch 20, Loss: 1.7582
Batch 30, Loss: 1.7581
Batch 40, Loss: 1.7586
Batch 50, Loss: 1.7587
Batch 60, Loss: 1.7590
Batch 70, Loss: 1.7584
Batch 80, Loss: 1.7581
Batch 90, Loss: 1.7572
Batch 100, Loss: 1.7573
Batch 110, Loss: 1.7587
Batch 120, Loss: 1.7584
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7575
Batch 160, Loss: 1.7588
Batch 170, Loss: 1.7584
Batch 180, Loss: 1.7586
Batch 190, Loss: 1.7583
Batch 200, Loss: 1.7579
Batch 210, Loss: 1.7583
Batch 220, Loss: 1.7575
Batch 230, Loss: 1.7581
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7575
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7587
Batch 280, Loss: 1.7577
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7582
Batch 330, Loss: 1.7575
Batch 340, Loss: 1.7584
Batch 350, Loss: 1.7590
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7582
Batch 380, Loss: 1.7583
Batch 390, Loss: 1.7578
Epoch 17 learning rate: 0.2
Epoch 17 time: 118.73110866546631 seconds
Epoch 17 accuracy: 10.0%
Batch 10, Loss: 1.7573
Batch 20, Loss: 1.7582
Batch 30, Loss: 1.7567
Batch 40, Loss: 1.7592
Batch 50, Loss: 1.7574
Batch 60, Loss: 1.7588
Batch 70, Loss: 1.7583
Batch 80, Loss: 1.7582
Batch 90, Loss: 1.7585
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7583
Batch 130, Loss: 1.7572
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7566
Batch 160, Loss: 1.7589
Batch 170, Loss: 1.7573
Batch 180, Loss: 1.7578
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7586
Batch 210, Loss: 1.7586
Batch 220, Loss: 1.7586
Batch 230, Loss: 1.7576
Batch 240, Loss: 1.7582
Batch 250, Loss: 1.7583
Batch 260, Loss: 1.7578
Batch 270, Loss: 1.7586
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7586
Batch 300, Loss: 1.7593
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7582
Batch 330, Loss: 1.7584
Batch 340, Loss: 1.7582
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7577
Batch 370, Loss: 1.7577
Batch 380, Loss: 1.7584
Batch 390, Loss: 1.7578
Epoch 18 learning rate: 0.2
Epoch 18 time: 118.72134709358215 seconds
Epoch 18 accuracy: 10.0%
Batch 10, Loss: 1.7584
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7582
Batch 40, Loss: 1.7581
Batch 50, Loss: 1.7578
Batch 60, Loss: 1.7583
Batch 70, Loss: 1.7583
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7582
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7584
Batch 120, Loss: 1.7573
Batch 130, Loss: 1.7587
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7587
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7583
Batch 190, Loss: 1.7572
Batch 200, Loss: 1.7573
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7588
Batch 230, Loss: 1.7588
Batch 240, Loss: 1.7586
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7585
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7589
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7577
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7583
Batch 340, Loss: 1.7587
Batch 350, Loss: 1.7580
Batch 360, Loss: 1.7584
Batch 370, Loss: 1.7583
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7580
Epoch 19 learning rate: 0.2
Epoch 19 time: 118.70987367630005 seconds
Epoch 19 accuracy: 10.0%
Batch 10, Loss: 1.7572
Batch 20, Loss: 1.7568
Batch 30, Loss: 1.7569
Batch 40, Loss: 1.7587
Batch 50, Loss: 1.7589
Batch 60, Loss: 1.7592
Batch 70, Loss: 1.7582
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7586
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7581
Batch 120, Loss: 1.7582
Batch 130, Loss: 1.7580
Batch 140, Loss: 1.7584
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7582
Batch 170, Loss: 1.7572
Batch 180, Loss: 1.7588
Batch 190, Loss: 1.7590
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7576
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7591
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7587
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7585
Batch 300, Loss: 1.7587
Batch 310, Loss: 1.7581
Batch 320, Loss: 1.7581
Batch 330, Loss: 1.7583
Batch 340, Loss: 1.7573
Batch 350, Loss: 1.7582
Batch 360, Loss: 1.7582
Batch 370, Loss: 1.7584
Batch 380, Loss: 1.7585
Batch 390, Loss: 1.7580
Epoch 20 learning rate: 0.2
Epoch 20 time: 118.7721631526947 seconds
Epoch 20 accuracy: 10.0%
Total training time: 2384.751838207245 seconds
