The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:216: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM/basicaug/lr-0.05/batchsize-128/2024-08-06-14:31:56
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 475.8165
Batch 200, Loss: 30.9181
Batch 300, Loss: 13.2356
Epoch 1 learning rate: 0.05
Epoch 1 time: 127.93131470680237 seconds
Epoch 1 accuracy: 10.29%
Batch 100, Loss: 9.4774
Batch 200, Loss: 8.5548
Batch 300, Loss: 8.1484
Epoch 2 learning rate: 0.05
Epoch 2 time: 118.98349952697754 seconds
Epoch 2 accuracy: 10.19%
Batch 100, Loss: 7.4998
Batch 200, Loss: 7.1587
Batch 300, Loss: 6.9660
Epoch 3 learning rate: 0.05
Epoch 3 time: 118.94456315040588 seconds
Epoch 3 accuracy: 7.06%
Batch 100, Loss: 6.5025
Batch 200, Loss: 6.2808
Batch 300, Loss: 6.0181
Epoch 4 learning rate: 0.05
Epoch 4 time: 119.00059342384338 seconds
Epoch 4 accuracy: 6.82%
Batch 100, Loss: 5.4875
Batch 200, Loss: 5.1673
Batch 300, Loss: 4.7125
Epoch 5 learning rate: 0.05
Epoch 5 time: 118.93010783195496 seconds
Epoch 5 accuracy: 6.84%
Batch 100, Loss: 3.3796
Batch 200, Loss: 2.8347
Batch 300, Loss: 2.6052
Epoch 6 learning rate: 0.05
Epoch 6 time: 118.92733979225159 seconds
Epoch 6 accuracy: 11.11%
Batch 100, Loss: 2.3149
Batch 200, Loss: 2.2345
Batch 300, Loss: 2.1765
Epoch 7 learning rate: 0.05
Epoch 7 time: 118.91783380508423 seconds
Epoch 7 accuracy: 14.0%
Batch 100, Loss: 2.0657
Batch 200, Loss: 2.0581
Batch 300, Loss: 2.0156
Epoch 8 learning rate: 0.05
Epoch 8 time: 118.85812568664551 seconds
Epoch 8 accuracy: 12.86%
Batch 100, Loss: 1.9733
Batch 200, Loss: 1.9486
Batch 300, Loss: 1.9334
Epoch 9 learning rate: 0.05
Epoch 9 time: 118.82949566841125 seconds
Epoch 9 accuracy: 13.89%
Batch 100, Loss: 1.9101
Batch 200, Loss: 1.8846
Batch 300, Loss: 1.8878
Epoch 10 learning rate: 0.05
Epoch 10 time: 118.82108306884766 seconds
Epoch 10 accuracy: 14.14%
Batch 100, Loss: 1.8592
Batch 200, Loss: 1.8398
Batch 300, Loss: 1.8239
Epoch 11 learning rate: 0.05
Epoch 11 time: 118.83205246925354 seconds
Epoch 11 accuracy: 14.3%
Batch 100, Loss: 1.8101
Batch 200, Loss: 1.7888
Batch 300, Loss: 1.7788
Epoch 12 learning rate: 0.05
Epoch 12 time: 118.84035754203796 seconds
Epoch 12 accuracy: 14.15%
Batch 100, Loss: 1.7609
Batch 200, Loss: 1.7564
Batch 300, Loss: 1.7508
Epoch 13 learning rate: 0.05
Epoch 13 time: 118.81399416923523 seconds
Epoch 13 accuracy: 14.85%
Batch 100, Loss: 1.7440
Batch 200, Loss: 1.7409
Batch 300, Loss: 1.7410
Epoch 14 learning rate: 0.05
Epoch 14 time: 118.80738973617554 seconds
Epoch 14 accuracy: 15.16%
Batch 100, Loss: 1.7368
Batch 200, Loss: 1.7378
Batch 300, Loss: 1.7357
Epoch 15 learning rate: 0.05
Epoch 15 time: 118.79369926452637 seconds
Epoch 15 accuracy: 14.9%
Batch 100, Loss: 1.7373
Batch 200, Loss: 1.7350
Batch 300, Loss: 1.7365
Epoch 16 learning rate: 0.05
Epoch 16 time: 118.81322312355042 seconds
Epoch 16 accuracy: 15.11%
Batch 100, Loss: 1.7373
Batch 200, Loss: 1.7360
Batch 300, Loss: 1.7358
Epoch 17 learning rate: 0.05
Epoch 17 time: 118.83096528053284 seconds
Epoch 17 accuracy: 15.0%
Batch 100, Loss: 1.7365
Batch 200, Loss: 1.7350
Batch 300, Loss: 1.7348
Epoch 18 learning rate: 0.05
Epoch 18 time: 118.83979272842407 seconds
Epoch 18 accuracy: 15.03%
Batch 100, Loss: 1.7345
Batch 200, Loss: 1.7358
Batch 300, Loss: 1.7380
Epoch 19 learning rate: 0.05
Epoch 19 time: 118.79437708854675 seconds
Epoch 19 accuracy: 14.8%
Batch 100, Loss: 1.7350
Batch 200, Loss: 1.7341
Batch 300, Loss: 1.7385
Epoch 20 learning rate: 0.05
Epoch 20 time: 118.76666617393494 seconds
Epoch 20 accuracy: 14.8%
Batch 100, Loss: 1.7369
Batch 200, Loss: 1.7351
Batch 300, Loss: 1.7376
Epoch 21 learning rate: 0.05
Epoch 21 time: 118.78951573371887 seconds
Epoch 21 accuracy: 14.49%
Batch 100, Loss: 1.7375
Batch 200, Loss: 1.7348
Batch 300, Loss: 1.7366
Epoch 22 learning rate: 0.05
Epoch 22 time: 118.80152010917664 seconds
Epoch 22 accuracy: 14.92%
Batch 100, Loss: 1.7370
Batch 200, Loss: 1.7387
Batch 300, Loss: 1.7364
Epoch 23 learning rate: 0.05
Epoch 23 time: 118.8254280090332 seconds
Epoch 23 accuracy: 14.22%
Batch 100, Loss: 1.7392
Batch 200, Loss: 1.7380
Batch 300, Loss: 1.7374
Epoch 24 learning rate: 0.05
Epoch 24 time: 118.83832693099976 seconds
Epoch 24 accuracy: 14.51%
Batch 100, Loss: 1.7400
Batch 200, Loss: 1.7399
Batch 300, Loss: 1.7392
Epoch 25 learning rate: 0.05
Epoch 25 time: 118.78850960731506 seconds
Epoch 25 accuracy: 14.9%
Batch 100, Loss: 1.7399
Batch 200, Loss: 1.7410
Batch 300, Loss: 1.7391
Epoch 26 learning rate: 0.05
Epoch 26 time: 118.82726621627808 seconds
Epoch 26 accuracy: 14.69%
Batch 100, Loss: 1.7400
Batch 200, Loss: 1.7408
Batch 300, Loss: 1.7408
Epoch 27 learning rate: 0.05
Epoch 27 time: 118.84307074546814 seconds
Epoch 27 accuracy: 13.68%
Batch 100, Loss: 1.7419
Batch 200, Loss: 1.7431
Batch 300, Loss: 1.7420
Epoch 28 learning rate: 0.05
Epoch 28 time: 118.83536195755005 seconds
Epoch 28 accuracy: 13.72%
Batch 100, Loss: 1.7438
Batch 200, Loss: 1.7435
Batch 300, Loss: 1.7431
Epoch 29 learning rate: 0.05
Epoch 29 time: 118.80310845375061 seconds
Epoch 29 accuracy: 13.01%
Batch 100, Loss: 1.7441
Batch 200, Loss: 1.7444
Batch 300, Loss: 1.7459
Epoch 30 learning rate: 0.05
Epoch 30 time: 118.88486242294312 seconds
Epoch 30 accuracy: 13.37%
Batch 100, Loss: 1.7460
Batch 200, Loss: 1.7458
Batch 300, Loss: 1.7464
Epoch 31 learning rate: 0.05
Epoch 31 time: 118.85685086250305 seconds
Epoch 31 accuracy: 13.94%
Batch 100, Loss: 1.7460
Batch 200, Loss: 1.7481
Batch 300, Loss: 1.7471
Epoch 32 learning rate: 0.05
Epoch 32 time: 118.79962539672852 seconds
Epoch 32 accuracy: 13.6%
Batch 100, Loss: 1.7480
Batch 200, Loss: 1.7474
Batch 300, Loss: 1.7494
Epoch 33 learning rate: 0.05
Epoch 33 time: 118.85036897659302 seconds
Epoch 33 accuracy: 12.48%
Batch 100, Loss: 1.7497
Batch 200, Loss: 1.7493
Batch 300, Loss: 1.7504
Epoch 34 learning rate: 0.05
Epoch 34 time: 118.80750870704651 seconds
Epoch 34 accuracy: 12.7%
Batch 100, Loss: 1.7505
Batch 200, Loss: 1.7515
Batch 300, Loss: 1.7515
Epoch 35 learning rate: 0.05
Epoch 35 time: 118.82168412208557 seconds
Epoch 35 accuracy: 11.36%
Batch 100, Loss: 1.7519
Batch 200, Loss: 1.7531
Batch 300, Loss: 1.7525
Epoch 36 learning rate: 0.05
Epoch 36 time: 118.85572600364685 seconds
Epoch 36 accuracy: 7.34%
Batch 100, Loss: 1.7529
Batch 200, Loss: 1.7533
Batch 300, Loss: 1.7542
Epoch 37 learning rate: 0.05
Epoch 37 time: 118.83139848709106 seconds
Epoch 37 accuracy: 7.69%
Batch 100, Loss: 1.7543
Batch 200, Loss: 1.7548
Batch 300, Loss: 1.7551
Epoch 38 learning rate: 0.05
Epoch 38 time: 118.79004120826721 seconds
Epoch 38 accuracy: 7.64%
Batch 100, Loss: 1.7552
Batch 200, Loss: 1.7557
Batch 300, Loss: 1.7561
Epoch 39 learning rate: 0.05
Epoch 39 time: 118.84618163108826 seconds
Epoch 39 accuracy: 12.37%
Batch 100, Loss: 1.7562
Batch 200, Loss: 1.7564
Batch 300, Loss: 1.7566
Epoch 40 learning rate: 0.05
Epoch 40 time: 118.85352516174316 seconds
Epoch 40 accuracy: 7.65%
Batch 100, Loss: 1.7569
Batch 200, Loss: 1.7569
Batch 300, Loss: 1.7571
Epoch 41 learning rate: 0.05
Epoch 41 time: 118.86627769470215 seconds
Epoch 41 accuracy: 6.55%
Batch 100, Loss: 1.7571
Batch 200, Loss: 1.7573
Batch 300, Loss: 1.7573
Epoch 42 learning rate: 0.05
Epoch 42 time: 118.8607177734375 seconds
Epoch 42 accuracy: 10.0%
Batch 100, Loss: 1.7574
Batch 200, Loss: 1.7574
Batch 300, Loss: 1.7575
Epoch 43 learning rate: 0.05
Epoch 43 time: 118.82490348815918 seconds
Epoch 43 accuracy: 10.0%
Batch 100, Loss: 1.7575
Batch 200, Loss: 1.7576
Batch 300, Loss: 1.7576
Epoch 44 learning rate: 0.05
Epoch 44 time: 118.84563183784485 seconds
Epoch 44 accuracy: 10.0%
Batch 100, Loss: 1.7576
Batch 200, Loss: 1.7577
Batch 300, Loss: 1.7576
Epoch 45 learning rate: 0.05
Epoch 45 time: 118.8876314163208 seconds
Epoch 45 accuracy: 10.0%
Batch 100, Loss: 1.7576
Batch 200, Loss: 1.7577
Batch 300, Loss: 1.7578
Epoch 46 learning rate: 0.05
Epoch 46 time: 118.82955503463745 seconds
Epoch 46 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7577
Batch 300, Loss: 1.7578
Epoch 47 learning rate: 0.05
Epoch 47 time: 118.81205534934998 seconds
Epoch 47 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 48 learning rate: 0.05
Epoch 48 time: 118.82669997215271 seconds
Epoch 48 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 49 learning rate: 0.05
Epoch 49 time: 118.83165144920349 seconds
Epoch 49 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7578
Epoch 50 learning rate: 0.05
Epoch 50 time: 118.85663414001465 seconds
Epoch 50 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 51 learning rate: 0.05
Epoch 51 time: 118.78846859931946 seconds
Epoch 51 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 52 learning rate: 0.05
Epoch 52 time: 118.83342242240906 seconds
Epoch 52 accuracy: 9.99%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 53 learning rate: 0.05
Epoch 53 time: 118.84737467765808 seconds
Epoch 53 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 54 learning rate: 0.05
Epoch 54 time: 118.83336615562439 seconds
Epoch 54 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 55 learning rate: 0.05
Epoch 55 time: 118.8245759010315 seconds
Epoch 55 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 56 learning rate: 0.05
Epoch 56 time: 118.78677344322205 seconds
Epoch 56 accuracy: 10.01%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 57 learning rate: 0.05
Epoch 57 time: 118.80783820152283 seconds
Epoch 57 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 58 learning rate: 0.05
Epoch 58 time: 118.7755720615387 seconds
Epoch 58 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 59 learning rate: 0.05
Epoch 59 time: 118.76920247077942 seconds
Epoch 59 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 60 learning rate: 0.05
Epoch 60 time: 118.81448340415955 seconds
Epoch 60 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 61 learning rate: 0.05
Epoch 61 time: 118.85412263870239 seconds
Epoch 61 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 62 learning rate: 0.05
Epoch 62 time: 118.84066343307495 seconds
Epoch 62 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 63 learning rate: 0.05
Epoch 63 time: 118.78466057777405 seconds
Epoch 63 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 64 learning rate: 0.05
Epoch 64 time: 118.85021567344666 seconds
Epoch 64 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 65 learning rate: 0.05
Epoch 65 time: 118.80839967727661 seconds
Epoch 65 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 66 learning rate: 0.05
Epoch 66 time: 118.82411408424377 seconds
Epoch 66 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 67 learning rate: 0.05
Epoch 67 time: 118.7995343208313 seconds
Epoch 67 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 68 learning rate: 0.05
Epoch 68 time: 118.92371010780334 seconds
Epoch 68 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 69 learning rate: 0.05
Epoch 69 time: 118.86384320259094 seconds
Epoch 69 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 70 learning rate: 0.05
Epoch 70 time: 118.86615657806396 seconds
Epoch 70 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 71 learning rate: 0.05
Epoch 71 time: 118.81672310829163 seconds
Epoch 71 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 72 learning rate: 0.05
Epoch 72 time: 118.8130567073822 seconds
Epoch 72 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 73 learning rate: 0.05
Epoch 73 time: 118.76427173614502 seconds
Epoch 73 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 74 learning rate: 0.05
Epoch 74 time: 118.78557443618774 seconds
Epoch 74 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 75 learning rate: 0.05
Epoch 75 time: 118.77525663375854 seconds
Epoch 75 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 76 learning rate: 0.05
Epoch 76 time: 118.79728603363037 seconds
Epoch 76 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 77 learning rate: 0.05
Epoch 77 time: 118.80561137199402 seconds
Epoch 77 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 78 learning rate: 0.05
Epoch 78 time: 118.81148076057434 seconds
Epoch 78 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 79 learning rate: 0.05
Epoch 79 time: 118.79802536964417 seconds
Epoch 79 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 80 learning rate: 0.05
Epoch 80 time: 118.83614587783813 seconds
Epoch 80 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 81 learning rate: 0.05
Epoch 81 time: 118.84251379966736 seconds
Epoch 81 accuracy: 10.0%
Batch 100, Loss: 1.7580
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 82 learning rate: 0.05
Epoch 82 time: 118.82752919197083 seconds
Epoch 82 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 83 learning rate: 0.05
Epoch 83 time: 118.80800604820251 seconds
Epoch 83 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 84 learning rate: 0.05
Epoch 84 time: 118.83251881599426 seconds
Epoch 84 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 85 learning rate: 0.05
Epoch 85 time: 118.83513402938843 seconds
Epoch 85 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 86 learning rate: 0.05
Epoch 86 time: 118.80307006835938 seconds
Epoch 86 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 87 learning rate: 0.05
Epoch 87 time: 118.80483484268188 seconds
Epoch 87 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 88 learning rate: 0.05
Epoch 88 time: 118.81090831756592 seconds
Epoch 88 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 89 learning rate: 0.05
Epoch 89 time: 118.8839020729065 seconds
Epoch 89 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 90 learning rate: 0.05
Epoch 90 time: 118.8209810256958 seconds
Epoch 90 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 91 learning rate: 0.05
Epoch 91 time: 118.82685828208923 seconds
Epoch 91 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 92 learning rate: 0.05
Epoch 92 time: 118.79777765274048 seconds
Epoch 92 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 93 learning rate: 0.05
Epoch 93 time: 118.80915594100952 seconds
Epoch 93 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 94 learning rate: 0.05
Epoch 94 time: 118.83300757408142 seconds
Epoch 94 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 95 learning rate: 0.05
Epoch 95 time: 118.85128593444824 seconds
Epoch 95 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 96 learning rate: 0.05
Epoch 96 time: 118.84857559204102 seconds
Epoch 96 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 97 learning rate: 0.05
Epoch 97 time: 118.80207777023315 seconds
Epoch 97 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7577
Batch 300, Loss: 1.7580
Epoch 98 learning rate: 0.05
Epoch 98 time: 118.84817266464233 seconds
Epoch 98 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 99 learning rate: 0.05
Epoch 99 time: 118.79808330535889 seconds
Epoch 99 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 100 learning rate: 0.05
Epoch 100 time: 118.8573567867279 seconds
Epoch 100 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 101 learning rate: 0.05
Epoch 101 time: 118.82181334495544 seconds
Epoch 101 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 102 learning rate: 0.05
Epoch 102 time: 118.76080870628357 seconds
Epoch 102 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 103 learning rate: 0.05
Epoch 103 time: 118.80478572845459 seconds
Epoch 103 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 104 learning rate: 0.05
Epoch 104 time: 118.7986249923706 seconds
Epoch 104 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 105 learning rate: 0.05
Epoch 105 time: 118.80513715744019 seconds
Epoch 105 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 106 learning rate: 0.05
Epoch 106 time: 118.78711485862732 seconds
Epoch 106 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 107 learning rate: 0.05
Epoch 107 time: 118.766286611557 seconds
Epoch 107 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 108 learning rate: 0.05
Epoch 108 time: 118.79308366775513 seconds
Epoch 108 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 109 learning rate: 0.05
Epoch 109 time: 118.78411483764648 seconds
Epoch 109 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 110 learning rate: 0.05
Epoch 110 time: 118.87169075012207 seconds
Epoch 110 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 111 learning rate: 0.05
Epoch 111 time: 118.79392457008362 seconds
Epoch 111 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 112 learning rate: 0.05
Epoch 112 time: 118.82494449615479 seconds
Epoch 112 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 113 learning rate: 0.05
Epoch 113 time: 118.77968978881836 seconds
Epoch 113 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 114 learning rate: 0.05
Epoch 114 time: 118.80677485466003 seconds
Epoch 114 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 115 learning rate: 0.05
Epoch 115 time: 118.79246973991394 seconds
Epoch 115 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 116 learning rate: 0.05
Epoch 116 time: 118.82242131233215 seconds
Epoch 116 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 117 learning rate: 0.05
Epoch 117 time: 118.8101007938385 seconds
Epoch 117 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7581
Epoch 118 learning rate: 0.05
Epoch 118 time: 118.8074676990509 seconds
Epoch 118 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 119 learning rate: 0.05
Epoch 119 time: 118.79797172546387 seconds
Epoch 119 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 120 learning rate: 0.05
Epoch 120 time: 118.88751697540283 seconds
Epoch 120 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 121 learning rate: 0.05
Epoch 121 time: 118.84289026260376 seconds
Epoch 121 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7578
Epoch 122 learning rate: 0.05
Epoch 122 time: 118.80005717277527 seconds
Epoch 122 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 123 learning rate: 0.05
Epoch 123 time: 118.80330181121826 seconds
Epoch 123 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 124 learning rate: 0.05
Epoch 124 time: 118.82349896430969 seconds
Epoch 124 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 125 learning rate: 0.05
Epoch 125 time: 118.84553623199463 seconds
Epoch 125 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 126 learning rate: 0.05
Epoch 126 time: 118.83037328720093 seconds
Epoch 126 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 127 learning rate: 0.05
Epoch 127 time: 118.86790490150452 seconds
Epoch 127 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 128 learning rate: 0.05
Epoch 128 time: 118.86536288261414 seconds
Epoch 128 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 129 learning rate: 0.05
Epoch 129 time: 118.80852484703064 seconds
Epoch 129 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 130 learning rate: 0.05
Epoch 130 time: 118.8298704624176 seconds
Epoch 130 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 131 learning rate: 0.05
Epoch 131 time: 118.90833067893982 seconds
Epoch 131 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 132 learning rate: 0.05
Epoch 132 time: 118.8408133983612 seconds
Epoch 132 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 133 learning rate: 0.05
Epoch 133 time: 118.84881162643433 seconds
Epoch 133 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 134 learning rate: 0.05
Epoch 134 time: 118.87518572807312 seconds
Epoch 134 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 135 learning rate: 0.05
Epoch 135 time: 118.83272695541382 seconds
Epoch 135 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 136 learning rate: 0.05
Epoch 136 time: 119.06581854820251 seconds
Epoch 136 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 137 learning rate: 0.05
Epoch 137 time: 118.80522012710571 seconds
Epoch 137 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 138 learning rate: 0.05
Epoch 138 time: 118.8254885673523 seconds
Epoch 138 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 139 learning rate: 0.05
Epoch 139 time: 118.81795644760132 seconds
Epoch 139 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 140 learning rate: 0.05
Epoch 140 time: 118.83836770057678 seconds
Epoch 140 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 141 learning rate: 0.05
Epoch 141 time: 118.83587336540222 seconds
Epoch 141 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 142 learning rate: 0.05
Epoch 142 time: 118.8165717124939 seconds
Epoch 142 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 143 learning rate: 0.05
Epoch 143 time: 118.80985856056213 seconds
Epoch 143 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 144 learning rate: 0.05
Epoch 144 time: 118.76791620254517 seconds
Epoch 144 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 145 learning rate: 0.05
Epoch 145 time: 118.78859829902649 seconds
Epoch 145 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 146 learning rate: 0.05
Epoch 146 time: 118.86585760116577 seconds
Epoch 146 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 147 learning rate: 0.05
Epoch 147 time: 118.79370141029358 seconds
Epoch 147 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7577
Batch 300, Loss: 1.7580
Epoch 148 learning rate: 0.05
Epoch 148 time: 118.84470820426941 seconds
Epoch 148 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 149 learning rate: 0.05
Epoch 149 time: 118.85104823112488 seconds
Epoch 149 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 150 learning rate: 0.05
Epoch 150 time: 118.82743978500366 seconds
Epoch 150 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 151 learning rate: 0.05
Epoch 151 time: 118.78754782676697 seconds
Epoch 151 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 152 learning rate: 0.05
Epoch 152 time: 118.85437250137329 seconds
Epoch 152 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 153 learning rate: 0.05
Epoch 153 time: 118.80336380004883 seconds
Epoch 153 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 154 learning rate: 0.05
Epoch 154 time: 118.85586619377136 seconds
Epoch 154 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7579
Epoch 155 learning rate: 0.05
Epoch 155 time: 118.80529856681824 seconds
Epoch 155 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 156 learning rate: 0.05
Epoch 156 time: 118.77344989776611 seconds
Epoch 156 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 157 learning rate: 0.05
Epoch 157 time: 118.88813185691833 seconds
Epoch 157 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 158 learning rate: 0.05
Epoch 158 time: 118.85635495185852 seconds
Epoch 158 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 159 learning rate: 0.05
Epoch 159 time: 118.85337448120117 seconds
Epoch 159 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7578
Epoch 160 learning rate: 0.05
Epoch 160 time: 118.77416157722473 seconds
Epoch 160 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 161 learning rate: 0.05
Epoch 161 time: 118.81776762008667 seconds
Epoch 161 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 162 learning rate: 0.05
Epoch 162 time: 118.88919687271118 seconds
Epoch 162 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 163 learning rate: 0.05
Epoch 163 time: 118.8142249584198 seconds
Epoch 163 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 164 learning rate: 0.05
Epoch 164 time: 118.84113478660583 seconds
Epoch 164 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 165 learning rate: 0.05
Epoch 165 time: 118.87249946594238 seconds
Epoch 165 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 166 learning rate: 0.05
Epoch 166 time: 118.79775500297546 seconds
Epoch 166 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 167 learning rate: 0.05
Epoch 167 time: 118.82129406929016 seconds
Epoch 167 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 168 learning rate: 0.05
Epoch 168 time: 118.82638692855835 seconds
Epoch 168 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 169 learning rate: 0.05
Epoch 169 time: 118.8331413269043 seconds
Epoch 169 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 170 learning rate: 0.05
Epoch 170 time: 118.84094309806824 seconds
Epoch 170 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7578
Epoch 171 learning rate: 0.05
Epoch 171 time: 118.7841534614563 seconds
Epoch 171 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 172 learning rate: 0.05
Epoch 172 time: 118.80672121047974 seconds
Epoch 172 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 173 learning rate: 0.05
Epoch 173 time: 118.81769919395447 seconds
Epoch 173 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 174 learning rate: 0.05
Epoch 174 time: 118.85067224502563 seconds
Epoch 174 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 175 learning rate: 0.05
Epoch 175 time: 118.81890940666199 seconds
Epoch 175 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 176 learning rate: 0.05
Epoch 176 time: 118.83793640136719 seconds
Epoch 176 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 177 learning rate: 0.05
Epoch 177 time: 119.0586166381836 seconds
Epoch 177 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 178 learning rate: 0.05
Epoch 178 time: 118.82688307762146 seconds
Epoch 178 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 179 learning rate: 0.05
Epoch 179 time: 118.79676580429077 seconds
Epoch 179 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7577
Batch 300, Loss: 1.7580
Epoch 180 learning rate: 0.05
Epoch 180 time: 118.77721095085144 seconds
Epoch 180 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 181 learning rate: 0.05
Epoch 181 time: 118.8019106388092 seconds
Epoch 181 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 182 learning rate: 0.05
Epoch 182 time: 118.80794668197632 seconds
Epoch 182 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 183 learning rate: 0.05
Epoch 183 time: 118.85372471809387 seconds
Epoch 183 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 184 learning rate: 0.05
Epoch 184 time: 118.86694645881653 seconds
Epoch 184 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7579
Epoch 185 learning rate: 0.05
Epoch 185 time: 118.82741928100586 seconds
Epoch 185 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 186 learning rate: 0.05
Epoch 186 time: 118.85617542266846 seconds
Epoch 186 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 187 learning rate: 0.05
Epoch 187 time: 118.84657573699951 seconds
Epoch 187 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 188 learning rate: 0.05
Epoch 188 time: 118.79584264755249 seconds
Epoch 188 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 189 learning rate: 0.05
Epoch 189 time: 118.8459095954895 seconds
Epoch 189 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 190 learning rate: 0.05
Epoch 190 time: 118.8360390663147 seconds
Epoch 190 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 191 learning rate: 0.05
Epoch 191 time: 118.86968231201172 seconds
Epoch 191 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 192 learning rate: 0.05
Epoch 192 time: 118.85214614868164 seconds
Epoch 192 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7580
Epoch 193 learning rate: 0.05
Epoch 193 time: 118.84670448303223 seconds
Epoch 193 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 194 learning rate: 0.05
Epoch 194 time: 118.82051801681519 seconds
Epoch 194 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 195 learning rate: 0.05
Epoch 195 time: 118.87077307701111 seconds
Epoch 195 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 196 learning rate: 0.05
Epoch 196 time: 118.81541085243225 seconds
Epoch 196 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7580
Batch 300, Loss: 1.7578
Epoch 197 learning rate: 0.05
Epoch 197 time: 118.81302881240845 seconds
Epoch 197 accuracy: 10.0%
Batch 100, Loss: 1.7577
Batch 200, Loss: 1.7578
Batch 300, Loss: 1.7580
Epoch 198 learning rate: 0.05
Epoch 198 time: 118.80823254585266 seconds
Epoch 198 accuracy: 10.0%
Batch 100, Loss: 1.7578
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 199 learning rate: 0.05
Epoch 199 time: 118.85160803794861 seconds
Epoch 199 accuracy: 10.0%
Batch 100, Loss: 1.7579
Batch 200, Loss: 1.7579
Batch 300, Loss: 1.7579
Epoch 200 learning rate: 0.05
Epoch 200 time: 118.81100797653198 seconds
Epoch 200 accuracy: 10.0%
rho:  0.04 , alpha:  0.3
Total training time: 23775.43967986107 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.1001
Norm of the Gradient: 3.4400977194e-02
Smallest Hessian Eigenvalue: -0.0973
