The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:07:46
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 307.5334
Batch 50, Loss: 77.3804
Batch 75, Loss: 36.4083
Batch 100, Loss: 25.5530
Batch 125, Loss: 19.6538
Batch 150, Loss: 16.0448
Batch 175, Loss: 13.6307
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 270.16541171073914 seconds
Epoch 1 accuracy: 8.17%
Batch 25, Loss: 10.9695
Batch 50, Loss: 9.9749
Batch 75, Loss: 9.2855
Batch 100, Loss: 8.7354
Batch 125, Loss: 8.2708
Batch 150, Loss: 7.8650
Batch 175, Loss: 7.4156
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 258.2575557231903 seconds
Epoch 2 accuracy: 9.12%
Batch 25, Loss: 6.8490
Batch 50, Loss: 6.6090
Batch 75, Loss: 6.3983
Batch 100, Loss: 6.2080
Batch 125, Loss: 6.0336
Batch 150, Loss: 5.8721
Batch 175, Loss: 5.7200
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 258.4888243675232 seconds
Epoch 3 accuracy: 9.49%
Batch 25, Loss: 5.4861
Batch 50, Loss: 5.3598
Batch 75, Loss: 5.2420
Batch 100, Loss: 5.1321
Batch 125, Loss: 5.0280
Batch 150, Loss: 4.9286
Batch 175, Loss: 4.8350
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 258.77446126937866 seconds
Epoch 4 accuracy: 10.0%
Batch 25, Loss: 4.6941
Batch 50, Loss: 4.6195
Batch 75, Loss: 4.5507
Batch 100, Loss: 4.4864
Batch 125, Loss: 4.4265
Batch 150, Loss: 4.3698
Batch 175, Loss: 4.3161
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 258.78572940826416 seconds
Epoch 5 accuracy: 10.11%
Batch 25, Loss: 4.2312
Batch 50, Loss: 4.1828
Batch 75, Loss: 4.1357
Batch 100, Loss: 4.0900
Batch 125, Loss: 4.0452
Batch 150, Loss: 4.0020
Batch 175, Loss: 3.9608
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 258.76253390312195 seconds
Epoch 6 accuracy: 10.32%
Batch 25, Loss: 3.8959
Batch 50, Loss: 3.8594
Batch 75, Loss: 3.8247
Batch 100, Loss: 3.7912
Batch 125, Loss: 3.7591
Batch 150, Loss: 3.7285
Batch 175, Loss: 3.6993
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 258.99960136413574 seconds
Epoch 7 accuracy: 10.57%
Batch 25, Loss: 3.6535
Batch 50, Loss: 3.6279
Batch 75, Loss: 3.6034
Batch 100, Loss: 3.5797
Batch 125, Loss: 3.5568
Batch 150, Loss: 3.5345
Batch 175, Loss: 3.5130
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 259.059543132782 seconds
Epoch 8 accuracy: 10.96%
Batch 25, Loss: 3.4783
Batch 50, Loss: 3.4585
Batch 75, Loss: 3.4392
Batch 100, Loss: 3.4205
Batch 125, Loss: 3.4022
Batch 150, Loss: 3.3842
Batch 175, Loss: 3.3667
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 260.55722856521606 seconds
Epoch 9 accuracy: 11.35%
Batch 25, Loss: 3.3384
Batch 50, Loss: 3.3222
Batch 75, Loss: 3.3063
Batch 100, Loss: 3.2906
Batch 125, Loss: 3.2752
Batch 150, Loss: 3.2600
Batch 175, Loss: 3.2451
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 259.0366611480713 seconds
Epoch 10 accuracy: 11.51%
Batch 25, Loss: 3.2207
Batch 50, Loss: 3.2066
Batch 75, Loss: 3.1927
Batch 100, Loss: 3.1791
Batch 125, Loss: 3.1658
Batch 150, Loss: 3.1527
Batch 175, Loss: 3.1399
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 260.5842242240906 seconds
Epoch 11 accuracy: 11.64%
Batch 25, Loss: 3.1188
Batch 50, Loss: 3.1066
Batch 75, Loss: 3.0944
Batch 100, Loss: 3.0825
Batch 125, Loss: 3.0708
Batch 150, Loss: 3.0593
Batch 175, Loss: 3.0479
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 259.3191578388214 seconds
Epoch 12 accuracy: 11.95%
Batch 25, Loss: 3.0290
Batch 50, Loss: 3.0179
Batch 75, Loss: 3.0069
Batch 100, Loss: 2.9961
Batch 125, Loss: 2.9853
Batch 150, Loss: 2.9747
Batch 175, Loss: 2.9641
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 258.58245372772217 seconds
Epoch 13 accuracy: 12.28%
Batch 25, Loss: 2.9466
Batch 50, Loss: 2.9364
Batch 75, Loss: 2.9262
Batch 100, Loss: 2.9163
Batch 125, Loss: 2.9065
Batch 150, Loss: 2.8970
Batch 175, Loss: 2.8878
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 259.18529653549194 seconds
Epoch 14 accuracy: 12.44%
Batch 25, Loss: 2.8727
Batch 50, Loss: 2.8641
Batch 75, Loss: 2.8556
Batch 100, Loss: 2.8472
Batch 125, Loss: 2.8392
Batch 150, Loss: 2.8313
Batch 175, Loss: 2.8237
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 258.8488597869873 seconds
Epoch 15 accuracy: 12.56%
Batch 25, Loss: 2.8112
Batch 50, Loss: 2.8040
Batch 75, Loss: 2.7968
Batch 100, Loss: 2.7898
Batch 125, Loss: 2.7829
Batch 150, Loss: 2.7761
Batch 175, Loss: 2.7694
Noise applied in 0 out of 3072 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 259.3276047706604 seconds
Epoch 16 accuracy: 12.78%
Batch 25, Loss: 2.7584
Batch 50, Loss: 2.7519
Batch 75, Loss: 2.7454
Batch 100, Loss: 2.7391
Batch 125, Loss: 2.7329
Batch 150, Loss: 2.7267
Batch 175, Loss: 2.7206
Noise applied in 0 out of 3264 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 259.05397033691406 seconds
Epoch 17 accuracy: 12.92%
Batch 25, Loss: 2.7105
Batch 50, Loss: 2.7045
Batch 75, Loss: 2.6986
Batch 100, Loss: 2.6927
Batch 125, Loss: 2.6868
Batch 150, Loss: 2.6811
Batch 175, Loss: 2.6754
Noise applied in 0 out of 3456 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 281.6940367221832 seconds
Epoch 18 accuracy: 13.03%
Batch 25, Loss: 2.6659
Batch 50, Loss: 2.6604
Batch 75, Loss: 2.6549
Batch 100, Loss: 2.6494
Batch 125, Loss: 2.6440
Batch 150, Loss: 2.6386
Batch 175, Loss: 2.6333
Noise applied in 0 out of 3648 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 295.71787571907043 seconds
Epoch 19 accuracy: 13.15%
Batch 25, Loss: 2.6244
Batch 50, Loss: 2.6192
Batch 75, Loss: 2.6140
Batch 100, Loss: 2.6089
Batch 125, Loss: 2.6038
Batch 150, Loss: 2.5987
Batch 175, Loss: 2.5936
Noise applied in 0 out of 3840 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 259.0397255420685 seconds
Epoch 20 accuracy: 13.4%
rho:  0.04 , alpha:  0.3
Total training time: 5252.25785779953 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 2.8849
Norm of the Gradient: 8.5553836823e-01
Smallest Hessian Eigenvalue: -0.1859
Noise Threshold: 0.4
Noise Radius: 0.5
