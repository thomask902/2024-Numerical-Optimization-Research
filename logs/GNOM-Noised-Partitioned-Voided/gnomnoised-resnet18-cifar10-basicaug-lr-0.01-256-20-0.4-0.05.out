The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-17:04:42
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 899.8227
Batch 50, Loss: 665.1866
Batch 75, Loss: 336.6025
Batch 100, Loss: 193.5801
Batch 125, Loss: 129.9267
Batch 150, Loss: 101.4008
Batch 175, Loss: 86.7834
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 289.01133608818054 seconds
Epoch 1 accuracy: 10.7%
Batch 25, Loss: 72.0869
Batch 50, Loss: 64.8600
Batch 75, Loss: 58.1907
Batch 100, Loss: 52.5686
Batch 125, Loss: 48.0240
Batch 150, Loss: 44.1528
Batch 175, Loss: 40.8822
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 294.7777292728424 seconds
Epoch 2 accuracy: 10.32%
Batch 25, Loss: 36.5410
Batch 50, Loss: 34.5760
Batch 75, Loss: 32.9776
Batch 100, Loss: 31.6466
Batch 125, Loss: 30.4318
Batch 150, Loss: 29.2284
Batch 175, Loss: 28.2490
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 291.382732629776 seconds
Epoch 3 accuracy: 10.13%
Batch 25, Loss: 26.8611
Batch 50, Loss: 26.0508
Batch 75, Loss: 25.3011
Batch 100, Loss: 24.6370
Batch 125, Loss: 24.0383
Batch 150, Loss: 23.4743
Batch 175, Loss: 22.9385
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 280.598778963089 seconds
Epoch 4 accuracy: 10.13%
Batch 25, Loss: 22.0894
Batch 50, Loss: 21.6128
Batch 75, Loss: 21.1443
Batch 100, Loss: 20.6999
Batch 125, Loss: 20.2952
Batch 150, Loss: 19.9142
Batch 175, Loss: 19.5494
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 279.77317786216736 seconds
Epoch 5 accuracy: 10.14%
Batch 25, Loss: 18.9701
Batch 50, Loss: 18.6372
Batch 75, Loss: 18.3130
Batch 100, Loss: 17.9974
Batch 125, Loss: 17.6926
Batch 150, Loss: 17.3957
Batch 175, Loss: 17.1065
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 293.7620415687561 seconds
Epoch 6 accuracy: 10.14%
Batch 25, Loss: 16.6452
Batch 50, Loss: 16.3841
Batch 75, Loss: 16.1305
Batch 100, Loss: 15.8816
Batch 125, Loss: 15.6397
Batch 150, Loss: 15.4034
Batch 175, Loss: 15.1740
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 420.1174647808075 seconds
Epoch 7 accuracy: 10.2%
Batch 25, Loss: 14.8158
Batch 50, Loss: 14.6154
Batch 75, Loss: 14.4216
Batch 100, Loss: 14.2333
Batch 125, Loss: 14.0522
Batch 150, Loss: 13.8799
Batch 175, Loss: 13.7134
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 286.72103905677795 seconds
Epoch 8 accuracy: 10.24%
Batch 25, Loss: 13.4444
Batch 50, Loss: 13.2893
Batch 75, Loss: 13.1379
Batch 100, Loss: 12.9905
Batch 125, Loss: 12.8472
Batch 150, Loss: 12.7069
Batch 175, Loss: 12.5703
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 290.630330324173 seconds
Epoch 9 accuracy: 10.31%
Batch 25, Loss: 12.3492
Batch 50, Loss: 12.2205
Batch 75, Loss: 12.0949
Batch 100, Loss: 11.9732
Batch 125, Loss: 11.8556
Batch 150, Loss: 11.7421
Batch 175, Loss: 11.6322
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 527.1571974754333 seconds
Epoch 10 accuracy: 10.25%
Batch 25, Loss: 11.4533
Batch 50, Loss: 11.3505
Batch 75, Loss: 11.2503
Batch 100, Loss: 11.1524
Batch 125, Loss: 11.0574
Batch 150, Loss: 10.9645
Batch 175, Loss: 10.8726
Noise applied in 0 out of 192 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 301.29934573173523 seconds
Epoch 11 accuracy: 10.23%
Batch 25, Loss: 10.7216
Batch 50, Loss: 10.6342
Batch 75, Loss: 10.5485
Batch 100, Loss: 10.4646
Batch 125, Loss: 10.3818
Batch 150, Loss: 10.3007
Batch 175, Loss: 10.2212
Noise applied in 0 out of 192 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 363.415319442749 seconds
Epoch 12 accuracy: 10.16%
Batch 25, Loss: 10.0898
Batch 50, Loss: 10.0125
Batch 75, Loss: 9.9357
Batch 100, Loss: 9.8598
Batch 125, Loss: 9.7850
Batch 150, Loss: 9.7107
Batch 175, Loss: 9.6371
Noise applied in 0 out of 192 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 295.3622899055481 seconds
Epoch 13 accuracy: 10.17%
Batch 25, Loss: 9.5148
Batch 50, Loss: 9.4429
Batch 75, Loss: 9.3719
Batch 100, Loss: 9.3006
Batch 125, Loss: 9.2292
Batch 150, Loss: 9.1580
Batch 175, Loss: 9.0869
Noise applied in 0 out of 192 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 279.1235821247101 seconds
Epoch 14 accuracy: 10.13%
Batch 25, Loss: 8.9670
Batch 50, Loss: 8.8957
Batch 75, Loss: 8.8244
Batch 100, Loss: 8.7529
Batch 125, Loss: 8.6807
Batch 150, Loss: 8.6079
Batch 175, Loss: 8.5340
Noise applied in 0 out of 192 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 278.7081925868988 seconds
Epoch 15 accuracy: 10.13%
Batch 25, Loss: 8.4081
Batch 50, Loss: 8.3316
Batch 75, Loss: 8.2539
Batch 100, Loss: 8.1744
Batch 125, Loss: 8.0929
Batch 150, Loss: 8.0095
Batch 175, Loss: 7.9237
Noise applied in 0 out of 192 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 279.8224458694458 seconds
Epoch 16 accuracy: 10.13%
Batch 25, Loss: 7.7730
Batch 50, Loss: 7.6792
Batch 75, Loss: 7.5817
Batch 100, Loss: 7.4801
Batch 125, Loss: 7.3734
Batch 150, Loss: 7.2604
Batch 175, Loss: 7.1401
Noise applied in 0 out of 192 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 279.3028039932251 seconds
Epoch 17 accuracy: 10.09%
Batch 25, Loss: 6.9178
Batch 50, Loss: 6.7712
Batch 75, Loss: 6.6123
Batch 100, Loss: 6.4404
Batch 125, Loss: 6.2574
Batch 150, Loss: 6.0698
Batch 175, Loss: 5.8896
Noise applied in 0 out of 192 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 278.43557834625244 seconds
Epoch 18 accuracy: 10.05%
Batch 25, Loss: 5.6226
Batch 50, Loss: 5.5280
Batch 75, Loss: 5.4061
Batch 100, Loss: 5.2415
Batch 125, Loss: 5.0907
Batch 150, Loss: 4.9645
Batch 175, Loss: 4.8633
Noise applied in 0 out of 192 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 280.62169551849365 seconds
Epoch 19 accuracy: 9.36%
Batch 25, Loss: 4.7394
Batch 50, Loss: 4.6767
Batch 75, Loss: 4.6195
Batch 100, Loss: 4.5665
Batch 125, Loss: 4.5165
Batch 150, Loss: 4.4695
Batch 175, Loss: 4.4257
Noise applied in 0 out of 192 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 280.71984672546387 seconds
Epoch 20 accuracy: 9.11%
rho:  0.04 , alpha:  0.3
Total training time: 6170.771120786667 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 9.2239
Norm of the Gradient: 1.8911964893e+00
Smallest Hessian Eigenvalue: -0.3048
Noise Threshold: 0.4
Noise Radius: 0.05
