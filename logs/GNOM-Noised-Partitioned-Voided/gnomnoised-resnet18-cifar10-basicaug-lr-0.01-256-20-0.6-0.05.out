The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-17:34:51
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 202.7841
Batch 50, Loss: 43.8176
Batch 75, Loss: 16.1317
Batch 100, Loss: 11.5809
Batch 125, Loss: 9.7318
Batch 150, Loss: 8.5460
Batch 175, Loss: 7.6826
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 425.39403462409973 seconds
Epoch 1 accuracy: 10.06%
Batch 25, Loss: 6.6288
Batch 50, Loss: 6.1587
Batch 75, Loss: 5.7781
Batch 100, Loss: 5.4638
Batch 125, Loss: 5.2014
Batch 150, Loss: 4.9791
Batch 175, Loss: 4.7893
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 276.8865852355957 seconds
Epoch 2 accuracy: 9.93%
Batch 25, Loss: 4.5253
Batch 50, Loss: 4.3940
Batch 75, Loss: 4.2785
Batch 100, Loss: 4.1759
Batch 125, Loss: 4.0840
Batch 150, Loss: 4.0007
Batch 175, Loss: 3.9252
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 290.0218765735626 seconds
Epoch 3 accuracy: 9.99%
Batch 25, Loss: 3.8125
Batch 50, Loss: 3.7531
Batch 75, Loss: 3.6983
Batch 100, Loss: 3.6478
Batch 125, Loss: 3.6011
Batch 150, Loss: 3.5579
Batch 175, Loss: 3.5174
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 533.3820946216583 seconds
Epoch 4 accuracy: 9.98%
Batch 25, Loss: 3.4541
Batch 50, Loss: 3.4190
Batch 75, Loss: 3.3860
Batch 100, Loss: 3.3545
Batch 125, Loss: 3.3244
Batch 150, Loss: 3.2958
Batch 175, Loss: 3.2684
Noise applied in 128 out of 192 batches, 66.67
Epoch 5 learning rate: 0.01
Epoch 5 time: 334.42621779441833 seconds
Epoch 5 accuracy: 10.07%
Batch 25, Loss: 3.2248
Batch 50, Loss: 3.2001
Batch 75, Loss: 3.1763
Batch 100, Loss: 3.1532
Batch 125, Loss: 3.1308
Batch 150, Loss: 3.1091
Batch 175, Loss: 3.0880
Noise applied in 310 out of 192 batches, 161.46
Epoch 6 learning rate: 0.01
Epoch 6 time: 524.7871382236481 seconds
Epoch 6 accuracy: 10.06%
Batch 25, Loss: 3.0537
Batch 50, Loss: 3.0341
Batch 75, Loss: 3.0150
Batch 100, Loss: 2.9965
Batch 125, Loss: 2.9784
Batch 150, Loss: 2.9606
Batch 175, Loss: 2.9433
Noise applied in 384 out of 192 batches, 200.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 468.1987271308899 seconds
Epoch 7 accuracy: 10.08%
Batch 25, Loss: 2.9153
Batch 50, Loss: 2.8992
Batch 75, Loss: 2.8834
Batch 100, Loss: 2.8680
Batch 125, Loss: 2.8529
Batch 150, Loss: 2.8381
Batch 175, Loss: 2.8237
Noise applied in 384 out of 192 batches, 200.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 468.477844953537 seconds
Epoch 8 accuracy: 10.13%
Batch 25, Loss: 2.8001
Batch 50, Loss: 2.7865
Batch 75, Loss: 2.7732
Batch 100, Loss: 2.7600
Batch 125, Loss: 2.7471
Batch 150, Loss: 2.7344
Batch 175, Loss: 2.7219
Noise applied in 384 out of 192 batches, 200.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 468.333872795105 seconds
Epoch 9 accuracy: 10.17%
Batch 25, Loss: 2.7016
Batch 50, Loss: 2.6897
Batch 75, Loss: 2.6781
Batch 100, Loss: 2.6667
Batch 125, Loss: 2.6555
Batch 150, Loss: 2.6445
Batch 175, Loss: 2.6337
Noise applied in 384 out of 192 batches, 200.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 468.8457043170929 seconds
Epoch 10 accuracy: 10.17%
Batch 25, Loss: 2.6157
Batch 50, Loss: 2.6052
Batch 75, Loss: 2.5950
Batch 100, Loss: 2.5851
Batch 125, Loss: 2.5753
Batch 150, Loss: 2.5658
Batch 175, Loss: 2.5566
Noise applied in 384 out of 192 batches, 200.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 471.4126374721527 seconds
Epoch 11 accuracy: 10.28%
Batch 25, Loss: 2.5416
Batch 50, Loss: 2.5330
Batch 75, Loss: 2.5245
Batch 100, Loss: 2.5163
Batch 125, Loss: 2.5082
Batch 150, Loss: 2.5002
Batch 175, Loss: 2.4926
Noise applied in 384 out of 192 batches, 200.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 478.9126350879669 seconds
Epoch 12 accuracy: 10.32%
Batch 25, Loss: 2.4801
Batch 50, Loss: 2.4728
Batch 75, Loss: 2.4656
Batch 100, Loss: 2.4584
Batch 125, Loss: 2.4515
Batch 150, Loss: 2.4447
Batch 175, Loss: 2.4381
Noise applied in 384 out of 192 batches, 200.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 486.0637354850769 seconds
Epoch 13 accuracy: 10.31%
Batch 25, Loss: 2.4275
Batch 50, Loss: 2.4214
Batch 75, Loss: 2.4155
Batch 100, Loss: 2.4097
Batch 125, Loss: 2.4042
Batch 150, Loss: 2.3988
Batch 175, Loss: 2.3936
Noise applied in 384 out of 192 batches, 200.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 510.51035737991333 seconds
Epoch 14 accuracy: 10.29%
Batch 25, Loss: 2.3851
Batch 50, Loss: 2.3802
Batch 75, Loss: 2.3753
Batch 100, Loss: 2.3705
Batch 125, Loss: 2.3659
Batch 150, Loss: 2.3614
Batch 175, Loss: 2.3571
Noise applied in 384 out of 192 batches, 200.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 473.428418636322 seconds
Epoch 15 accuracy: 10.21%
Batch 25, Loss: 2.3501
Batch 50, Loss: 2.3460
Batch 75, Loss: 2.3421
Batch 100, Loss: 2.3382
Batch 125, Loss: 2.3345
Batch 150, Loss: 2.3308
Batch 175, Loss: 2.3272
Noise applied in 384 out of 192 batches, 200.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 476.98606872558594 seconds
Epoch 16 accuracy: 10.08%
Batch 25, Loss: 2.3213
Batch 50, Loss: 2.3179
Batch 75, Loss: 2.3146
Batch 100, Loss: 2.3113
Batch 125, Loss: 2.3081
Batch 150, Loss: 2.3049
Batch 175, Loss: 2.3018
Noise applied in 384 out of 192 batches, 200.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 482.17756485939026 seconds
Epoch 17 accuracy: 9.99%
Batch 25, Loss: 2.2966
Batch 50, Loss: 2.2936
Batch 75, Loss: 2.2906
Batch 100, Loss: 2.2876
Batch 125, Loss: 2.2847
Batch 150, Loss: 2.2818
Batch 175, Loss: 2.2791
Noise applied in 384 out of 192 batches, 200.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 472.44668436050415 seconds
Epoch 18 accuracy: 9.96%
Batch 25, Loss: 2.2744
Batch 50, Loss: 2.2717
Batch 75, Loss: 2.2690
Batch 100, Loss: 2.2663
Batch 125, Loss: 2.2637
Batch 150, Loss: 2.2611
Batch 175, Loss: 2.2585
Noise applied in 384 out of 192 batches, 200.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 474.1422121524811 seconds
Epoch 19 accuracy: 9.94%
Batch 25, Loss: 2.2543
Batch 50, Loss: 2.2518
Batch 75, Loss: 2.2494
Batch 100, Loss: 2.2469
Batch 125, Loss: 2.2444
Batch 150, Loss: 2.2420
slurmstepd: error: *** JOB 24621249 ON gra966 CANCELLED AT 2024-09-04T20:05:06 DUE TO TIME LIMIT ***
