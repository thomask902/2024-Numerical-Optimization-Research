The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:04:40
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 94.2181
Batch 50, Loss: 27.5277
Batch 75, Loss: 12.6283
Batch 100, Loss: 9.3768
Batch 125, Loss: 7.9615
Batch 150, Loss: 7.0446
Batch 175, Loss: 6.3741
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 292.6572935581207 seconds
Epoch 1 accuracy: 10.48%
Batch 25, Loss: 5.5977
Batch 50, Loss: 5.2793
Batch 75, Loss: 5.0162
Batch 100, Loss: 4.7885
Batch 125, Loss: 4.5868
Batch 150, Loss: 4.4049
Batch 175, Loss: 4.2392
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 263.8915100097656 seconds
Epoch 2 accuracy: 10.6%
Batch 25, Loss: 3.9903
Batch 50, Loss: 3.8566
Batch 75, Loss: 3.7323
Batch 100, Loss: 3.6162
Batch 125, Loss: 3.5073
Batch 150, Loss: 3.4048
Batch 175, Loss: 3.3081
Noise applied in 32 out of 576 batches, 5.56
Epoch 3 learning rate: 0.01
Epoch 3 time: 278.886616230011 seconds
Epoch 3 accuracy: 11.12%
Batch 25, Loss: 3.1567
Batch 50, Loss: 3.0727
Batch 75, Loss: 2.9928
Batch 100, Loss: 2.9169
Batch 125, Loss: 2.8446
Batch 150, Loss: 2.7758
Batch 175, Loss: 2.7105
Noise applied in 224 out of 768 batches, 29.17
Epoch 4 learning rate: 0.01
Epoch 4 time: 355.88518142700195 seconds
Epoch 4 accuracy: 11.3%
Batch 25, Loss: 2.6095
Batch 50, Loss: 2.5550
Batch 75, Loss: 2.5059
Batch 100, Loss: 2.4626
Batch 125, Loss: 2.4254
Batch 150, Loss: 2.3938
Batch 175, Loss: 2.3670
Noise applied in 416 out of 960 batches, 43.33
Epoch 5 learning rate: 0.01
Epoch 5 time: 354.9697759151459 seconds
Epoch 5 accuracy: 11.46%
Batch 25, Loss: 2.3283
Batch 50, Loss: 2.3078
Batch 75, Loss: 2.2887
Batch 100, Loss: 2.2711
Batch 125, Loss: 2.2547
Batch 150, Loss: 2.2393
Batch 175, Loss: 2.2249
Noise applied in 608 out of 1152 batches, 52.78
Epoch 6 learning rate: 0.01
Epoch 6 time: 356.0854251384735 seconds
Epoch 6 accuracy: 11.17%
Batch 25, Loss: 2.2027
Batch 50, Loss: 2.1907
Batch 75, Loss: 2.1793
Batch 100, Loss: 2.1685
Batch 125, Loss: 2.1585
Batch 150, Loss: 2.1489
Batch 175, Loss: 2.1398
Noise applied in 800 out of 1344 batches, 59.52
Epoch 7 learning rate: 0.01
Epoch 7 time: 355.8115191459656 seconds
Epoch 7 accuracy: 10.99%
Batch 25, Loss: 2.1256
Batch 50, Loss: 2.1176
Batch 75, Loss: 2.1101
Batch 100, Loss: 2.1028
Batch 125, Loss: 2.0959
Batch 150, Loss: 2.0891
Batch 175, Loss: 2.0827
Noise applied in 992 out of 1536 batches, 64.58
Epoch 8 learning rate: 0.01
Epoch 8 time: 356.93734431266785 seconds
Epoch 8 accuracy: 10.96%
Batch 25, Loss: 2.0723
Batch 50, Loss: 2.0665
Batch 75, Loss: 2.0608
Batch 100, Loss: 2.0553
Batch 125, Loss: 2.0497
Batch 150, Loss: 2.0444
Batch 175, Loss: 2.0392
Noise applied in 1184 out of 1728 batches, 68.52
Epoch 9 learning rate: 0.01
Epoch 9 time: 356.6666650772095 seconds
Epoch 9 accuracy: 10.87%
Batch 25, Loss: 2.0307
Batch 50, Loss: 2.0259
Batch 75, Loss: 2.0209
Batch 100, Loss: 2.0161
Batch 125, Loss: 2.0114
Batch 150, Loss: 2.0067
Batch 175, Loss: 2.0020
Noise applied in 1376 out of 1920 batches, 71.67
Epoch 10 learning rate: 0.01
Epoch 10 time: 355.8919138908386 seconds
Epoch 10 accuracy: 10.75%
Batch 25, Loss: 1.9944
Batch 50, Loss: 1.9900
Batch 75, Loss: 1.9857
Batch 100, Loss: 1.9814
Batch 125, Loss: 1.9772
Batch 150, Loss: 1.9731
Batch 175, Loss: 1.9691
Noise applied in 1568 out of 2112 batches, 74.24
Epoch 11 learning rate: 0.01
Epoch 11 time: 355.91983366012573 seconds
Epoch 11 accuracy: 10.83%
Batch 25, Loss: 1.9623
Batch 50, Loss: 1.9584
Batch 75, Loss: 1.9546
Batch 100, Loss: 1.9508
Batch 125, Loss: 1.9470
Batch 150, Loss: 1.9433
Batch 175, Loss: 1.9395
Noise applied in 1760 out of 2304 batches, 76.39
Epoch 12 learning rate: 0.01
Epoch 12 time: 356.7973268032074 seconds
Epoch 12 accuracy: 10.84%
Batch 25, Loss: 1.9334
Batch 50, Loss: 1.9298
Batch 75, Loss: 1.9264
Batch 100, Loss: 1.9229
Batch 125, Loss: 1.9195
Batch 150, Loss: 1.9161
Batch 175, Loss: 1.9127
Noise applied in 1952 out of 2496 batches, 78.21
Epoch 13 learning rate: 0.01
Epoch 13 time: 356.4212393760681 seconds
Epoch 13 accuracy: 10.75%
Batch 25, Loss: 1.9072
Batch 50, Loss: 1.9039
Batch 75, Loss: 1.9007
Batch 100, Loss: 1.8976
Batch 125, Loss: 1.8945
Batch 150, Loss: 1.8914
Batch 175, Loss: 1.8884
Noise applied in 2144 out of 2688 batches, 79.76
Epoch 14 learning rate: 0.01
Epoch 14 time: 367.56399488449097 seconds
Epoch 14 accuracy: 10.74%
Batch 25, Loss: 1.8834
Batch 50, Loss: 1.8806
Batch 75, Loss: 1.8777
Batch 100, Loss: 1.8749
Batch 125, Loss: 1.8722
Batch 150, Loss: 1.8695
Batch 175, Loss: 1.8667
Noise applied in 2336 out of 2880 batches, 81.11
Epoch 15 learning rate: 0.01
Epoch 15 time: 401.550000667572 seconds
Epoch 15 accuracy: 10.67%
Batch 25, Loss: 1.8621
Batch 50, Loss: 1.8594
Batch 75, Loss: 1.8569
Batch 100, Loss: 1.8544
Batch 125, Loss: 1.8518
Batch 150, Loss: 1.8494
Batch 175, Loss: 1.8471
Noise applied in 2528 out of 3072 batches, 82.29
Epoch 16 learning rate: 0.01
Epoch 16 time: 357.3149881362915 seconds
Epoch 16 accuracy: 10.65%
Batch 25, Loss: 1.8432
Batch 50, Loss: 1.8410
Batch 75, Loss: 1.8388
Batch 100, Loss: 1.8366
Batch 125, Loss: 1.8345
Batch 150, Loss: 1.8324
Batch 175, Loss: 1.8303
Noise applied in 2720 out of 3264 batches, 83.33
Epoch 17 learning rate: 0.01
Epoch 17 time: 357.80923080444336 seconds
Epoch 17 accuracy: 10.58%
Batch 25, Loss: 1.8269
Batch 50, Loss: 1.8249
Batch 75, Loss: 1.8229
Batch 100, Loss: 1.8210
Batch 125, Loss: 1.8192
Batch 150, Loss: 1.8174
Batch 175, Loss: 1.8157
Noise applied in 2912 out of 3456 batches, 84.26
Epoch 18 learning rate: 0.01
Epoch 18 time: 408.69492173194885 seconds
Epoch 18 accuracy: 10.34%
Batch 25, Loss: 1.8127
Batch 50, Loss: 1.8110
Batch 75, Loss: 1.8094
Batch 100, Loss: 1.8078
Batch 125, Loss: 1.8063
Batch 150, Loss: 1.8048
Batch 175, Loss: 1.8033
Noise applied in 3104 out of 3648 batches, 85.09
Epoch 19 learning rate: 0.01
Epoch 19 time: 356.75359535217285 seconds
Epoch 19 accuracy: 10.36%
Batch 25, Loss: 1.8009
Batch 50, Loss: 1.7996
Batch 75, Loss: 1.7982
Batch 100, Loss: 1.7969
Batch 125, Loss: 1.7956
Batch 150, Loss: 1.7944
Batch 175, Loss: 1.7932
Noise applied in 3296 out of 3840 batches, 85.83
Epoch 20 learning rate: 0.01
Epoch 20 time: 362.2826075553894 seconds
Epoch 20 accuracy: 10.71%
rho:  0.04 , alpha:  0.3
Total training time: 7008.808130979538 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 2.3394
Norm of the Gradient: 4.2329084873e-01
Smallest Hessian Eigenvalue: -0.1570
Noise Threshold: 0.8
Noise Radius: 0.25
