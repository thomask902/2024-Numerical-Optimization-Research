The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:216: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GAM/basicaug/lr-0.1/batchsize-128/2024-08-04-19:31:19
Batch 10, Loss: 3.8124
Batch 20, Loss: 2.3238
Batch 30, Loss: 2.0311
Batch 40, Loss: 2.0974
Batch 50, Loss: 1.7364
Batch 60, Loss: 1.6927
Batch 70, Loss: 1.6278
Batch 80, Loss: 1.6306
Batch 90, Loss: 1.6043
Batch 100, Loss: 1.5859
Batch 110, Loss: 1.5712
Batch 120, Loss: 1.5457
Batch 130, Loss: 1.5874
Batch 140, Loss: 1.5586
Batch 150, Loss: 1.5735
Batch 160, Loss: 1.5321
Batch 170, Loss: 1.5376
Batch 180, Loss: 1.5124
Batch 190, Loss: 1.4969
Batch 200, Loss: 1.4915
Batch 210, Loss: 1.4718
Batch 220, Loss: 1.4667
Batch 230, Loss: 1.4707
Batch 240, Loss: 1.4177
Batch 250, Loss: 1.4410
Batch 260, Loss: 1.4329
Batch 270, Loss: 1.4064
Batch 280, Loss: 1.4717
Batch 290, Loss: 1.4310
Batch 300, Loss: 1.3843
Batch 310, Loss: 1.4087
Batch 320, Loss: 1.4056
Batch 330, Loss: 1.3653
Batch 340, Loss: 1.3731
Batch 350, Loss: 1.3864
Batch 360, Loss: 1.3740
Batch 370, Loss: 1.3501
Batch 380, Loss: 1.3611
Batch 390, Loss: 1.3391
Epoch 1 learning rate: 0.09990133642141358
Epoch 1 time: 105.95935606956482 seconds
Epoch 1 accuracy: 35.91%
Batch 10, Loss: 1.3670
Batch 20, Loss: 1.3313
Batch 30, Loss: 1.3007
Batch 40, Loss: 1.3317
Batch 50, Loss: 1.3332
Batch 60, Loss: 1.3405
Batch 70, Loss: 1.3034
Batch 80, Loss: 1.3570
Batch 90, Loss: 1.3577
Batch 100, Loss: 1.3019
Batch 110, Loss: 1.3000
Batch 120, Loss: 1.2672
Batch 130, Loss: 1.2685
Batch 140, Loss: 1.2828
Batch 150, Loss: 1.2796
Batch 160, Loss: 1.2804
Batch 170, Loss: 1.2479
Batch 180, Loss: 1.2455
Batch 190, Loss: 1.3117
Batch 200, Loss: 1.2617
Batch 210, Loss: 1.2483
Batch 220, Loss: 1.2633
Batch 230, Loss: 1.2421
Batch 240, Loss: 1.2549
Batch 250, Loss: 1.2368
Batch 260, Loss: 1.2710
Batch 270, Loss: 1.2567
Batch 280, Loss: 1.1970
Batch 290, Loss: 1.2142
Batch 300, Loss: 1.2230
Batch 310, Loss: 1.1844
Batch 320, Loss: 1.2189
Batch 330, Loss: 1.2100
Batch 340, Loss: 1.1965
Batch 350, Loss: 1.2194
Batch 360, Loss: 1.2207
Batch 370, Loss: 1.1982
Batch 380, Loss: 1.1632
Batch 390, Loss: 1.2052
Epoch 2 learning rate: 0.0996057350657239
Epoch 2 time: 99.55289578437805 seconds
Epoch 2 accuracy: 41.6%
Batch 10, Loss: 1.2139
Batch 20, Loss: 1.1932
Batch 30, Loss: 1.1759
Batch 40, Loss: 1.1294
Batch 50, Loss: 1.1524
Batch 60, Loss: 1.1473
Batch 70, Loss: 1.1646
Batch 80, Loss: 1.1464
Batch 90, Loss: 1.1264
Batch 100, Loss: 1.1533
Batch 110, Loss: 1.1268
Batch 120, Loss: 1.1006
Batch 130, Loss: 1.1114
Batch 140, Loss: 1.1037
Batch 150, Loss: 1.0948
Batch 160, Loss: 1.0842
Batch 170, Loss: 1.1199
Batch 180, Loss: 1.1088
Batch 190, Loss: 1.0974
Batch 200, Loss: 1.1473
Batch 210, Loss: 1.0791
Batch 220, Loss: 1.1238
Batch 230, Loss: 1.0971
Batch 240, Loss: 1.0873
Batch 250, Loss: 1.0696
Batch 260, Loss: 1.0794
Batch 270, Loss: 1.0600
Batch 280, Loss: 1.0580
Batch 290, Loss: 1.0685
Batch 300, Loss: 1.0679
Batch 310, Loss: 1.0748
Batch 320, Loss: 1.0429
Batch 330, Loss: 1.0664
Batch 340, Loss: 1.0303
Batch 350, Loss: 1.0484
Batch 360, Loss: 1.0234
Batch 370, Loss: 1.0176
Batch 380, Loss: 1.0070
Batch 390, Loss: 0.9999
Epoch 3 learning rate: 0.09911436253643444
Epoch 3 time: 99.59317588806152 seconds
Epoch 3 accuracy: 52.38%
Batch 10, Loss: 0.9738
Batch 20, Loss: 1.0324
Batch 30, Loss: 1.0381
Batch 40, Loss: 1.0145
Batch 50, Loss: 0.9839
Batch 60, Loss: 0.9762
Batch 70, Loss: 0.9889
Batch 80, Loss: 0.9902
Batch 90, Loss: 0.9225
Batch 100, Loss: 0.9824
Batch 110, Loss: 0.9647
Batch 120, Loss: 0.9787
Batch 130, Loss: 0.9390
Batch 140, Loss: 0.9344
Batch 150, Loss: 0.9773
Batch 160, Loss: 0.9526
Batch 170, Loss: 0.9582
Batch 180, Loss: 0.9661
Batch 190, Loss: 0.9124
Batch 200, Loss: 0.8967
Batch 210, Loss: 0.8887
Batch 220, Loss: 0.8719
Batch 230, Loss: 0.9176
Batch 240, Loss: 0.8996
Batch 250, Loss: 0.9010
Batch 260, Loss: 0.9030
Batch 270, Loss: 0.8782
Batch 280, Loss: 0.8892
Batch 290, Loss: 0.9128
Batch 300, Loss: 0.9053
Batch 310, Loss: 0.8730
Batch 320, Loss: 0.8605
Batch 330, Loss: 0.8466
Batch 340, Loss: 0.8550
Batch 350, Loss: 0.8521
Batch 360, Loss: 0.8620
Batch 370, Loss: 0.8613
Batch 380, Loss: 0.8639
Batch 390, Loss: 0.8998
Epoch 4 learning rate: 0.09842915805643154
Epoch 4 time: 99.51803636550903 seconds
Epoch 4 accuracy: 62.31%
Batch 10, Loss: 0.8218
Batch 20, Loss: 0.7855
Batch 30, Loss: 0.8119
Batch 40, Loss: 0.7846
Batch 50, Loss: 0.7572
Batch 60, Loss: 0.8241
Batch 70, Loss: 0.7951
Batch 80, Loss: 0.7705
Batch 90, Loss: 0.7819
Batch 100, Loss: 0.7893
Batch 110, Loss: 0.8160
Batch 120, Loss: 0.7745
Batch 130, Loss: 0.8282
Batch 140, Loss: 0.7478
Batch 150, Loss: 0.8007
Batch 160, Loss: 0.8010
Batch 170, Loss: 0.7911
Batch 180, Loss: 0.7893
Batch 190, Loss: 0.8352
Batch 200, Loss: 0.7674
Batch 210, Loss: 0.7420
Batch 220, Loss: 0.7153
Batch 230, Loss: 0.7197
Batch 240, Loss: 0.7533
Batch 250, Loss: 0.7827
Batch 260, Loss: 0.7663
Batch 270, Loss: 0.7766
Batch 280, Loss: 0.7239
Batch 290, Loss: 0.7418
Batch 300, Loss: 0.7463
Batch 310, Loss: 0.7358
Batch 320, Loss: 0.7083
Batch 330, Loss: 0.7123
Batch 340, Loss: 0.6935
Batch 350, Loss: 0.7059
Batch 360, Loss: 0.7096
Batch 370, Loss: 0.7131
Batch 380, Loss: 0.6481
Batch 390, Loss: 0.7367
Epoch 5 learning rate: 0.09755282581475767
Epoch 5 time: 99.65621709823608 seconds
Epoch 5 accuracy: 67.63%
Batch 10, Loss: 0.6790
Batch 20, Loss: 0.7191
Batch 30, Loss: 0.6742
Batch 40, Loss: 0.7022
Batch 50, Loss: 0.6852
Batch 60, Loss: 0.6690
Batch 70, Loss: 0.6921
Batch 80, Loss: 0.6785
Batch 90, Loss: 0.6897
Batch 100, Loss: 0.6651
Batch 110, Loss: 0.6477
Batch 120, Loss: 0.6516
Batch 130, Loss: 0.6952
Batch 140, Loss: 0.6542
Batch 150, Loss: 0.6790
Batch 160, Loss: 0.6723
Batch 170, Loss: 0.6110
Batch 180, Loss: 0.6377
Batch 190, Loss: 0.6305
Batch 200, Loss: 0.6684
Batch 210, Loss: 0.6248
Batch 220, Loss: 0.6710
Batch 230, Loss: 0.6836
Batch 240, Loss: 0.6243
Batch 250, Loss: 0.6282
Batch 260, Loss: 0.6137
Batch 270, Loss: 0.6277
Batch 280, Loss: 0.6242
Batch 290, Loss: 0.6265
Batch 300, Loss: 0.6158
Batch 310, Loss: 0.6209
Batch 320, Loss: 0.6156
Batch 330, Loss: 0.6232
Batch 340, Loss: 0.6105
Batch 350, Loss: 0.6347
Batch 360, Loss: 0.5890
Batch 370, Loss: 0.6209
Batch 380, Loss: 0.6506
Batch 390, Loss: 0.6244
Epoch 6 learning rate: 0.09648882429441256
Epoch 6 time: 99.5135269165039 seconds
Epoch 6 accuracy: 73.59%
Batch 10, Loss: 0.5653
Batch 20, Loss: 0.6008
Batch 30, Loss: 0.6139
Batch 40, Loss: 0.5792
Batch 50, Loss: 0.5399
Batch 60, Loss: 0.6091
Batch 70, Loss: 0.5709
Batch 80, Loss: 0.6051
Batch 90, Loss: 0.5475
Batch 100, Loss: 0.5958
Batch 110, Loss: 0.5840
Batch 120, Loss: 0.5426
Batch 130, Loss: 0.5863
Batch 140, Loss: 0.6219
Batch 150, Loss: 0.5294
Batch 160, Loss: 0.5567
Batch 170, Loss: 0.5609
Batch 180, Loss: 0.5072
Batch 190, Loss: 0.5673
Batch 200, Loss: 0.5941
Batch 210, Loss: 0.5623
Batch 220, Loss: 0.5224
Batch 230, Loss: 0.5426
Batch 240, Loss: 0.5111
Batch 250, Loss: 0.5356
Batch 260, Loss: 0.5067
Batch 270, Loss: 0.4926
Batch 280, Loss: 0.5720
Batch 290, Loss: 0.5283
Batch 300, Loss: 0.5790
Batch 310, Loss: 0.5570
Batch 320, Loss: 0.5251
Batch 330, Loss: 0.5389
Batch 340, Loss: 0.4924
Batch 350, Loss: 0.5397
Batch 360, Loss: 0.5064
Batch 370, Loss: 0.5010
Batch 380, Loss: 0.5017
Batch 390, Loss: 0.5073
Epoch 7 learning rate: 0.09524135262330098
Epoch 7 time: 99.53489923477173 seconds
Epoch 7 accuracy: 74.75%
Batch 10, Loss: 0.5268
Batch 20, Loss: 0.4942
Batch 30, Loss: 0.4820
Batch 40, Loss: 0.5174
Batch 50, Loss: 0.4870
Batch 60, Loss: 0.5147
Batch 70, Loss: 0.5212
Batch 80, Loss: 0.4785
Batch 90, Loss: 0.5081
Batch 100, Loss: 0.4979
Batch 110, Loss: 0.4782
Batch 120, Loss: 0.4874
Batch 130, Loss: 0.5167
Batch 140, Loss: 0.4933
Batch 150, Loss: 0.4706
Batch 160, Loss: 0.5249
Batch 170, Loss: 0.4809
Batch 180, Loss: 0.4830
Batch 190, Loss: 0.5298
Batch 200, Loss: 0.4671
Batch 210, Loss: 0.5038
Batch 220, Loss: 0.4590
Batch 230, Loss: 0.4724
Batch 240, Loss: 0.4964
Batch 250, Loss: 0.4678
Batch 260, Loss: 0.4654
Batch 270, Loss: 0.4311
Batch 280, Loss: 0.4489
Batch 290, Loss: 0.4881
Batch 300, Loss: 0.4482
Batch 310, Loss: 0.4714
Batch 320, Loss: 0.4746
Batch 330, Loss: 0.4421
Batch 340, Loss: 0.4741
Batch 350, Loss: 0.4670
Batch 360, Loss: 0.5104
Batch 370, Loss: 0.4770
Batch 380, Loss: 0.4546
Batch 390, Loss: 0.4757
Epoch 8 learning rate: 0.09381533400219318
Epoch 8 time: 99.45358324050903 seconds
Epoch 8 accuracy: 79.83%
Batch 10, Loss: 0.4111
Batch 20, Loss: 0.4321
Batch 30, Loss: 0.4525
Batch 40, Loss: 0.4617
Batch 50, Loss: 0.4501
Batch 60, Loss: 0.4562
Batch 70, Loss: 0.4419
Batch 80, Loss: 0.4276
Batch 90, Loss: 0.4565
Batch 100, Loss: 0.4308
Batch 110, Loss: 0.4418
Batch 120, Loss: 0.4364
Batch 130, Loss: 0.4183
Batch 140, Loss: 0.4398
Batch 150, Loss: 0.4302
Batch 160, Loss: 0.4293
Batch 170, Loss: 0.4622
Batch 180, Loss: 0.4404
Batch 190, Loss: 0.4662
Batch 200, Loss: 0.4053
Batch 210, Loss: 0.4397
Batch 220, Loss: 0.4204
Batch 230, Loss: 0.4423
Batch 240, Loss: 0.4440
Batch 250, Loss: 0.4239
Batch 260, Loss: 0.4062
Batch 270, Loss: 0.4172
Batch 280, Loss: 0.4143
Batch 290, Loss: 0.4390
Batch 300, Loss: 0.4276
Batch 310, Loss: 0.4446
Batch 320, Loss: 0.4447
Batch 330, Loss: 0.4373
Batch 340, Loss: 0.4263
Batch 350, Loss: 0.4324
Batch 360, Loss: 0.4679
Batch 370, Loss: 0.4151
Batch 380, Loss: 0.4406
Batch 390, Loss: 0.4126
Epoch 9 learning rate: 0.09221639627510075
Epoch 9 time: 99.57757472991943 seconds
Epoch 9 accuracy: 81.98%
Batch 10, Loss: 0.4339
Batch 20, Loss: 0.4250
Batch 30, Loss: 0.3937
Batch 40, Loss: 0.4103
Batch 50, Loss: 0.4065
Batch 60, Loss: 0.4055
Batch 70, Loss: 0.4043
Batch 80, Loss: 0.3686
Batch 90, Loss: 0.4078
Batch 100, Loss: 0.4010
Batch 110, Loss: 0.3779
Batch 120, Loss: 0.3836
Batch 130, Loss: 0.3928
Batch 140, Loss: 0.3863
Batch 150, Loss: 0.3981
Batch 160, Loss: 0.4252
Batch 170, Loss: 0.4038
Batch 180, Loss: 0.3589
Batch 190, Loss: 0.4338
Batch 200, Loss: 0.3795
Batch 210, Loss: 0.4044
Batch 220, Loss: 0.4055
Batch 230, Loss: 0.3948
Batch 240, Loss: 0.4083
Batch 250, Loss: 0.4102
Batch 260, Loss: 0.4212
Batch 270, Loss: 0.3876
Batch 280, Loss: 0.4107
Batch 290, Loss: 0.3690
Batch 300, Loss: 0.3471
Batch 310, Loss: 0.3843
Batch 320, Loss: 0.4147
Batch 330, Loss: 0.4017
Batch 340, Loss: 0.4090
Batch 350, Loss: 0.3896
Batch 360, Loss: 0.3830
Batch 370, Loss: 0.3978
Batch 380, Loss: 0.3858
Batch 390, Loss: 0.4465
Epoch 10 learning rate: 0.09045084971874737
Epoch 10 time: 99.58298993110657 seconds
Epoch 10 accuracy: 83.02%
Batch 10, Loss: 0.3618
Batch 20, Loss: 0.3563
Batch 30, Loss: 0.3634
Batch 40, Loss: 0.3378
Batch 50, Loss: 0.3847
Batch 60, Loss: 0.3865
Batch 70, Loss: 0.3634
Batch 80, Loss: 0.3586
Batch 90, Loss: 0.3870
Batch 100, Loss: 0.4088
Batch 110, Loss: 0.3511
Batch 120, Loss: 0.3889
Batch 130, Loss: 0.3728
Batch 140, Loss: 0.3689
Batch 150, Loss: 0.3925
Batch 160, Loss: 0.3934
Batch 170, Loss: 0.4075
Batch 180, Loss: 0.3548
Batch 190, Loss: 0.3642
Batch 200, Loss: 0.3611
Batch 210, Loss: 0.3980
Batch 220, Loss: 0.3665
Batch 230, Loss: 0.3776
Batch 240, Loss: 0.3698
Batch 250, Loss: 0.3600
Batch 260, Loss: 0.3311
Batch 270, Loss: 0.3705
Batch 280, Loss: 0.3631
Batch 290, Loss: 0.3872
Batch 300, Loss: 0.3895
Batch 310, Loss: 0.3707
Batch 320, Loss: 0.3865
Batch 330, Loss: 0.3657
Batch 340, Loss: 0.3623
Batch 350, Loss: 0.3513
Batch 360, Loss: 0.3460
Batch 370, Loss: 0.4134
Batch 380, Loss: 0.3811
Batch 390, Loss: 0.3771
Epoch 11 learning rate: 0.08852566213878946
Epoch 11 time: 99.45429682731628 seconds
Epoch 11 accuracy: 83.78%
Batch 10, Loss: 0.3651
Batch 20, Loss: 0.3641
Batch 30, Loss: 0.3404
Batch 40, Loss: 0.3732
Batch 50, Loss: 0.3498
Batch 60, Loss: 0.3694
Batch 70, Loss: 0.3572
Batch 80, Loss: 0.3362
Batch 90, Loss: 0.3422
Batch 100, Loss: 0.3409
Batch 110, Loss: 0.3533
Batch 120, Loss: 0.3387
Batch 130, Loss: 0.3357
Batch 140, Loss: 0.3577
Batch 150, Loss: 0.3575
Batch 160, Loss: 0.3434
Batch 170, Loss: 0.3597
Batch 180, Loss: 0.3361
Batch 190, Loss: 0.3507
Batch 200, Loss: 0.3587
Batch 210, Loss: 0.3328
Batch 220, Loss: 0.3542
Batch 230, Loss: 0.3390
Batch 240, Loss: 0.3670
Batch 250, Loss: 0.3821
Batch 260, Loss: 0.3660
Batch 270, Loss: 0.3405
Batch 280, Loss: 0.3682
Batch 290, Loss: 0.3362
Batch 300, Loss: 0.3415
Batch 310, Loss: 0.3583
Batch 320, Loss: 0.3493
Batch 330, Loss: 0.3830
Batch 340, Loss: 0.3477
Batch 350, Loss: 0.3340
Batch 360, Loss: 0.3416
Batch 370, Loss: 0.3328
Batch 380, Loss: 0.3712
Batch 390, Loss: 0.3598
Epoch 12 learning rate: 0.08644843137107057
Epoch 12 time: 99.47192049026489 seconds
Epoch 12 accuracy: 83.27%
Batch 10, Loss: 0.3358
Batch 20, Loss: 0.3297
Batch 30, Loss: 0.3526
Batch 40, Loss: 0.3101
Batch 50, Loss: 0.3182
Batch 60, Loss: 0.3247
Batch 70, Loss: 0.3212
Batch 80, Loss: 0.3370
Batch 90, Loss: 0.3262
Batch 100, Loss: 0.2964
Batch 110, Loss: 0.3117
Batch 120, Loss: 0.3475
Batch 130, Loss: 0.3564
Batch 140, Loss: 0.3465
Batch 150, Loss: 0.3392
Batch 160, Loss: 0.2943
Batch 170, Loss: 0.3320
Batch 180, Loss: 0.3465
Batch 190, Loss: 0.3332
Batch 200, Loss: 0.3488
Batch 210, Loss: 0.2953
Batch 220, Loss: 0.3163
Batch 230, Loss: 0.3209
Batch 240, Loss: 0.3269
Batch 250, Loss: 0.3256
Batch 260, Loss: 0.3347
Batch 270, Loss: 0.3180
Batch 280, Loss: 0.3186
Batch 290, Loss: 0.3306
Batch 300, Loss: 0.2952
Batch 310, Loss: 0.3361
Batch 320, Loss: 0.3328
Batch 330, Loss: 0.3246
Batch 340, Loss: 0.3371
Batch 350, Loss: 0.3405
Batch 360, Loss: 0.3374
Batch 370, Loss: 0.3427
Batch 380, Loss: 0.3534
Batch 390, Loss: 0.3537
Epoch 13 learning rate: 0.08422735529643442
Epoch 13 time: 99.569983959198 seconds
Epoch 13 accuracy: 84.58%
Batch 10, Loss: 0.3270
Batch 20, Loss: 0.3255
Batch 30, Loss: 0.3068
Batch 40, Loss: 0.3369
Batch 50, Loss: 0.3127
Batch 60, Loss: 0.3379
Batch 70, Loss: 0.3274
Batch 80, Loss: 0.3115
Batch 90, Loss: 0.2948
Batch 100, Loss: 0.3376
Batch 110, Loss: 0.3108
Batch 120, Loss: 0.3064
Batch 130, Loss: 0.3077
Batch 140, Loss: 0.3189
Batch 150, Loss: 0.3238
Batch 160, Loss: 0.3273
Batch 170, Loss: 0.2965
Batch 180, Loss: 0.3105
Batch 190, Loss: 0.3215
Batch 200, Loss: 0.3270
Batch 210, Loss: 0.3485
Batch 220, Loss: 0.3323
Batch 230, Loss: 0.3029
Batch 240, Loss: 0.3020
Batch 250, Loss: 0.3261
Batch 260, Loss: 0.3358
Batch 270, Loss: 0.3257
Batch 280, Loss: 0.3303
Batch 290, Loss: 0.2931
Batch 300, Loss: 0.2935
Batch 310, Loss: 0.3339
Batch 320, Loss: 0.3343
Batch 330, Loss: 0.3330
Batch 340, Loss: 0.3213
Batch 350, Loss: 0.3192
Batch 360, Loss: 0.3120
Batch 370, Loss: 0.3270
Batch 380, Loss: 0.2871
Batch 390, Loss: 0.3143
Epoch 14 learning rate: 0.08187119948743447
Epoch 14 time: 99.45178174972534 seconds
Epoch 14 accuracy: 86.54%
Batch 10, Loss: 0.2913
Batch 20, Loss: 0.2955
Batch 30, Loss: 0.3125
Batch 40, Loss: 0.2927
Batch 50, Loss: 0.2817
Batch 60, Loss: 0.2996
Batch 70, Loss: 0.3059
Batch 80, Loss: 0.2767
Batch 90, Loss: 0.3001
Batch 100, Loss: 0.2798
Batch 110, Loss: 0.2880
Batch 120, Loss: 0.2960
Batch 130, Loss: 0.3146
Batch 140, Loss: 0.2954
Batch 150, Loss: 0.3165
Batch 160, Loss: 0.2820
Batch 170, Loss: 0.2655
Batch 180, Loss: 0.3083
Batch 190, Loss: 0.2987
Batch 200, Loss: 0.3208
Batch 210, Loss: 0.2864
Batch 220, Loss: 0.3356
Batch 230, Loss: 0.3053
Batch 240, Loss: 0.3013
Batch 250, Loss: 0.2957
Batch 260, Loss: 0.2935
Batch 270, Loss: 0.2874
Batch 280, Loss: 0.2909
Batch 290, Loss: 0.2989
Batch 300, Loss: 0.3353
Batch 310, Loss: 0.3136
Batch 320, Loss: 0.3261
Batch 330, Loss: 0.3118
Batch 340, Loss: 0.3169
Batch 350, Loss: 0.2746
Batch 360, Loss: 0.3103
Batch 370, Loss: 0.2955
Batch 380, Loss: 0.2939
Batch 390, Loss: 0.3097
Epoch 15 learning rate: 0.07938926261462366
Epoch 15 time: 99.44847774505615 seconds
Epoch 15 accuracy: 83.84%
Batch 10, Loss: 0.2965
Batch 20, Loss: 0.2773
Batch 30, Loss: 0.3016
Batch 40, Loss: 0.2799
Batch 50, Loss: 0.2930
Batch 60, Loss: 0.2596
Batch 70, Loss: 0.2789
Batch 80, Loss: 0.2845
Batch 90, Loss: 0.2765
Batch 100, Loss: 0.2799
Batch 110, Loss: 0.2912
Batch 120, Loss: 0.2727
Batch 130, Loss: 0.2550
Batch 140, Loss: 0.2777
Batch 150, Loss: 0.2832
Batch 160, Loss: 0.3139
Batch 170, Loss: 0.2902
Batch 180, Loss: 0.3148
Batch 190, Loss: 0.2927
Batch 200, Loss: 0.3049
Batch 210, Loss: 0.2836
Batch 220, Loss: 0.2920
Batch 230, Loss: 0.2862
Batch 240, Loss: 0.2754
Batch 250, Loss: 0.2902
Batch 260, Loss: 0.2815
Batch 270, Loss: 0.2879
Batch 280, Loss: 0.2879
Batch 290, Loss: 0.2677
Batch 300, Loss: 0.2727
Batch 310, Loss: 0.2765
Batch 320, Loss: 0.2963
Batch 330, Loss: 0.2599
Batch 340, Loss: 0.2679
Batch 350, Loss: 0.3123
Batch 360, Loss: 0.2686
Batch 370, Loss: 0.2937
Batch 380, Loss: 0.2907
Batch 390, Loss: 0.2900
Epoch 16 learning rate: 0.07679133974894982
Epoch 16 time: 99.39115333557129 seconds
Epoch 16 accuracy: 86.77%
Batch 10, Loss: 0.2844
Batch 20, Loss: 0.2864
Batch 30, Loss: 0.2608
Batch 40, Loss: 0.2890
Batch 50, Loss: 0.2673
Batch 60, Loss: 0.2378
Batch 70, Loss: 0.2434
Batch 80, Loss: 0.2796
Batch 90, Loss: 0.2362
Batch 100, Loss: 0.2400
Batch 110, Loss: 0.2705
Batch 120, Loss: 0.2828
Batch 130, Loss: 0.2701
Batch 140, Loss: 0.2574
Batch 150, Loss: 0.2797
Batch 160, Loss: 0.2715
Batch 170, Loss: 0.2787
Batch 180, Loss: 0.2700
Batch 190, Loss: 0.2832
Batch 200, Loss: 0.2578
Batch 210, Loss: 0.2759
Batch 220, Loss: 0.2846
Batch 230, Loss: 0.2721
Batch 240, Loss: 0.2908
Batch 250, Loss: 0.2871
Batch 260, Loss: 0.2774
Batch 270, Loss: 0.2647
Batch 280, Loss: 0.2569
Batch 290, Loss: 0.2694
Batch 300, Loss: 0.2683
Batch 310, Loss: 0.2776
Batch 320, Loss: 0.2665
Batch 330, Loss: 0.2682
Batch 340, Loss: 0.2641
Batch 350, Loss: 0.2773
Batch 360, Loss: 0.2611
Batch 370, Loss: 0.2491
Batch 380, Loss: 0.2862
Batch 390, Loss: 0.2673
Epoch 17 learning rate: 0.07408768370508576
Epoch 17 time: 99.57093548774719 seconds
Epoch 17 accuracy: 87.11%
Batch 10, Loss: 0.2442
Batch 20, Loss: 0.2585
Batch 30, Loss: 0.2391
Batch 40, Loss: 0.2432
Batch 50, Loss: 0.2741
Batch 60, Loss: 0.2380
Batch 70, Loss: 0.2451
Batch 80, Loss: 0.2597
Batch 90, Loss: 0.2367
Batch 100, Loss: 0.2753
Batch 110, Loss: 0.2701
Batch 120, Loss: 0.2620
Batch 130, Loss: 0.2430
Batch 140, Loss: 0.2402
Batch 150, Loss: 0.2404
Batch 160, Loss: 0.2400
Batch 170, Loss: 0.2530
Batch 180, Loss: 0.2582
Batch 190, Loss: 0.2400
Batch 200, Loss: 0.2605
Batch 210, Loss: 0.2710
Batch 220, Loss: 0.2753
Batch 230, Loss: 0.2696
Batch 240, Loss: 0.2586
Batch 250, Loss: 0.2576
Batch 260, Loss: 0.2449
Batch 270, Loss: 0.2579
Batch 280, Loss: 0.2638
Batch 290, Loss: 0.2766
Batch 300, Loss: 0.2507
Batch 310, Loss: 0.2612
Batch 320, Loss: 0.2642
Batch 330, Loss: 0.2657
Batch 340, Loss: 0.2575
Batch 350, Loss: 0.2813
Batch 360, Loss: 0.2938
Batch 370, Loss: 0.3092
Batch 380, Loss: 0.2768
Batch 390, Loss: 0.2665
Epoch 18 learning rate: 0.07128896457825362
Epoch 18 time: 99.58066916465759 seconds
Epoch 18 accuracy: 86.11%
Batch 10, Loss: 0.2501
Batch 20, Loss: 0.2539
Batch 30, Loss: 0.2151
Batch 40, Loss: 0.2288
Batch 50, Loss: 0.2390
Batch 60, Loss: 0.2640
Batch 70, Loss: 0.2666
Batch 80, Loss: 0.2333
Batch 90, Loss: 0.2643
Batch 100, Loss: 0.2410
Batch 110, Loss: 0.2337
Batch 120, Loss: 0.2608
Batch 130, Loss: 0.2540
Batch 140, Loss: 0.2270
Batch 150, Loss: 0.2374
Batch 160, Loss: 0.2437
Batch 170, Loss: 0.2403
Batch 180, Loss: 0.2572
Batch 190, Loss: 0.2421
Batch 200, Loss: 0.2399
Batch 210, Loss: 0.2630
Batch 220, Loss: 0.2524
Batch 230, Loss: 0.2302
Batch 240, Loss: 0.2367
Batch 250, Loss: 0.2582
Batch 260, Loss: 0.2559
Batch 270, Loss: 0.2380
Batch 280, Loss: 0.2681
Batch 290, Loss: 0.2719
Batch 300, Loss: 0.2395
Batch 310, Loss: 0.2719
Batch 320, Loss: 0.2303
Batch 330, Loss: 0.2642
Batch 340, Loss: 0.2488
Batch 350, Loss: 0.2518
Batch 360, Loss: 0.2609
Batch 370, Loss: 0.2297
Batch 380, Loss: 0.2341
Batch 390, Loss: 0.2649
Epoch 19 learning rate: 0.06840622763423389
Epoch 19 time: 99.5736448764801 seconds
Epoch 19 accuracy: 88.25%
Batch 10, Loss: 0.2291
Batch 20, Loss: 0.2363
Batch 30, Loss: 0.2382
Batch 40, Loss: 0.2216
Batch 50, Loss: 0.2294
Batch 60, Loss: 0.2248
Batch 70, Loss: 0.2201
Batch 80, Loss: 0.2428
Batch 90, Loss: 0.2198
Batch 100, Loss: 0.2346
Batch 110, Loss: 0.2286
Batch 120, Loss: 0.2467
Batch 130, Loss: 0.2479
Batch 140, Loss: 0.2660
Batch 150, Loss: 0.2474
Batch 160, Loss: 0.2239
Batch 170, Loss: 0.2349
Batch 180, Loss: 0.2398
Batch 190, Loss: 0.2327
Batch 200, Loss: 0.2367
Batch 210, Loss: 0.2348
Batch 220, Loss: 0.2089
Batch 230, Loss: 0.2343
Batch 240, Loss: 0.2342
Batch 250, Loss: 0.2372
Batch 260, Loss: 0.2397
Batch 270, Loss: 0.2355
Batch 280, Loss: 0.2360
Batch 290, Loss: 0.2408
Batch 300, Loss: 0.2229
Batch 310, Loss: 0.2345
Batch 320, Loss: 0.2168
Batch 330, Loss: 0.2545
Batch 340, Loss: 0.2537
Batch 350, Loss: 0.2540
Batch 360, Loss: 0.2425
Batch 370, Loss: 0.2506
Batch 380, Loss: 0.2581
Batch 390, Loss: 0.2319
Epoch 20 learning rate: 0.06545084971874736
Epoch 20 time: 99.4977719783783 seconds
Epoch 20 accuracy: 88.63%
Batch 10, Loss: 0.2238
Batch 20, Loss: 0.2041
Batch 30, Loss: 0.2326
Batch 40, Loss: 0.2239
Batch 50, Loss: 0.2077
Batch 60, Loss: 0.2100
Batch 70, Loss: 0.2434
Batch 80, Loss: 0.2299
Batch 90, Loss: 0.2110
Batch 100, Loss: 0.2134
Batch 110, Loss: 0.2293
Batch 120, Loss: 0.2265
Batch 130, Loss: 0.2410
Batch 140, Loss: 0.2320
Batch 150, Loss: 0.2091
Batch 160, Loss: 0.2383
Batch 170, Loss: 0.1964
Batch 180, Loss: 0.2387
Batch 190, Loss: 0.2331
Batch 200, Loss: 0.2146
Batch 210, Loss: 0.2510
Batch 220, Loss: 0.2302
Batch 230, Loss: 0.2291
Batch 240, Loss: 0.2075
Batch 250, Loss: 0.2171
Batch 260, Loss: 0.2512
Batch 270, Loss: 0.2581
Batch 280, Loss: 0.2530
Batch 290, Loss: 0.2473
Batch 300, Loss: 0.2251
Batch 310, Loss: 0.2172
Batch 320, Loss: 0.2421
Batch 330, Loss: 0.2435
Batch 340, Loss: 0.2338
Batch 350, Loss: 0.2046
Batch 360, Loss: 0.2357
Batch 370, Loss: 0.2430
Batch 380, Loss: 0.1987
Batch 390, Loss: 0.2575
Epoch 21 learning rate: 0.06243449435824272
Epoch 21 time: 99.54408574104309 seconds
Epoch 21 accuracy: 88.86%
Batch 10, Loss: 0.2280
Batch 20, Loss: 0.2323
Batch 30, Loss: 0.2084
Batch 40, Loss: 0.1923
Batch 50, Loss: 0.2012
Batch 60, Loss: 0.1936
Batch 70, Loss: 0.2171
Batch 80, Loss: 0.2241
Batch 90, Loss: 0.2345
Batch 100, Loss: 0.2026
Batch 110, Loss: 0.1955
Batch 120, Loss: 0.2232
Batch 130, Loss: 0.2233
Batch 140, Loss: 0.2225
Batch 150, Loss: 0.2298
Batch 160, Loss: 0.2217
Batch 170, Loss: 0.2306
Batch 180, Loss: 0.2173
Batch 190, Loss: 0.1996
Batch 200, Loss: 0.2265
Batch 210, Loss: 0.2229
Batch 220, Loss: 0.1954
Batch 230, Loss: 0.2189
Batch 240, Loss: 0.2041
Batch 250, Loss: 0.2267
Batch 260, Loss: 0.2278
Batch 270, Loss: 0.2246
Batch 280, Loss: 0.1969
Batch 290, Loss: 0.2265
Batch 300, Loss: 0.2354
Batch 310, Loss: 0.2257
Batch 320, Loss: 0.2142
Batch 330, Loss: 0.2372
Batch 340, Loss: 0.2401
Batch 350, Loss: 0.2038
Batch 360, Loss: 0.2173
Batch 370, Loss: 0.2241
Batch 380, Loss: 0.2050
Batch 390, Loss: 0.2016
Epoch 22 learning rate: 0.05936906572928623
Epoch 22 time: 99.57609224319458 seconds
Epoch 22 accuracy: 89.59%
Batch 10, Loss: 0.1915
Batch 20, Loss: 0.1617
Batch 30, Loss: 0.2081
Batch 40, Loss: 0.1891
Batch 50, Loss: 0.2069
Batch 60, Loss: 0.1844
Batch 70, Loss: 0.2083
Batch 80, Loss: 0.1965
Batch 90, Loss: 0.2063
Batch 100, Loss: 0.2165
Batch 110, Loss: 0.1838
Batch 120, Loss: 0.1894
Batch 130, Loss: 0.2023
Batch 140, Loss: 0.1986
Batch 150, Loss: 0.2076
Batch 160, Loss: 0.2278
Batch 170, Loss: 0.2150
Batch 180, Loss: 0.1969
Batch 190, Loss: 0.1942
Batch 200, Loss: 0.1918
Batch 210, Loss: 0.2105
Batch 220, Loss: 0.2153
Batch 230, Loss: 0.1952
Batch 240, Loss: 0.1921
Batch 250, Loss: 0.2134
Batch 260, Loss: 0.2044
Batch 270, Loss: 0.2120
Batch 280, Loss: 0.2028
Batch 290, Loss: 0.2067
Batch 300, Loss: 0.2186
Batch 310, Loss: 0.1956
Batch 320, Loss: 0.2137
Batch 330, Loss: 0.2079
Batch 340, Loss: 0.2148
Batch 350, Loss: 0.1879
Batch 360, Loss: 0.2077
Batch 370, Loss: 0.2152
Batch 380, Loss: 0.1811
Batch 390, Loss: 0.2049
Epoch 23 learning rate: 0.056266661678215216
Epoch 23 time: 99.40135884284973 seconds
Epoch 23 accuracy: 90.7%
Batch 10, Loss: 0.1918
Batch 20, Loss: 0.1918
Batch 30, Loss: 0.1933
Batch 40, Loss: 0.1712
Batch 50, Loss: 0.1945
Batch 60, Loss: 0.1901
Batch 70, Loss: 0.1746
Batch 80, Loss: 0.1933
Batch 90, Loss: 0.1796
Batch 100, Loss: 0.1920
Batch 110, Loss: 0.1814
Batch 120, Loss: 0.1813
Batch 130, Loss: 0.1953
Batch 140, Loss: 0.1820
Batch 150, Loss: 0.2098
Batch 160, Loss: 0.1782
Batch 170, Loss: 0.1986
Batch 180, Loss: 0.1939
Batch 190, Loss: 0.1944
Batch 200, Loss: 0.2245
Batch 210, Loss: 0.1922
Batch 220, Loss: 0.1999
Batch 230, Loss: 0.2047
Batch 240, Loss: 0.1933
Batch 250, Loss: 0.1944
Batch 260, Loss: 0.1842
Batch 270, Loss: 0.1955
Batch 280, Loss: 0.1777
Batch 290, Loss: 0.1905
Batch 300, Loss: 0.1912
Batch 310, Loss: 0.2019
Batch 320, Loss: 0.1990
Batch 330, Loss: 0.1955
Batch 340, Loss: 0.1810
Batch 350, Loss: 0.1979
Batch 360, Loss: 0.2072
Batch 370, Loss: 0.1997
Batch 380, Loss: 0.2194
Batch 390, Loss: 0.1971
Epoch 24 learning rate: 0.053139525976465665
Epoch 24 time: 99.49655222892761 seconds
Epoch 24 accuracy: 90.26%
Batch 10, Loss: 0.1823
Batch 20, Loss: 0.1857
Batch 30, Loss: 0.1693
Batch 40, Loss: 0.1657
Batch 50, Loss: 0.1721
Batch 60, Loss: 0.1889
Batch 70, Loss: 0.1755
Batch 80, Loss: 0.1852
Batch 90, Loss: 0.1907
Batch 100, Loss: 0.1818
Batch 110, Loss: 0.2130
Batch 120, Loss: 0.1834
Batch 130, Loss: 0.1604
Batch 140, Loss: 0.1852
Batch 150, Loss: 0.1837
Batch 160, Loss: 0.1872
Batch 170, Loss: 0.1664
Batch 180, Loss: 0.1810
Batch 190, Loss: 0.1775
Batch 200, Loss: 0.1715
Batch 210, Loss: 0.1971
Batch 220, Loss: 0.1972
Batch 230, Loss: 0.1881
Batch 240, Loss: 0.1722
Batch 250, Loss: 0.1758
Batch 260, Loss: 0.1773
Batch 270, Loss: 0.1884
Batch 280, Loss: 0.2055
Batch 290, Loss: 0.1955
Batch 300, Loss: 0.2102
Batch 310, Loss: 0.2015
Batch 320, Loss: 0.2032
Batch 330, Loss: 0.1907
Batch 340, Loss: 0.1844
Batch 350, Loss: 0.1726
Batch 360, Loss: 0.2029
Batch 370, Loss: 0.1634
Batch 380, Loss: 0.1993
Batch 390, Loss: 0.1855
Epoch 25 learning rate: 0.049999999999999996
Epoch 25 time: 99.53508639335632 seconds
Epoch 25 accuracy: 90.04%
Batch 10, Loss: 0.1750
Batch 20, Loss: 0.1796
Batch 30, Loss: 0.1507
Batch 40, Loss: 0.1768
Batch 50, Loss: 0.1677
Batch 60, Loss: 0.1541
Batch 70, Loss: 0.1608
Batch 80, Loss: 0.1827
Batch 90, Loss: 0.1667
Batch 100, Loss: 0.1719
Batch 110, Loss: 0.2118
Batch 120, Loss: 0.1696
Batch 130, Loss: 0.1906
Batch 140, Loss: 0.1868
Batch 150, Loss: 0.1848
Batch 160, Loss: 0.1688
Batch 170, Loss: 0.1943
Batch 180, Loss: 0.1952
Batch 190, Loss: 0.1702
Batch 200, Loss: 0.1960
Batch 210, Loss: 0.1819
Batch 220, Loss: 0.1825
Batch 230, Loss: 0.1714
Batch 240, Loss: 0.1589
Batch 250, Loss: 0.1705
Batch 260, Loss: 0.1819
Batch 270, Loss: 0.1672
Batch 280, Loss: 0.1851
Batch 290, Loss: 0.1940
Batch 300, Loss: 0.1650
Batch 310, Loss: 0.1604
Batch 320, Loss: 0.1867
Batch 330, Loss: 0.1769
Batch 340, Loss: 0.1956
Batch 350, Loss: 0.1785
Batch 360, Loss: 0.1735
Batch 370, Loss: 0.1723
Batch 380, Loss: 0.1744
Batch 390, Loss: 0.1867
Epoch 26 learning rate: 0.046860474023534326
Epoch 26 time: 99.37395572662354 seconds
Epoch 26 accuracy: 90.93%
Batch 10, Loss: 0.1467
Batch 20, Loss: 0.1593
Batch 30, Loss: 0.1616
Batch 40, Loss: 0.1716
Batch 50, Loss: 0.1866
Batch 60, Loss: 0.1826
Batch 70, Loss: 0.2070
Batch 80, Loss: 0.1648
Batch 90, Loss: 0.1654
Batch 100, Loss: 0.1592
Batch 110, Loss: 0.1598
Batch 120, Loss: 0.1606
Batch 130, Loss: 0.1733
Batch 140, Loss: 0.1655
Batch 150, Loss: 0.1703
Batch 160, Loss: 0.1593
Batch 170, Loss: 0.1744
Batch 180, Loss: 0.1607
Batch 190, Loss: 0.1429
Batch 200, Loss: 0.1726
Batch 210, Loss: 0.1686
Batch 220, Loss: 0.1545
Batch 230, Loss: 0.1718
Batch 240, Loss: 0.1696
Batch 250, Loss: 0.1625
Batch 260, Loss: 0.1651
Batch 270, Loss: 0.1566
Batch 280, Loss: 0.1390
Batch 290, Loss: 0.1712
Batch 300, Loss: 0.1754
Batch 310, Loss: 0.1650
Batch 320, Loss: 0.1692
Batch 330, Loss: 0.1755
Batch 340, Loss: 0.1847
Batch 350, Loss: 0.1904
Batch 360, Loss: 0.1861
Batch 370, Loss: 0.1635
Batch 380, Loss: 0.1672
Batch 390, Loss: 0.1840
Epoch 27 learning rate: 0.04373333832178478
Epoch 27 time: 99.37669801712036 seconds
Epoch 27 accuracy: 91.2%
Batch 10, Loss: 0.1677
Batch 20, Loss: 0.1429
Batch 30, Loss: 0.1699
Batch 40, Loss: 0.1423
Batch 50, Loss: 0.1455
Batch 60, Loss: 0.1621
Batch 70, Loss: 0.1516
Batch 80, Loss: 0.1390
Batch 90, Loss: 0.1692
Batch 100, Loss: 0.1524
Batch 110, Loss: 0.1412
Batch 120, Loss: 0.1697
Batch 130, Loss: 0.1583
Batch 140, Loss: 0.1630
Batch 150, Loss: 0.1711
Batch 160, Loss: 0.1568
Batch 170, Loss: 0.1484
Batch 180, Loss: 0.1445
Batch 190, Loss: 0.1630
Batch 200, Loss: 0.1483
Batch 210, Loss: 0.1560
Batch 220, Loss: 0.1594
Batch 230, Loss: 0.1614
Batch 240, Loss: 0.1631
Batch 250, Loss: 0.1532
Batch 260, Loss: 0.1413
Batch 270, Loss: 0.1401
Batch 280, Loss: 0.1686
Batch 290, Loss: 0.1672
Batch 300, Loss: 0.1576
Batch 310, Loss: 0.1497
Batch 320, Loss: 0.1581
Batch 330, Loss: 0.1555
Batch 340, Loss: 0.1628
Batch 350, Loss: 0.1577
Batch 360, Loss: 0.1733
Batch 370, Loss: 0.1717
Batch 380, Loss: 0.1645
Batch 390, Loss: 0.1627
Epoch 28 learning rate: 0.040630934270713764
Epoch 28 time: 99.43834805488586 seconds
Epoch 28 accuracy: 91.65%
Batch 10, Loss: 0.1452
Batch 20, Loss: 0.1493
Batch 30, Loss: 0.1462
Batch 40, Loss: 0.1472
Batch 50, Loss: 0.1366
Batch 60, Loss: 0.1357
Batch 70, Loss: 0.1556
Batch 80, Loss: 0.1499
Batch 90, Loss: 0.1523
Batch 100, Loss: 0.1410
Batch 110, Loss: 0.1376
Batch 120, Loss: 0.1548
Batch 130, Loss: 0.1395
Batch 140, Loss: 0.1368
Batch 150, Loss: 0.1490
Batch 160, Loss: 0.1487
Batch 170, Loss: 0.1579
Batch 180, Loss: 0.1358
Batch 190, Loss: 0.1662
Batch 200, Loss: 0.1544
Batch 210, Loss: 0.1470
Batch 220, Loss: 0.1602
Batch 230, Loss: 0.1492
Batch 240, Loss: 0.1603
Batch 250, Loss: 0.1667
Batch 260, Loss: 0.1602
Batch 270, Loss: 0.1524
Batch 280, Loss: 0.1511
Batch 290, Loss: 0.1327
Batch 300, Loss: 0.1521
Batch 310, Loss: 0.1420
Batch 320, Loss: 0.1704
Batch 330, Loss: 0.1514
Batch 340, Loss: 0.1546
Batch 350, Loss: 0.1577
Batch 360, Loss: 0.1518
Batch 370, Loss: 0.1527
Batch 380, Loss: 0.1672
Batch 390, Loss: 0.1500
Epoch 29 learning rate: 0.03756550564175726
Epoch 29 time: 99.48133373260498 seconds
Epoch 29 accuracy: 91.69%
Batch 10, Loss: 0.1447
Batch 20, Loss: 0.1386
Batch 30, Loss: 0.1378
Batch 40, Loss: 0.1362
Batch 50, Loss: 0.1383
Batch 60, Loss: 0.1490
Batch 70, Loss: 0.1396
Batch 80, Loss: 0.1407
Batch 90, Loss: 0.1267
Batch 100, Loss: 0.1329
Batch 110, Loss: 0.1160
Batch 120, Loss: 0.1345
Batch 130, Loss: 0.1315
Batch 140, Loss: 0.1292
Batch 150, Loss: 0.1413
Batch 160, Loss: 0.1422
Batch 170, Loss: 0.1416
Batch 180, Loss: 0.1525
Batch 190, Loss: 0.1372
Batch 200, Loss: 0.1308
Batch 210, Loss: 0.1293
Batch 220, Loss: 0.1468
Batch 230, Loss: 0.1285
Batch 240, Loss: 0.1511
Batch 250, Loss: 0.1278
Batch 260, Loss: 0.1525
Batch 270, Loss: 0.1540
Batch 280, Loss: 0.1433
Batch 290, Loss: 0.1459
Batch 300, Loss: 0.1455
Batch 310, Loss: 0.1374
Batch 320, Loss: 0.1321
Batch 330, Loss: 0.1436
Batch 340, Loss: 0.1461
Batch 350, Loss: 0.1339
Batch 360, Loss: 0.1456
Batch 370, Loss: 0.1476
Batch 380, Loss: 0.1579
Batch 390, Loss: 0.1377
Epoch 30 learning rate: 0.03454915028125265
Epoch 30 time: 99.38936591148376 seconds
Epoch 30 accuracy: 91.53%
Batch 10, Loss: 0.1280
Batch 20, Loss: 0.1368
Batch 30, Loss: 0.1268
Batch 40, Loss: 0.1388
Batch 50, Loss: 0.1174
Batch 60, Loss: 0.1285
Batch 70, Loss: 0.1240
Batch 80, Loss: 0.1454
Batch 90, Loss: 0.1157
Batch 100, Loss: 0.1257
Batch 110, Loss: 0.1238
Batch 120, Loss: 0.1272
Batch 130, Loss: 0.1279
Batch 140, Loss: 0.1339
Batch 150, Loss: 0.1501
Batch 160, Loss: 0.1364
Batch 170, Loss: 0.1162
Batch 180, Loss: 0.1251
Batch 190, Loss: 0.1298
Batch 200, Loss: 0.1280
Batch 210, Loss: 0.1288
Batch 220, Loss: 0.1430
Batch 230, Loss: 0.1178
Batch 240, Loss: 0.1300
Batch 250, Loss: 0.1404
Batch 260, Loss: 0.1377
Batch 270, Loss: 0.1246
Batch 280, Loss: 0.1404
Batch 290, Loss: 0.1392
Batch 300, Loss: 0.1162
Batch 310, Loss: 0.1485
Batch 320, Loss: 0.1486
Batch 330, Loss: 0.1359
Batch 340, Loss: 0.1431
Batch 350, Loss: 0.1338
Batch 360, Loss: 0.1375
Batch 370, Loss: 0.1516
Batch 380, Loss: 0.1433
Batch 390, Loss: 0.1304
Epoch 31 learning rate: 0.03159377236576612
Epoch 31 time: 99.58179545402527 seconds
Epoch 31 accuracy: 92.07%
Batch 10, Loss: 0.1203
Batch 20, Loss: 0.1055
Batch 30, Loss: 0.1175
Batch 40, Loss: 0.1248
Batch 50, Loss: 0.1212
Batch 60, Loss: 0.1209
Batch 70, Loss: 0.1282
Batch 80, Loss: 0.1195
Batch 90, Loss: 0.1118
Batch 100, Loss: 0.1116
Batch 110, Loss: 0.1107
Batch 120, Loss: 0.1153
Batch 130, Loss: 0.1102
Batch 140, Loss: 0.1304
Batch 150, Loss: 0.1171
Batch 160, Loss: 0.1248
Batch 170, Loss: 0.1054
Batch 180, Loss: 0.0997
Batch 190, Loss: 0.1163
Batch 200, Loss: 0.1201
Batch 210, Loss: 0.1384
Batch 220, Loss: 0.1199
Batch 230, Loss: 0.1305
Batch 240, Loss: 0.1174
Batch 250, Loss: 0.1336
Batch 260, Loss: 0.1279
Batch 270, Loss: 0.1243
Batch 280, Loss: 0.1211
Batch 290, Loss: 0.1178
Batch 300, Loss: 0.1486
Batch 310, Loss: 0.1185
Batch 320, Loss: 0.1375
Batch 330, Loss: 0.1204
Batch 340, Loss: 0.1245
Batch 350, Loss: 0.1251
Batch 360, Loss: 0.1220
Batch 370, Loss: 0.1373
Batch 380, Loss: 0.1184
Batch 390, Loss: 0.1568
Epoch 32 learning rate: 0.028711035421746366
Epoch 32 time: 99.60987329483032 seconds
Epoch 32 accuracy: 92.81%
Batch 10, Loss: 0.1254
Batch 20, Loss: 0.1106
Batch 30, Loss: 0.1213
Batch 40, Loss: 0.1034
Batch 50, Loss: 0.1230
Batch 60, Loss: 0.1233
Batch 70, Loss: 0.1178
Batch 80, Loss: 0.1095
Batch 90, Loss: 0.1047
Batch 100, Loss: 0.1021
Batch 110, Loss: 0.1017
Batch 120, Loss: 0.1135
Batch 130, Loss: 0.1172
Batch 140, Loss: 0.1198
Batch 150, Loss: 0.1150
Batch 160, Loss: 0.1173
Batch 170, Loss: 0.1067
Batch 180, Loss: 0.1167
Batch 190, Loss: 0.1161
Batch 200, Loss: 0.1226
Batch 210, Loss: 0.1259
Batch 220, Loss: 0.1171
Batch 230, Loss: 0.1267
Batch 240, Loss: 0.1206
Batch 250, Loss: 0.1227
Batch 260, Loss: 0.1119
Batch 270, Loss: 0.1105
Batch 280, Loss: 0.1290
Batch 290, Loss: 0.1282
Batch 300, Loss: 0.1068
Batch 310, Loss: 0.1118
Batch 320, Loss: 0.1219
Batch 330, Loss: 0.1144
Batch 340, Loss: 0.1281
Batch 350, Loss: 0.1291
Batch 360, Loss: 0.1139
Batch 370, Loss: 0.1304
Batch 380, Loss: 0.1143
Batch 390, Loss: 0.1197
Epoch 33 learning rate: 0.025912316294914226
Epoch 33 time: 99.53121709823608 seconds
Epoch 33 accuracy: 92.76%
Batch 10, Loss: 0.1090
Batch 20, Loss: 0.1205
Batch 30, Loss: 0.1102
Batch 40, Loss: 0.1074
Batch 50, Loss: 0.0996
Batch 60, Loss: 0.1038
Batch 70, Loss: 0.1015
Batch 80, Loss: 0.1080
Batch 90, Loss: 0.1124
Batch 100, Loss: 0.1042
Batch 110, Loss: 0.1105
Batch 120, Loss: 0.1155
Batch 130, Loss: 0.1110
Batch 140, Loss: 0.1054
Batch 150, Loss: 0.1062
Batch 160, Loss: 0.1059
Batch 170, Loss: 0.1208
Batch 180, Loss: 0.1157
Batch 190, Loss: 0.1006
Batch 200, Loss: 0.1064
Batch 210, Loss: 0.0981
Batch 220, Loss: 0.1058
Batch 230, Loss: 0.1031
Batch 240, Loss: 0.1101
Batch 250, Loss: 0.1245
Batch 260, Loss: 0.1135
Batch 270, Loss: 0.0986
Batch 280, Loss: 0.1203
Batch 290, Loss: 0.1097
Batch 300, Loss: 0.1062
Batch 310, Loss: 0.1095
Batch 320, Loss: 0.1136
Batch 330, Loss: 0.1097
Batch 340, Loss: 0.1069
Batch 350, Loss: 0.1097
Batch 360, Loss: 0.0898
Batch 370, Loss: 0.1116
Batch 380, Loss: 0.1198
Batch 390, Loss: 0.1153
Epoch 34 learning rate: 0.023208660251050155
Epoch 34 time: 99.43599581718445 seconds
Epoch 34 accuracy: 93.29%
Batch 10, Loss: 0.0981
Batch 20, Loss: 0.1026
Batch 30, Loss: 0.1096
Batch 40, Loss: 0.1043
Batch 50, Loss: 0.0957
Batch 60, Loss: 0.0923
Batch 70, Loss: 0.1009
Batch 80, Loss: 0.0979
Batch 90, Loss: 0.0928
Batch 100, Loss: 0.0950
Batch 110, Loss: 0.1002
Batch 120, Loss: 0.0982
Batch 130, Loss: 0.1113
Batch 140, Loss: 0.1076
Batch 150, Loss: 0.1073
Batch 160, Loss: 0.1062
Batch 170, Loss: 0.1016
Batch 180, Loss: 0.1076
Batch 190, Loss: 0.0996
Batch 200, Loss: 0.0980
Batch 210, Loss: 0.1034
Batch 220, Loss: 0.1004
Batch 230, Loss: 0.1021
Batch 240, Loss: 0.1151
Batch 250, Loss: 0.1069
Batch 260, Loss: 0.0919
Batch 270, Loss: 0.0925
Batch 280, Loss: 0.1063
Batch 290, Loss: 0.1096
Batch 300, Loss: 0.1052
Batch 310, Loss: 0.1067
Batch 320, Loss: 0.1009
Batch 330, Loss: 0.1047
Batch 340, Loss: 0.1145
Batch 350, Loss: 0.1060
Batch 360, Loss: 0.1029
Batch 370, Loss: 0.0970
Batch 380, Loss: 0.1050
Batch 390, Loss: 0.1084
Epoch 35 learning rate: 0.020610737385376346
Epoch 35 time: 99.46710801124573 seconds
Epoch 35 accuracy: 93.46%
Batch 10, Loss: 0.1061
Batch 20, Loss: 0.0935
Batch 30, Loss: 0.1022
Batch 40, Loss: 0.0855
Batch 50, Loss: 0.0874
Batch 60, Loss: 0.0834
Batch 70, Loss: 0.0969
Batch 80, Loss: 0.1029
Batch 90, Loss: 0.0948
Batch 100, Loss: 0.0797
Batch 110, Loss: 0.0998
Batch 120, Loss: 0.0859
Batch 130, Loss: 0.0818
Batch 140, Loss: 0.1022
Batch 150, Loss: 0.1009
Batch 160, Loss: 0.0889
Batch 170, Loss: 0.0965
Batch 180, Loss: 0.1064
Batch 190, Loss: 0.1013
Batch 200, Loss: 0.0915
Batch 210, Loss: 0.0787
Batch 220, Loss: 0.1007
Batch 230, Loss: 0.1025
Batch 240, Loss: 0.0954
Batch 250, Loss: 0.0950
Batch 260, Loss: 0.1161
Batch 270, Loss: 0.1009
Batch 280, Loss: 0.0923
Batch 290, Loss: 0.1081
Batch 300, Loss: 0.0955
Batch 310, Loss: 0.0943
Batch 320, Loss: 0.0939
Batch 330, Loss: 0.0850
Batch 340, Loss: 0.0951
Batch 350, Loss: 0.0962
Batch 360, Loss: 0.0886
Batch 370, Loss: 0.0869
Batch 380, Loss: 0.0948
Batch 390, Loss: 0.0979
Epoch 36 learning rate: 0.01812880051256551
Epoch 36 time: 99.41410970687866 seconds
Epoch 36 accuracy: 93.71%
Batch 10, Loss: 0.0808
Batch 20, Loss: 0.0779
Batch 30, Loss: 0.0915
Batch 40, Loss: 0.0849
Batch 50, Loss: 0.0864
Batch 60, Loss: 0.0864
Batch 70, Loss: 0.0908
Batch 80, Loss: 0.0824
Batch 90, Loss: 0.0830
Batch 100, Loss: 0.0996
Batch 110, Loss: 0.0869
Batch 120, Loss: 0.0848
Batch 130, Loss: 0.0906
Batch 140, Loss: 0.0903
Batch 150, Loss: 0.0906
Batch 160, Loss: 0.0889
Batch 170, Loss: 0.1025
Batch 180, Loss: 0.0930
Batch 190, Loss: 0.0890
Batch 200, Loss: 0.0834
Batch 210, Loss: 0.0992
Batch 220, Loss: 0.0911
Batch 230, Loss: 0.1013
Batch 240, Loss: 0.0855
Batch 250, Loss: 0.0775
Batch 260, Loss: 0.1019
Batch 270, Loss: 0.0949
Batch 280, Loss: 0.0953
Batch 290, Loss: 0.0901
Batch 300, Loss: 0.0963
Batch 310, Loss: 0.0891
Batch 320, Loss: 0.0927
Batch 330, Loss: 0.0857
Batch 340, Loss: 0.0908
Batch 350, Loss: 0.0878
Batch 360, Loss: 0.0857
Batch 370, Loss: 0.0901
Batch 380, Loss: 0.0892
Batch 390, Loss: 0.0975
Epoch 37 learning rate: 0.015772644703565562
Epoch 37 time: 99.40816879272461 seconds
Epoch 37 accuracy: 93.99%
Batch 10, Loss: 0.0801
Batch 20, Loss: 0.0820
Batch 30, Loss: 0.0869
Batch 40, Loss: 0.0685
Batch 50, Loss: 0.0798
Batch 60, Loss: 0.0881
Batch 70, Loss: 0.0845
Batch 80, Loss: 0.0767
Batch 90, Loss: 0.0835
Batch 100, Loss: 0.0774
Batch 110, Loss: 0.0829
Batch 120, Loss: 0.0779
Batch 130, Loss: 0.0680
Batch 140, Loss: 0.0906
Batch 150, Loss: 0.0755
Batch 160, Loss: 0.0806
Batch 170, Loss: 0.0960
Batch 180, Loss: 0.0881
Batch 190, Loss: 0.0955
Batch 200, Loss: 0.0903
Batch 210, Loss: 0.0814
Batch 220, Loss: 0.0916
Batch 230, Loss: 0.0858
Batch 240, Loss: 0.0833
Batch 250, Loss: 0.0775
Batch 260, Loss: 0.0782
Batch 270, Loss: 0.0832
Batch 280, Loss: 0.0868
Batch 290, Loss: 0.0895
Batch 300, Loss: 0.0812
Batch 310, Loss: 0.0935
Batch 320, Loss: 0.0821
Batch 330, Loss: 0.0952
Batch 340, Loss: 0.1038
Batch 350, Loss: 0.0849
Batch 360, Loss: 0.0963
Batch 370, Loss: 0.0780
Batch 380, Loss: 0.0769
Batch 390, Loss: 0.0806
Epoch 38 learning rate: 0.013551568628929433
Epoch 38 time: 99.55555868148804 seconds
Epoch 38 accuracy: 94.04%
Batch 10, Loss: 0.0783
Batch 20, Loss: 0.0817
Batch 30, Loss: 0.0750
Batch 40, Loss: 0.0833
Batch 50, Loss: 0.0785
Batch 60, Loss: 0.0721
Batch 70, Loss: 0.0816
Batch 80, Loss: 0.0780
Batch 90, Loss: 0.0744
Batch 100, Loss: 0.0762
Batch 110, Loss: 0.0753
Batch 120, Loss: 0.0759
Batch 130, Loss: 0.0828
Batch 140, Loss: 0.0805
Batch 150, Loss: 0.0711
Batch 160, Loss: 0.0742
Batch 170, Loss: 0.0771
Batch 180, Loss: 0.0687
Batch 190, Loss: 0.0725
Batch 200, Loss: 0.0802
Batch 210, Loss: 0.0829
Batch 220, Loss: 0.0788
Batch 230, Loss: 0.0806
Batch 240, Loss: 0.0829
Batch 250, Loss: 0.0704
Batch 260, Loss: 0.0816
Batch 270, Loss: 0.0791
Batch 280, Loss: 0.0751
Batch 290, Loss: 0.0803
Batch 300, Loss: 0.0770
Batch 310, Loss: 0.0805
Batch 320, Loss: 0.0672
Batch 330, Loss: 0.0788
Batch 340, Loss: 0.0774
Batch 350, Loss: 0.0810
Batch 360, Loss: 0.0735
Batch 370, Loss: 0.0857
Batch 380, Loss: 0.0893
Batch 390, Loss: 0.0703
Epoch 39 learning rate: 0.011474337861210542
Epoch 39 time: 99.5398998260498 seconds
Epoch 39 accuracy: 94.3%
Batch 10, Loss: 0.0679
Batch 20, Loss: 0.0759
Batch 30, Loss: 0.0788
Batch 40, Loss: 0.0717
Batch 50, Loss: 0.0710
Batch 60, Loss: 0.0714
Batch 70, Loss: 0.0650
Batch 80, Loss: 0.0703
Batch 90, Loss: 0.0668
Batch 100, Loss: 0.0678
Batch 110, Loss: 0.0641
Batch 120, Loss: 0.0678
Batch 130, Loss: 0.0713
Batch 140, Loss: 0.0638
Batch 150, Loss: 0.0750
Batch 160, Loss: 0.0793
Batch 170, Loss: 0.0708
Batch 180, Loss: 0.0715
Batch 190, Loss: 0.0792
Batch 200, Loss: 0.0708
Batch 210, Loss: 0.0737
Batch 220, Loss: 0.0750
Batch 230, Loss: 0.0727
Batch 240, Loss: 0.0723
Batch 250, Loss: 0.0696
Batch 260, Loss: 0.0754
Batch 270, Loss: 0.0833
Batch 280, Loss: 0.0727
Batch 290, Loss: 0.0726
Batch 300, Loss: 0.0689
Batch 310, Loss: 0.0745
Batch 320, Loss: 0.0642
Batch 330, Loss: 0.0790
Batch 340, Loss: 0.0732
Batch 350, Loss: 0.0753
Batch 360, Loss: 0.0713
Batch 370, Loss: 0.0742
Batch 380, Loss: 0.0750
Batch 390, Loss: 0.0795
Epoch 40 learning rate: 0.009549150281252632
Epoch 40 time: 99.4576849937439 seconds
Epoch 40 accuracy: 94.22%
Batch 10, Loss: 0.0676
Batch 20, Loss: 0.0702
Batch 30, Loss: 0.0627
Batch 40, Loss: 0.0597
Batch 50, Loss: 0.0628
Batch 60, Loss: 0.0655
Batch 70, Loss: 0.0656
Batch 80, Loss: 0.0669
Batch 90, Loss: 0.0728
Batch 100, Loss: 0.0760
Batch 110, Loss: 0.0635
Batch 120, Loss: 0.0674
Batch 130, Loss: 0.0580
Batch 140, Loss: 0.0727
Batch 150, Loss: 0.0697
Batch 160, Loss: 0.0784
Batch 170, Loss: 0.0792
Batch 180, Loss: 0.0660
Batch 190, Loss: 0.0583
Batch 200, Loss: 0.0679
Batch 210, Loss: 0.0812
Batch 220, Loss: 0.0669
Batch 230, Loss: 0.0631
Batch 240, Loss: 0.0656
Batch 250, Loss: 0.0694
Batch 260, Loss: 0.0716
Batch 270, Loss: 0.0718
Batch 280, Loss: 0.0613
Batch 290, Loss: 0.0636
Batch 300, Loss: 0.0657
Batch 310, Loss: 0.0690
Batch 320, Loss: 0.0601
Batch 330, Loss: 0.0792
Batch 340, Loss: 0.0793
Batch 350, Loss: 0.0707
Batch 360, Loss: 0.0799
Batch 370, Loss: 0.0685
Batch 380, Loss: 0.0685
Batch 390, Loss: 0.0781
Epoch 41 learning rate: 0.007783603724899257
Epoch 41 time: 99.45322108268738 seconds
Epoch 41 accuracy: 94.38%
Batch 10, Loss: 0.0684
Batch 20, Loss: 0.0600
Batch 30, Loss: 0.0677
Batch 40, Loss: 0.0643
Batch 50, Loss: 0.0695
Batch 60, Loss: 0.0666
Batch 70, Loss: 0.0661
Batch 80, Loss: 0.0666
Batch 90, Loss: 0.0625
Batch 100, Loss: 0.0623
Batch 110, Loss: 0.0686
Batch 120, Loss: 0.0577
Batch 130, Loss: 0.0594
Batch 140, Loss: 0.0636
Batch 150, Loss: 0.0578
Batch 160, Loss: 0.0682
Batch 170, Loss: 0.0672
Batch 180, Loss: 0.0650
Batch 190, Loss: 0.0608
Batch 200, Loss: 0.0676
Batch 210, Loss: 0.0690
Batch 220, Loss: 0.0649
Batch 230, Loss: 0.0575
Batch 240, Loss: 0.0617
Batch 250, Loss: 0.0627
Batch 260, Loss: 0.0637
Batch 270, Loss: 0.0624
Batch 280, Loss: 0.0748
Batch 290, Loss: 0.0611
Batch 300, Loss: 0.0673
Batch 310, Loss: 0.0674
Batch 320, Loss: 0.0619
Batch 330, Loss: 0.0599
Batch 340, Loss: 0.0696
Batch 350, Loss: 0.0620
Batch 360, Loss: 0.0760
Batch 370, Loss: 0.0646
Batch 380, Loss: 0.0555
Batch 390, Loss: 0.0627
Epoch 42 learning rate: 0.0061846659978068205
Epoch 42 time: 99.55236506462097 seconds
Epoch 42 accuracy: 94.6%
Batch 10, Loss: 0.0581
Batch 20, Loss: 0.0571
Batch 30, Loss: 0.0636
Batch 40, Loss: 0.0549
Batch 50, Loss: 0.0471
Batch 60, Loss: 0.0587
Batch 70, Loss: 0.0543
Batch 80, Loss: 0.0584
Batch 90, Loss: 0.0597
Batch 100, Loss: 0.0639
Batch 110, Loss: 0.0584
Batch 120, Loss: 0.0623
Batch 130, Loss: 0.0526
Batch 140, Loss: 0.0569
Batch 150, Loss: 0.0654
Batch 160, Loss: 0.0573
Batch 170, Loss: 0.0762
Batch 180, Loss: 0.0572
Batch 190, Loss: 0.0564
Batch 200, Loss: 0.0614
Batch 210, Loss: 0.0569
Batch 220, Loss: 0.0605
Batch 230, Loss: 0.0610
Batch 240, Loss: 0.0621
Batch 250, Loss: 0.0555
Batch 260, Loss: 0.0666
Batch 270, Loss: 0.0618
Batch 280, Loss: 0.0531
Batch 290, Loss: 0.0635
Batch 300, Loss: 0.0662
Batch 310, Loss: 0.0646
Batch 320, Loss: 0.0563
Batch 330, Loss: 0.0625
Batch 340, Loss: 0.0611
Batch 350, Loss: 0.0610
Batch 360, Loss: 0.0657
Batch 370, Loss: 0.0649
Batch 380, Loss: 0.0552
Batch 390, Loss: 0.0546
Epoch 43 learning rate: 0.004758647376699033
Epoch 43 time: 99.6289472579956 seconds
Epoch 43 accuracy: 94.7%
Batch 10, Loss: 0.0545
Batch 20, Loss: 0.0553
Batch 30, Loss: 0.0674
Batch 40, Loss: 0.0541
Batch 50, Loss: 0.0578
Batch 60, Loss: 0.0565
Batch 70, Loss: 0.0600
Batch 80, Loss: 0.0568
Batch 90, Loss: 0.0605
Batch 100, Loss: 0.0665
Batch 110, Loss: 0.0559
Batch 120, Loss: 0.0528
Batch 130, Loss: 0.0517
Batch 140, Loss: 0.0581
Batch 150, Loss: 0.0604
Batch 160, Loss: 0.0507
Batch 170, Loss: 0.0557
Batch 180, Loss: 0.0532
Batch 190, Loss: 0.0568
Batch 200, Loss: 0.0589
Batch 210, Loss: 0.0547
Batch 220, Loss: 0.0492
Batch 230, Loss: 0.0557
Batch 240, Loss: 0.0508
Batch 250, Loss: 0.0534
Batch 260, Loss: 0.0568
Batch 270, Loss: 0.0558
Batch 280, Loss: 0.0587
Batch 290, Loss: 0.0583
Batch 300, Loss: 0.0602
Batch 310, Loss: 0.0576
Batch 320, Loss: 0.0584
Batch 330, Loss: 0.0559
Batch 340, Loss: 0.0542
Batch 350, Loss: 0.0597
Batch 360, Loss: 0.0527
Batch 370, Loss: 0.0599
Batch 380, Loss: 0.0607
Batch 390, Loss: 0.0539
Epoch 44 learning rate: 0.0035111757055874327
Epoch 44 time: 99.60880637168884 seconds
Epoch 44 accuracy: 94.73%
Batch 10, Loss: 0.0476
Batch 20, Loss: 0.0521
Batch 30, Loss: 0.0496
Batch 40, Loss: 0.0530
Batch 50, Loss: 0.0589
Batch 60, Loss: 0.0571
Batch 70, Loss: 0.0636
Batch 80, Loss: 0.0500
Batch 90, Loss: 0.0519
Batch 100, Loss: 0.0517
Batch 110, Loss: 0.0520
Batch 120, Loss: 0.0514
Batch 130, Loss: 0.0538
Batch 140, Loss: 0.0574
Batch 150, Loss: 0.0575
Batch 160, Loss: 0.0505
Batch 170, Loss: 0.0454
Batch 180, Loss: 0.0617
Batch 190, Loss: 0.0449
Batch 200, Loss: 0.0563
Batch 210, Loss: 0.0568
Batch 220, Loss: 0.0609
Batch 230, Loss: 0.0620
Batch 240, Loss: 0.0511
Batch 250, Loss: 0.0549
Batch 260, Loss: 0.0576
Batch 270, Loss: 0.0519
Batch 280, Loss: 0.0638
Batch 290, Loss: 0.0591
Batch 300, Loss: 0.0521
Batch 310, Loss: 0.0469
Batch 320, Loss: 0.0567
Batch 330, Loss: 0.0540
Batch 340, Loss: 0.0533
Batch 350, Loss: 0.0525
Batch 360, Loss: 0.0504
Batch 370, Loss: 0.0534
Batch 380, Loss: 0.0631
Batch 390, Loss: 0.0463
Epoch 45 learning rate: 0.0024471741852423235
Epoch 45 time: 99.50483846664429 seconds
Epoch 45 accuracy: 94.71%
Batch 10, Loss: 0.0520
Batch 20, Loss: 0.0516
Batch 30, Loss: 0.0498
Batch 40, Loss: 0.0571
Batch 50, Loss: 0.0521
Batch 60, Loss: 0.0531
Batch 70, Loss: 0.0540
Batch 80, Loss: 0.0486
Batch 90, Loss: 0.0576
Batch 100, Loss: 0.0547
Batch 110, Loss: 0.0580
Batch 120, Loss: 0.0448
Batch 130, Loss: 0.0475
Batch 140, Loss: 0.0583
Batch 150, Loss: 0.0512
Batch 160, Loss: 0.0540
Batch 170, Loss: 0.0602
Batch 180, Loss: 0.0524
Batch 190, Loss: 0.0588
Batch 200, Loss: 0.0567
Batch 210, Loss: 0.0535
Batch 220, Loss: 0.0501
Batch 230, Loss: 0.0512
Batch 240, Loss: 0.0503
Batch 250, Loss: 0.0509
Batch 260, Loss: 0.0552
Batch 270, Loss: 0.0488
Batch 280, Loss: 0.0493
Batch 290, Loss: 0.0536
Batch 300, Loss: 0.0516
Batch 310, Loss: 0.0603
Batch 320, Loss: 0.0499
Batch 330, Loss: 0.0536
Batch 340, Loss: 0.0520
Batch 350, Loss: 0.0444
Batch 360, Loss: 0.0495
Batch 370, Loss: 0.0512
Batch 380, Loss: 0.0492
Batch 390, Loss: 0.0527
Epoch 46 learning rate: 0.0015708419435684518
Epoch 46 time: 99.4544517993927 seconds
Epoch 46 accuracy: 94.75%
Batch 10, Loss: 0.0528
Batch 20, Loss: 0.0447
Batch 30, Loss: 0.0497
Batch 40, Loss: 0.0548
Batch 50, Loss: 0.0487
Batch 60, Loss: 0.0564
Batch 70, Loss: 0.0469
Batch 80, Loss: 0.0483
Batch 90, Loss: 0.0495
Batch 100, Loss: 0.0512
Batch 110, Loss: 0.0522
Batch 120, Loss: 0.0476
Batch 130, Loss: 0.0528
Batch 140, Loss: 0.0501
Batch 150, Loss: 0.0468
Batch 160, Loss: 0.0520
Batch 170, Loss: 0.0529
Batch 180, Loss: 0.0428
Batch 190, Loss: 0.0478
Batch 200, Loss: 0.0444
Batch 210, Loss: 0.0505
Batch 220, Loss: 0.0543
Batch 230, Loss: 0.0530
Batch 240, Loss: 0.0505
Batch 250, Loss: 0.0591
Batch 260, Loss: 0.0514
Batch 270, Loss: 0.0494
Batch 280, Loss: 0.0470
Batch 290, Loss: 0.0491
Batch 300, Loss: 0.0605
Batch 310, Loss: 0.0521
Batch 320, Loss: 0.0493
Batch 330, Loss: 0.0474
Batch 340, Loss: 0.0588
Batch 350, Loss: 0.0598
Batch 360, Loss: 0.0512
Batch 370, Loss: 0.0484
Batch 380, Loss: 0.0478
Batch 390, Loss: 0.0593
Epoch 47 learning rate: 0.000885637463565564
Epoch 47 time: 99.64108347892761 seconds
Epoch 47 accuracy: 94.89%
Batch 10, Loss: 0.0577
Batch 20, Loss: 0.0524
Batch 30, Loss: 0.0485
Batch 40, Loss: 0.0488
Batch 50, Loss: 0.0498
Batch 60, Loss: 0.0503
Batch 70, Loss: 0.0544
Batch 80, Loss: 0.0485
Batch 90, Loss: 0.0486
Batch 100, Loss: 0.0477
Batch 110, Loss: 0.0516
Batch 120, Loss: 0.0518
Batch 130, Loss: 0.0505
Batch 140, Loss: 0.0537
Batch 150, Loss: 0.0555
Batch 160, Loss: 0.0497
Batch 170, Loss: 0.0531
Batch 180, Loss: 0.0475
Batch 190, Loss: 0.0548
Batch 200, Loss: 0.0478
Batch 210, Loss: 0.0541
Batch 220, Loss: 0.0449
Batch 230, Loss: 0.0512
Batch 240, Loss: 0.0520
Batch 250, Loss: 0.0516
Batch 260, Loss: 0.0495
Batch 270, Loss: 0.0438
Batch 280, Loss: 0.0505
Batch 290, Loss: 0.0524
Batch 300, Loss: 0.0503
Batch 310, Loss: 0.0496
Batch 320, Loss: 0.0529
Batch 330, Loss: 0.0464
Batch 340, Loss: 0.0469
Batch 350, Loss: 0.0474
Batch 360, Loss: 0.0522
Batch 370, Loss: 0.0499
Batch 380, Loss: 0.0505
Batch 390, Loss: 0.0444
Epoch 48 learning rate: 0.00039426493427611173
Epoch 48 time: 99.46771550178528 seconds
Epoch 48 accuracy: 94.77%
Batch 10, Loss: 0.0521
Batch 20, Loss: 0.0508
Batch 30, Loss: 0.0510
Batch 40, Loss: 0.0531
Batch 50, Loss: 0.0524
Batch 60, Loss: 0.0461
Batch 70, Loss: 0.0490
Batch 80, Loss: 0.0510
Batch 90, Loss: 0.0510
Batch 100, Loss: 0.0440
Batch 110, Loss: 0.0505
Batch 120, Loss: 0.0508
Batch 130, Loss: 0.0481
Batch 140, Loss: 0.0472
Batch 150, Loss: 0.0522
Batch 160, Loss: 0.0499
Batch 170, Loss: 0.0523
Batch 180, Loss: 0.0410
Batch 190, Loss: 0.0475
Batch 200, Loss: 0.0488
Batch 210, Loss: 0.0467
Batch 220, Loss: 0.0462
Batch 230, Loss: 0.0496
Batch 240, Loss: 0.0490
Batch 250, Loss: 0.0457
Batch 260, Loss: 0.0491
Batch 270, Loss: 0.0507
Batch 280, Loss: 0.0490
Batch 290, Loss: 0.0486
Batch 300, Loss: 0.0531
Batch 310, Loss: 0.0523
Batch 320, Loss: 0.0541
Batch 330, Loss: 0.0475
Batch 340, Loss: 0.0505
Batch 350, Loss: 0.0491
Batch 360, Loss: 0.0487
Batch 370, Loss: 0.0508
Batch 380, Loss: 0.0443
Batch 390, Loss: 0.0474
Epoch 49 learning rate: 9.866357858642205e-05
Epoch 49 time: 99.60714650154114 seconds
Epoch 49 accuracy: 94.81%
Batch 10, Loss: 0.0462
Batch 20, Loss: 0.0480
Batch 30, Loss: 0.0521
Batch 40, Loss: 0.0513
Batch 50, Loss: 0.0437
Batch 60, Loss: 0.0499
Batch 70, Loss: 0.0471
Batch 80, Loss: 0.0578
Batch 90, Loss: 0.0430
Batch 100, Loss: 0.0575
Batch 110, Loss: 0.0542
Batch 120, Loss: 0.0485
Batch 130, Loss: 0.0550
Batch 140, Loss: 0.0550
Batch 150, Loss: 0.0451
Batch 160, Loss: 0.0547
Batch 170, Loss: 0.0424
Batch 180, Loss: 0.0467
Batch 190, Loss: 0.0426
Batch 200, Loss: 0.0527
Batch 210, Loss: 0.0451
Batch 220, Loss: 0.0523
Batch 230, Loss: 0.0450
Batch 240, Loss: 0.0464
Batch 250, Loss: 0.0520
Batch 260, Loss: 0.0526
Batch 270, Loss: 0.0575
Batch 280, Loss: 0.0487
Batch 290, Loss: 0.0547
Batch 300, Loss: 0.0415
Batch 310, Loss: 0.0456
Batch 320, Loss: 0.0450
Batch 330, Loss: 0.0486
Batch 340, Loss: 0.0448
Batch 350, Loss: 0.0465
Batch 360, Loss: 0.0511
Batch 370, Loss: 0.0491
Batch 380, Loss: 0.0448
Batch 390, Loss: 0.0485
Epoch 50 learning rate: 0.0
Epoch 50 time: 99.62390089035034 seconds
Epoch 50 accuracy: 94.77%
rho:  0.04 , alpha:  0.3
Total training time: 4982.1366057395935 seconds
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
The top Hessian eigenvalue of this model is 34.1805
Norm of the Gradient: 9.1558462381e-01
