The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:216: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GAMNonAccelerated/basicaug/lr-0.1/batchsize-128/2024-08-04-19:31:39
Using non-accelerated GAM
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 5.3241
Batch 20, Loss: 3.6690
Batch 30, Loss: 1.9325
Batch 40, Loss: 1.7970
Batch 50, Loss: 1.7437
Batch 60, Loss: 1.7241
Batch 70, Loss: 1.6265
Batch 80, Loss: 1.6027
Batch 90, Loss: 1.5677
Batch 100, Loss: 1.5717
Batch 110, Loss: 1.5600
Batch 120, Loss: 1.5453
Batch 130, Loss: 1.5473
Batch 140, Loss: 1.5078
Batch 150, Loss: 1.5442
Batch 160, Loss: 1.4949
Batch 170, Loss: 1.5208
Batch 180, Loss: 1.4788
Batch 190, Loss: 1.4772
Batch 200, Loss: 1.4766
Batch 210, Loss: 1.4637
Batch 220, Loss: 1.4477
Batch 230, Loss: 1.4522
Batch 240, Loss: 1.4329
Batch 250, Loss: 1.4263
Batch 260, Loss: 1.4298
Batch 270, Loss: 1.4092
Batch 280, Loss: 1.4240
Batch 290, Loss: 1.4048
Batch 300, Loss: 1.3779
Batch 310, Loss: 1.3882
Batch 320, Loss: 1.3960
Batch 330, Loss: 1.3574
Batch 340, Loss: 1.3512
Batch 350, Loss: 1.3432
Batch 360, Loss: 1.3336
Batch 370, Loss: 1.3042
Batch 380, Loss: 1.3382
Batch 390, Loss: 1.3215
Epoch 1 learning rate: 0.09990133642141358
Epoch 1 time: 245.76387858390808 seconds
Epoch 1 accuracy: 37.85%
Batch 10, Loss: 1.2907
Batch 20, Loss: 1.2941
Batch 30, Loss: 1.2951
Batch 40, Loss: 1.2644
Batch 50, Loss: 1.2902
Batch 60, Loss: 1.2845
Batch 70, Loss: 1.2919
Batch 80, Loss: 1.2381
Batch 90, Loss: 1.2406
Batch 100, Loss: 1.2461
Batch 110, Loss: 1.2678
Batch 120, Loss: 1.2464
Batch 130, Loss: 1.2149
Batch 140, Loss: 1.2248
Batch 150, Loss: 1.2227
Batch 160, Loss: 1.2091
Batch 170, Loss: 1.2034
Batch 180, Loss: 1.2675
Batch 190, Loss: 1.2208
Batch 200, Loss: 1.1732
Batch 210, Loss: 1.2341
Batch 220, Loss: 1.1789
Batch 230, Loss: 1.2280
Batch 240, Loss: 1.1760
Batch 250, Loss: 1.1438
Batch 260, Loss: 1.1923
Batch 270, Loss: 1.1539
Batch 280, Loss: 1.1385
Batch 290, Loss: 1.1778
Batch 300, Loss: 1.1541
Batch 310, Loss: 1.1553
Batch 320, Loss: 1.1195
Batch 330, Loss: 1.1357
Batch 340, Loss: 1.1606
Batch 350, Loss: 1.1210
Batch 360, Loss: 1.2126
Batch 370, Loss: 1.1180
Batch 380, Loss: 1.1351
Batch 390, Loss: 1.0899
Epoch 2 learning rate: 0.0996057350657239
Epoch 2 time: 236.65310096740723 seconds
Epoch 2 accuracy: 47.47%
Batch 10, Loss: 1.1188
Batch 20, Loss: 1.0716
Batch 30, Loss: 1.0731
Batch 40, Loss: 1.1136
Batch 50, Loss: 1.1090
Batch 60, Loss: 1.1001
Batch 70, Loss: 1.0641
Batch 80, Loss: 1.0815
Batch 90, Loss: 1.0611
Batch 100, Loss: 1.0579
Batch 110, Loss: 1.0381
Batch 120, Loss: 1.0128
Batch 130, Loss: 1.0475
Batch 140, Loss: 1.0215
Batch 150, Loss: 1.0440
Batch 160, Loss: 1.0442
Batch 170, Loss: 1.0163
Batch 180, Loss: 0.9718
Batch 190, Loss: 1.0806
Batch 200, Loss: 1.0392
Batch 210, Loss: 1.0380
Batch 220, Loss: 0.9735
Batch 230, Loss: 1.0019
Batch 240, Loss: 1.0235
Batch 250, Loss: 0.9954
Batch 260, Loss: 1.0482
Batch 270, Loss: 0.9937
Batch 280, Loss: 1.0146
Batch 290, Loss: 1.0083
Batch 300, Loss: 1.0079
Batch 310, Loss: 0.9811
Batch 320, Loss: 0.9847
Batch 330, Loss: 0.9707
Batch 340, Loss: 0.9460
Batch 350, Loss: 0.9302
Batch 360, Loss: 0.9556
Batch 370, Loss: 0.9119
Batch 380, Loss: 0.9182
Batch 390, Loss: 0.9215
Epoch 3 learning rate: 0.09911436253643444
Epoch 3 time: 236.39090085029602 seconds
Epoch 3 accuracy: 55.25%
Batch 10, Loss: 0.9206
Batch 20, Loss: 0.9239
Batch 30, Loss: 0.8919
Batch 40, Loss: 0.9114
Batch 50, Loss: 0.9172
Batch 60, Loss: 0.9156
Batch 70, Loss: 0.9090
Batch 80, Loss: 0.8800
Batch 90, Loss: 0.8599
Batch 100, Loss: 0.8946
Batch 110, Loss: 0.8928
Batch 120, Loss: 0.8659
Batch 130, Loss: 0.8677
Batch 140, Loss: 0.8434
Batch 150, Loss: 0.8911
Batch 160, Loss: 0.8251
Batch 170, Loss: 0.8115
Batch 180, Loss: 0.8461
Batch 190, Loss: 0.8390
Batch 200, Loss: 0.8290
Batch 210, Loss: 0.8329
Batch 220, Loss: 0.8831
Batch 230, Loss: 0.8878
Batch 240, Loss: 0.8200
Batch 250, Loss: 0.8606
Batch 260, Loss: 0.8463
Batch 270, Loss: 0.8509
Batch 280, Loss: 0.8623
Batch 290, Loss: 0.8281
Batch 300, Loss: 0.8227
Batch 310, Loss: 0.7990
Batch 320, Loss: 0.7981
Batch 330, Loss: 0.8335
Batch 340, Loss: 0.8261
Batch 350, Loss: 0.7822
Batch 360, Loss: 0.8237
Batch 370, Loss: 0.7829
Batch 380, Loss: 0.8050
Batch 390, Loss: 0.7880
Epoch 4 learning rate: 0.09842915805643154
Epoch 4 time: 236.44417548179626 seconds
Epoch 4 accuracy: 62.43%
Batch 10, Loss: 0.7851
Batch 20, Loss: 0.7516
Batch 30, Loss: 0.8048
Batch 40, Loss: 0.8024
Batch 50, Loss: 0.7918
Batch 60, Loss: 0.7825
Batch 70, Loss: 0.8084
Batch 80, Loss: 0.7127
Batch 90, Loss: 0.7656
Batch 100, Loss: 0.7796
Batch 110, Loss: 0.7642
Batch 120, Loss: 0.7568
Batch 130, Loss: 0.7279
Batch 140, Loss: 0.7391
Batch 150, Loss: 0.7445
Batch 160, Loss: 0.7710
Batch 170, Loss: 0.7041
Batch 180, Loss: 0.7159
Batch 190, Loss: 0.7501
Batch 200, Loss: 0.7617
Batch 210, Loss: 0.7472
Batch 220, Loss: 0.7181
Batch 230, Loss: 0.7445
Batch 240, Loss: 0.7332
Batch 250, Loss: 0.7358
Batch 260, Loss: 0.7523
Batch 270, Loss: 0.7570
Batch 280, Loss: 0.7326
Batch 290, Loss: 0.7088
Batch 300, Loss: 0.7141
Batch 310, Loss: 0.7047
Batch 320, Loss: 0.7178
Batch 330, Loss: 0.7012
Batch 340, Loss: 0.7335
Batch 350, Loss: 0.7419
Batch 360, Loss: 0.7218
Batch 370, Loss: 0.7013
Batch 380, Loss: 0.7164
Batch 390, Loss: 0.6892
Epoch 5 learning rate: 0.09755282581475767
Epoch 5 time: 236.26254868507385 seconds
Epoch 5 accuracy: 66.33%
Batch 10, Loss: 0.6969
Batch 20, Loss: 0.7117
Batch 30, Loss: 0.6736
Batch 40, Loss: 0.6345
Batch 50, Loss: 0.6516
Batch 60, Loss: 0.6730
Batch 70, Loss: 0.6826
Batch 80, Loss: 0.7029
Batch 90, Loss: 0.6899
Batch 100, Loss: 0.6926
Batch 110, Loss: 0.6733
Batch 120, Loss: 0.6885
Batch 130, Loss: 0.6740
Batch 140, Loss: 0.6728
Batch 150, Loss: 0.6857
Batch 160, Loss: 0.6362
Batch 170, Loss: 0.7009
Batch 180, Loss: 0.6507
Batch 190, Loss: 0.6600
Batch 200, Loss: 0.7127
Batch 210, Loss: 0.6720
Batch 220, Loss: 0.6693
Batch 230, Loss: 0.6502
Batch 240, Loss: 0.6673
Batch 250, Loss: 0.6250
Batch 260, Loss: 0.6512
Batch 270, Loss: 0.6461
Batch 280, Loss: 0.6682
Batch 290, Loss: 0.6242
Batch 300, Loss: 0.6392
Batch 310, Loss: 0.6298
Batch 320, Loss: 0.6435
Batch 330, Loss: 0.6907
Batch 340, Loss: 0.6506
Batch 350, Loss: 0.6414
Batch 360, Loss: 0.6157
Batch 370, Loss: 0.6226
Batch 380, Loss: 0.6099
Batch 390, Loss: 0.6699
Epoch 6 learning rate: 0.09648882429441256
Epoch 6 time: 236.18958926200867 seconds
Epoch 6 accuracy: 68.72%
Batch 10, Loss: 0.6475
Batch 20, Loss: 0.5768
Batch 30, Loss: 0.5681
Batch 40, Loss: 0.6212
Batch 50, Loss: 0.6012
Batch 60, Loss: 0.5815
Batch 70, Loss: 0.6023
Batch 80, Loss: 0.5951
Batch 90, Loss: 0.5786
Batch 100, Loss: 0.6275
Batch 110, Loss: 0.5893
Batch 120, Loss: 0.5794
Batch 130, Loss: 0.5920
Batch 140, Loss: 0.5709
Batch 150, Loss: 0.5921
Batch 160, Loss: 0.5716
Batch 170, Loss: 0.5729
Batch 180, Loss: 0.6261
Batch 190, Loss: 0.5889
Batch 200, Loss: 0.5881
Batch 210, Loss: 0.5669
Batch 220, Loss: 0.5369
Batch 230, Loss: 0.5885
Batch 240, Loss: 0.5365
Batch 250, Loss: 0.5398
Batch 260, Loss: 0.5415
Batch 270, Loss: 0.5472
Batch 280, Loss: 0.5307
Batch 290, Loss: 0.5730
Batch 300, Loss: 0.5531
Batch 310, Loss: 0.5405
Batch 320, Loss: 0.5643
Batch 330, Loss: 0.5522
Batch 340, Loss: 0.5376
Batch 350, Loss: 0.5641
Batch 360, Loss: 0.5229
Batch 370, Loss: 0.5368
Batch 380, Loss: 0.5203
Batch 390, Loss: 0.5132
Epoch 7 learning rate: 0.09524135262330098
Epoch 7 time: 236.26343822479248 seconds
Epoch 7 accuracy: 74.41%
Batch 10, Loss: 0.5132
Batch 20, Loss: 0.5141
Batch 30, Loss: 0.5085
Batch 40, Loss: 0.5387
Batch 50, Loss: 0.4962
Batch 60, Loss: 0.5160
Batch 70, Loss: 0.4823
Batch 80, Loss: 0.5196
Batch 90, Loss: 0.4920
Batch 100, Loss: 0.5115
Batch 110, Loss: 0.4944
Batch 120, Loss: 0.4980
Batch 130, Loss: 0.5227
Batch 140, Loss: 0.5091
Batch 150, Loss: 0.4998
Batch 160, Loss: 0.4765
Batch 170, Loss: 0.5270
Batch 180, Loss: 0.5191
Batch 190, Loss: 0.5115
Batch 200, Loss: 0.4538
Batch 210, Loss: 0.5050
Batch 220, Loss: 0.4657
Batch 230, Loss: 0.4942
Batch 240, Loss: 0.4635
Batch 250, Loss: 0.4874
Batch 260, Loss: 0.5091
Batch 270, Loss: 0.4777
Batch 280, Loss: 0.5014
Batch 290, Loss: 0.4624
Batch 300, Loss: 0.4576
Batch 310, Loss: 0.4891
Batch 320, Loss: 0.4721
Batch 330, Loss: 0.4820
Batch 340, Loss: 0.4915
Batch 350, Loss: 0.4992
Batch 360, Loss: 0.4822
Batch 370, Loss: 0.4558
Batch 380, Loss: 0.4818
Batch 390, Loss: 0.5018
Epoch 8 learning rate: 0.09381533400219318
Epoch 8 time: 236.28166961669922 seconds
Epoch 8 accuracy: 76.72%
Batch 10, Loss: 0.4872
Batch 20, Loss: 0.4601
Batch 30, Loss: 0.4484
Batch 40, Loss: 0.4567
Batch 50, Loss: 0.4302
Batch 60, Loss: 0.4347
Batch 70, Loss: 0.4811
Batch 80, Loss: 0.4490
Batch 90, Loss: 0.4787
Batch 100, Loss: 0.5007
Batch 110, Loss: 0.4284
Batch 120, Loss: 0.4303
Batch 130, Loss: 0.4380
Batch 140, Loss: 0.4981
Batch 150, Loss: 0.4474
Batch 160, Loss: 0.4326
Batch 170, Loss: 0.4362
Batch 180, Loss: 0.4368
Batch 190, Loss: 0.4542
Batch 200, Loss: 0.4341
Batch 210, Loss: 0.4288
Batch 220, Loss: 0.4455
Batch 230, Loss: 0.4315
Batch 240, Loss: 0.4697
Batch 250, Loss: 0.4594
Batch 260, Loss: 0.4508
Batch 270, Loss: 0.4578
Batch 280, Loss: 0.4873
Batch 290, Loss: 0.4550
Batch 300, Loss: 0.3939
Batch 310, Loss: 0.4343
Batch 320, Loss: 0.4548
Batch 330, Loss: 0.4352
Batch 340, Loss: 0.4413
Batch 350, Loss: 0.4339
Batch 360, Loss: 0.4431
Batch 370, Loss: 0.4406
Batch 380, Loss: 0.4538
Batch 390, Loss: 0.4141
Epoch 9 learning rate: 0.09221639627510075
Epoch 9 time: 236.28685426712036 seconds
Epoch 9 accuracy: 76.94%
Batch 10, Loss: 0.4125
Batch 20, Loss: 0.4664
Batch 30, Loss: 0.4175
Batch 40, Loss: 0.4252
Batch 50, Loss: 0.4036
Batch 60, Loss: 0.4186
Batch 70, Loss: 0.4295
Batch 80, Loss: 0.4061
Batch 90, Loss: 0.4184
Batch 100, Loss: 0.4254
Batch 110, Loss: 0.3745
Batch 120, Loss: 0.4091
Batch 130, Loss: 0.4427
Batch 140, Loss: 0.3674
Batch 150, Loss: 0.4497
Batch 160, Loss: 0.4117
Batch 170, Loss: 0.4277
Batch 180, Loss: 0.3752
Batch 190, Loss: 0.4167
Batch 200, Loss: 0.3872
Batch 210, Loss: 0.4275
Batch 220, Loss: 0.4045
Batch 230, Loss: 0.4225
Batch 240, Loss: 0.4320
Batch 250, Loss: 0.4371
Batch 260, Loss: 0.4027
Batch 270, Loss: 0.4351
Batch 280, Loss: 0.4502
Batch 290, Loss: 0.3672
Batch 300, Loss: 0.4181
Batch 310, Loss: 0.4565
Batch 320, Loss: 0.3911
Batch 330, Loss: 0.3738
Batch 340, Loss: 0.3971
Batch 350, Loss: 0.4120
Batch 360, Loss: 0.4115
Batch 370, Loss: 0.4231
Batch 380, Loss: 0.4254
Batch 390, Loss: 0.3680
Epoch 10 learning rate: 0.09045084971874737
Epoch 10 time: 236.41054511070251 seconds
Epoch 10 accuracy: 79.9%
Batch 10, Loss: 0.3706
Batch 20, Loss: 0.3753
Batch 30, Loss: 0.4244
Batch 40, Loss: 0.3517
Batch 50, Loss: 0.4105
Batch 60, Loss: 0.3903
Batch 70, Loss: 0.4113
Batch 80, Loss: 0.3788
Batch 90, Loss: 0.4160
Batch 100, Loss: 0.3991
Batch 110, Loss: 0.3984
Batch 120, Loss: 0.4285
Batch 130, Loss: 0.4244
Batch 140, Loss: 0.3952
Batch 150, Loss: 0.4405
Batch 160, Loss: 0.3967
Batch 170, Loss: 0.4002
Batch 180, Loss: 0.4017
Batch 190, Loss: 0.3687
Batch 200, Loss: 0.4087
Batch 210, Loss: 0.3748
Batch 220, Loss: 0.3898
Batch 230, Loss: 0.3811
Batch 240, Loss: 0.3611
Batch 250, Loss: 0.3986
Batch 260, Loss: 0.3781
Batch 270, Loss: 0.4227
Batch 280, Loss: 0.4424
Batch 290, Loss: 0.4035
Batch 300, Loss: 0.3904
Batch 310, Loss: 0.4002
Batch 320, Loss: 0.3704
Batch 330, Loss: 0.4278
Batch 340, Loss: 0.3957
Batch 350, Loss: 0.4072
Batch 360, Loss: 0.4083
Batch 370, Loss: 0.3956
Batch 380, Loss: 0.3525
Batch 390, Loss: 0.3866
Epoch 11 learning rate: 0.08852566213878946
Epoch 11 time: 236.2318561077118 seconds
Epoch 11 accuracy: 81.65%
Batch 10, Loss: 0.3838
Batch 20, Loss: 0.3651
Batch 30, Loss: 0.3641
Batch 40, Loss: 0.3371
Batch 50, Loss: 0.4062
Batch 60, Loss: 0.3508
Batch 70, Loss: 0.3611
Batch 80, Loss: 0.4138
Batch 90, Loss: 0.3833
Batch 100, Loss: 0.3799
Batch 110, Loss: 0.3746
Batch 120, Loss: 0.3605
Batch 130, Loss: 0.3581
Batch 140, Loss: 0.3827
Batch 150, Loss: 0.3796
Batch 160, Loss: 0.3897
Batch 170, Loss: 0.3904
Batch 180, Loss: 0.3827
Batch 190, Loss: 0.3597
Batch 200, Loss: 0.3627
Batch 210, Loss: 0.3527
Batch 220, Loss: 0.3353
Batch 230, Loss: 0.3401
Batch 240, Loss: 0.3361
Batch 250, Loss: 0.3715
Batch 260, Loss: 0.3498
Batch 270, Loss: 0.3775
Batch 280, Loss: 0.3656
Batch 290, Loss: 0.3692
Batch 300, Loss: 0.3455
Batch 310, Loss: 0.3620
Batch 320, Loss: 0.3672
Batch 330, Loss: 0.3508
Batch 340, Loss: 0.3841
Batch 350, Loss: 0.3682
Batch 360, Loss: 0.3534
Batch 370, Loss: 0.3574
Batch 380, Loss: 0.3378
Batch 390, Loss: 0.3636
Epoch 12 learning rate: 0.08644843137107057
Epoch 12 time: 236.25716948509216 seconds
Epoch 12 accuracy: 81.05%
Batch 10, Loss: 0.3880
Batch 20, Loss: 0.3680
Batch 30, Loss: 0.3573
Batch 40, Loss: 0.3753
Batch 50, Loss: 0.3369
Batch 60, Loss: 0.3619
Batch 70, Loss: 0.3495
Batch 80, Loss: 0.3415
Batch 90, Loss: 0.3386
Batch 100, Loss: 0.3374
Batch 110, Loss: 0.3398
Batch 120, Loss: 0.3229
Batch 130, Loss: 0.3287
Batch 140, Loss: 0.3589
Batch 150, Loss: 0.4031
Batch 160, Loss: 0.3576
Batch 170, Loss: 0.3831
Batch 180, Loss: 0.3523
Batch 190, Loss: 0.3467
Batch 200, Loss: 0.3606
Batch 210, Loss: 0.3441
Batch 220, Loss: 0.3157
Batch 230, Loss: 0.3582
Batch 240, Loss: 0.3577
Batch 250, Loss: 0.3812
Batch 260, Loss: 0.3587
Batch 270, Loss: 0.3328
Batch 280, Loss: 0.3508
Batch 290, Loss: 0.3594
Batch 300, Loss: 0.3730
Batch 310, Loss: 0.3675
Batch 320, Loss: 0.3435
Batch 330, Loss: 0.3586
Batch 340, Loss: 0.3274
Batch 350, Loss: 0.3507
Batch 360, Loss: 0.3477
Batch 370, Loss: 0.3642
Batch 380, Loss: 0.3756
Batch 390, Loss: 0.3708
Epoch 13 learning rate: 0.08422735529643442
Epoch 13 time: 236.2192485332489 seconds
Epoch 13 accuracy: 81.84%
Batch 10, Loss: 0.3594
Batch 20, Loss: 0.3465
Batch 30, Loss: 0.3482
Batch 40, Loss: 0.3524
Batch 50, Loss: 0.3073
Batch 60, Loss: 0.3207
Batch 70, Loss: 0.3195
Batch 80, Loss: 0.3419
Batch 90, Loss: 0.3419
Batch 100, Loss: 0.3657
Batch 110, Loss: 0.3223
Batch 120, Loss: 0.3378
Batch 130, Loss: 0.3374
Batch 140, Loss: 0.3375
Batch 150, Loss: 0.3133
Batch 160, Loss: 0.3362
Batch 170, Loss: 0.3390
Batch 180, Loss: 0.3332
Batch 190, Loss: 0.3258
Batch 200, Loss: 0.3428
Batch 210, Loss: 0.3408
Batch 220, Loss: 0.3463
Batch 230, Loss: 0.3264
Batch 240, Loss: 0.3353
Batch 250, Loss: 0.3232
Batch 260, Loss: 0.3218
Batch 270, Loss: 0.3468
Batch 280, Loss: 0.3837
Batch 290, Loss: 0.3008
Batch 300, Loss: 0.3261
Batch 310, Loss: 0.3239
Batch 320, Loss: 0.3429
Batch 330, Loss: 0.3643
Batch 340, Loss: 0.3280
Batch 350, Loss: 0.3681
Batch 360, Loss: 0.3851
Batch 370, Loss: 0.3570
Batch 380, Loss: 0.3380
Batch 390, Loss: 0.3406
Epoch 14 learning rate: 0.08187119948743447
Epoch 14 time: 236.2243103981018 seconds
Epoch 14 accuracy: 81.11%
Batch 10, Loss: 0.3532
Batch 20, Loss: 0.3021
Batch 30, Loss: 0.3066
Batch 40, Loss: 0.3258
Batch 50, Loss: 0.3571
Batch 60, Loss: 0.3163
Batch 70, Loss: 0.3058
Batch 80, Loss: 0.3475
Batch 90, Loss: 0.3149
Batch 100, Loss: 0.3800
Batch 110, Loss: 0.2776
Batch 120, Loss: 0.3368
Batch 130, Loss: 0.3031
Batch 140, Loss: 0.3467
Batch 150, Loss: 0.3471
Batch 160, Loss: 0.3137
Batch 170, Loss: 0.3393
Batch 180, Loss: 0.3376
Batch 190, Loss: 0.3225
Batch 200, Loss: 0.3326
Batch 210, Loss: 0.3339
Batch 220, Loss: 0.3304
Batch 230, Loss: 0.3009
Batch 240, Loss: 0.3045
Batch 250, Loss: 0.3174
Batch 260, Loss: 0.3287
Batch 270, Loss: 0.3413
Batch 280, Loss: 0.3143
Batch 290, Loss: 0.3412
Batch 300, Loss: 0.3122
Batch 310, Loss: 0.3131
Batch 320, Loss: 0.3258
Batch 330, Loss: 0.3299
Batch 340, Loss: 0.3223
Batch 350, Loss: 0.3200
Batch 360, Loss: 0.3190
Batch 370, Loss: 0.3134
Batch 380, Loss: 0.3101
Batch 390, Loss: 0.3283
Epoch 15 learning rate: 0.07938926261462366
Epoch 15 time: 236.32311415672302 seconds
Epoch 15 accuracy: 81.39%
Batch 10, Loss: 0.3081
Batch 20, Loss: 0.3047
Batch 30, Loss: 0.3098
Batch 40, Loss: 0.3137
Batch 50, Loss: 0.2938
Batch 60, Loss: 0.2624
Batch 70, Loss: 0.3015
Batch 80, Loss: 0.2860
Batch 90, Loss: 0.3256
Batch 100, Loss: 0.3157
Batch 110, Loss: 0.3270
Batch 120, Loss: 0.2997
Batch 130, Loss: 0.2973
Batch 140, Loss: 0.2866
Batch 150, Loss: 0.3249
Batch 160, Loss: 0.3089
Batch 170, Loss: 0.3003
Batch 180, Loss: 0.2793
Batch 190, Loss: 0.3264
Batch 200, Loss: 0.2739
Batch 210, Loss: 0.3116
Batch 220, Loss: 0.2832
Batch 230, Loss: 0.3209
Batch 240, Loss: 0.3348
Batch 250, Loss: 0.3541
Batch 260, Loss: 0.3194
Batch 270, Loss: 0.3345
Batch 280, Loss: 0.3384
Batch 290, Loss: 0.3350
Batch 300, Loss: 0.2846
Batch 310, Loss: 0.3509
Batch 320, Loss: 0.3245
Batch 330, Loss: 0.3240
Batch 340, Loss: 0.3223
Batch 350, Loss: 0.3276
Batch 360, Loss: 0.3155
Batch 370, Loss: 0.3087
Batch 380, Loss: 0.3362
Batch 390, Loss: 0.3225
Epoch 16 learning rate: 0.07679133974894982
Epoch 16 time: 236.34709858894348 seconds
Epoch 16 accuracy: 85.9%
Batch 10, Loss: 0.2909
Batch 20, Loss: 0.3011
Batch 30, Loss: 0.3053
Batch 40, Loss: 0.2506
Batch 50, Loss: 0.2668
Batch 60, Loss: 0.2956
Batch 70, Loss: 0.3133
Batch 80, Loss: 0.3000
Batch 90, Loss: 0.3088
Batch 100, Loss: 0.2726
Batch 110, Loss: 0.2936
Batch 120, Loss: 0.2960
Batch 130, Loss: 0.3261
Batch 140, Loss: 0.3126
Batch 150, Loss: 0.2683
Batch 160, Loss: 0.2988
Batch 170, Loss: 0.3237
Batch 180, Loss: 0.3241
Batch 190, Loss: 0.3243
Batch 200, Loss: 0.3176
Batch 210, Loss: 0.3093
Batch 220, Loss: 0.3224
Batch 230, Loss: 0.3010
Batch 240, Loss: 0.2845
Batch 250, Loss: 0.2935
Batch 260, Loss: 0.2727
Batch 270, Loss: 0.2785
Batch 280, Loss: 0.2926
Batch 290, Loss: 0.3117
Batch 300, Loss: 0.2821
Batch 310, Loss: 0.3105
Batch 320, Loss: 0.3229
Batch 330, Loss: 0.2988
Batch 340, Loss: 0.2964
Batch 350, Loss: 0.2966
Batch 360, Loss: 0.2807
Batch 370, Loss: 0.3013
Batch 380, Loss: 0.3087
Batch 390, Loss: 0.2939
Epoch 17 learning rate: 0.07408768370508576
Epoch 17 time: 236.1825671195984 seconds
Epoch 17 accuracy: 84.74%
Batch 10, Loss: 0.2610
Batch 20, Loss: 0.2813
Batch 30, Loss: 0.2646
Batch 40, Loss: 0.2820
Batch 50, Loss: 0.2436
Batch 60, Loss: 0.2733
Batch 70, Loss: 0.2861
Batch 80, Loss: 0.2800
Batch 90, Loss: 0.2543
Batch 100, Loss: 0.2612
Batch 110, Loss: 0.3096
Batch 120, Loss: 0.2531
Batch 130, Loss: 0.2917
Batch 140, Loss: 0.2958
Batch 150, Loss: 0.2473
Batch 160, Loss: 0.2987
Batch 170, Loss: 0.3084
Batch 180, Loss: 0.2973
Batch 190, Loss: 0.2850
Batch 200, Loss: 0.3099
Batch 210, Loss: 0.2914
Batch 220, Loss: 0.3027
Batch 230, Loss: 0.3136
Batch 240, Loss: 0.2983
Batch 250, Loss: 0.3218
Batch 260, Loss: 0.2775
Batch 270, Loss: 0.2657
Batch 280, Loss: 0.2614
Batch 290, Loss: 0.2742
Batch 300, Loss: 0.2940
Batch 310, Loss: 0.3043
Batch 320, Loss: 0.2840
Batch 330, Loss: 0.2733
Batch 340, Loss: 0.2845
Batch 350, Loss: 0.2902
Batch 360, Loss: 0.2983
Batch 370, Loss: 0.3023
Batch 380, Loss: 0.3098
Batch 390, Loss: 0.3010
Epoch 18 learning rate: 0.07128896457825362
Epoch 18 time: 236.2477991580963 seconds
Epoch 18 accuracy: 84.67%
Batch 10, Loss: 0.2733
Batch 20, Loss: 0.2473
Batch 30, Loss: 0.2499
Batch 40, Loss: 0.2465
Batch 50, Loss: 0.2537
Batch 60, Loss: 0.2500
Batch 70, Loss: 0.2387
Batch 80, Loss: 0.2485
Batch 90, Loss: 0.2922
Batch 100, Loss: 0.2510
Batch 110, Loss: 0.2563
Batch 120, Loss: 0.2840
Batch 130, Loss: 0.2827
Batch 140, Loss: 0.3053
Batch 150, Loss: 0.2928
Batch 160, Loss: 0.2789
Batch 170, Loss: 0.2417
Batch 180, Loss: 0.2661
Batch 190, Loss: 0.2550
Batch 200, Loss: 0.2752
Batch 210, Loss: 0.2643
Batch 220, Loss: 0.3126
Batch 230, Loss: 0.2874
Batch 240, Loss: 0.2816
Batch 250, Loss: 0.2673
Batch 260, Loss: 0.3028
Batch 270, Loss: 0.2796
Batch 280, Loss: 0.2789
Batch 290, Loss: 0.2621
Batch 300, Loss: 0.2508
Batch 310, Loss: 0.2517
Batch 320, Loss: 0.2583
Batch 330, Loss: 0.2775
Batch 340, Loss: 0.2978
Batch 350, Loss: 0.3029
Batch 360, Loss: 0.3251
Batch 370, Loss: 0.3315
Batch 380, Loss: 0.2985
Batch 390, Loss: 0.2813
Epoch 19 learning rate: 0.06840622763423389
Epoch 19 time: 236.0987434387207 seconds
Epoch 19 accuracy: 84.89%
Batch 10, Loss: 0.2919
Batch 20, Loss: 0.2567
Batch 30, Loss: 0.2565
Batch 40, Loss: 0.2243
Batch 50, Loss: 0.2615
Batch 60, Loss: 0.2486
Batch 70, Loss: 0.2555
Batch 80, Loss: 0.2328
Batch 90, Loss: 0.2212
Batch 100, Loss: 0.2228
Batch 110, Loss: 0.2462
Batch 120, Loss: 0.2585
Batch 130, Loss: 0.2413
Batch 140, Loss: 0.2757
Batch 150, Loss: 0.2483
Batch 160, Loss: 0.2493
Batch 170, Loss: 0.2748
Batch 180, Loss: 0.2704
Batch 190, Loss: 0.2899
Batch 200, Loss: 0.2928
Batch 210, Loss: 0.2785
Batch 220, Loss: 0.2482
Batch 230, Loss: 0.2332
Batch 240, Loss: 0.2516
Batch 250, Loss: 0.2369
Batch 260, Loss: 0.2518
Batch 270, Loss: 0.2494
Batch 280, Loss: 0.3001
Batch 290, Loss: 0.2956
Batch 300, Loss: 0.2762
Batch 310, Loss: 0.2910
Batch 320, Loss: 0.2610
Batch 330, Loss: 0.2534
Batch 340, Loss: 0.2621
Batch 350, Loss: 0.2817
Batch 360, Loss: 0.2832
Batch 370, Loss: 0.2763
Batch 380, Loss: 0.3038
Batch 390, Loss: 0.2805
Epoch 20 learning rate: 0.06545084971874736
Epoch 20 time: 236.26098537445068 seconds
Epoch 20 accuracy: 85.54%
Batch 10, Loss: 0.2327
Batch 20, Loss: 0.2613
Batch 30, Loss: 0.2473
Batch 40, Loss: 0.2710
Batch 50, Loss: 0.2483
Batch 60, Loss: 0.2280
Batch 70, Loss: 0.2566
Batch 80, Loss: 0.2652
Batch 90, Loss: 0.2795
Batch 100, Loss: 0.2528
Batch 110, Loss: 0.2685
Batch 120, Loss: 0.2550
Batch 130, Loss: 0.2742
Batch 140, Loss: 0.2771
Batch 150, Loss: 0.2808
Batch 160, Loss: 0.2488
Batch 170, Loss: 0.2418
Batch 180, Loss: 0.2401
Batch 190, Loss: 0.2662
Batch 200, Loss: 0.2392
Batch 210, Loss: 0.2586
Batch 220, Loss: 0.2221
Batch 230, Loss: 0.2363
Batch 240, Loss: 0.2364
Batch 250, Loss: 0.2532
Batch 260, Loss: 0.2539
Batch 270, Loss: 0.2611
Batch 280, Loss: 0.2484
Batch 290, Loss: 0.2833
Batch 300, Loss: 0.2698
Batch 310, Loss: 0.2547
Batch 320, Loss: 0.2524
Batch 330, Loss: 0.2567
Batch 340, Loss: 0.2431
Batch 350, Loss: 0.2721
Batch 360, Loss: 0.2882
Batch 370, Loss: 0.2762
Batch 380, Loss: 0.2524
Batch 390, Loss: 0.2477
Epoch 21 learning rate: 0.06243449435824272
Epoch 21 time: 236.36051535606384 seconds
Epoch 21 accuracy: 86.48%
Batch 10, Loss: 0.2305
Batch 20, Loss: 0.2239
Batch 30, Loss: 0.2153
Batch 40, Loss: 0.2170
Batch 50, Loss: 0.2283
Batch 60, Loss: 0.2326
Batch 70, Loss: 0.2361
Batch 80, Loss: 0.2314
Batch 90, Loss: 0.2627
Batch 100, Loss: 0.2015
Batch 110, Loss: 0.2459
Batch 120, Loss: 0.2444
Batch 130, Loss: 0.2064
Batch 140, Loss: 0.2401
Batch 150, Loss: 0.2243
Batch 160, Loss: 0.2542
Batch 170, Loss: 0.2610
Batch 180, Loss: 0.2561
Batch 190, Loss: 0.2307
Batch 200, Loss: 0.2428
Batch 210, Loss: 0.2220
Batch 220, Loss: 0.2671
Batch 230, Loss: 0.2459
Batch 240, Loss: 0.2193
Batch 250, Loss: 0.2369
Batch 260, Loss: 0.2446
Batch 270, Loss: 0.2556
Batch 280, Loss: 0.2565
Batch 290, Loss: 0.2322
Batch 300, Loss: 0.2377
Batch 310, Loss: 0.2501
Batch 320, Loss: 0.2122
Batch 330, Loss: 0.2424
Batch 340, Loss: 0.2446
Batch 350, Loss: 0.2392
Batch 360, Loss: 0.2568
Batch 370, Loss: 0.2607
Batch 380, Loss: 0.2228
Batch 390, Loss: 0.2496
Epoch 22 learning rate: 0.05936906572928623
Epoch 22 time: 236.27074670791626 seconds
Epoch 22 accuracy: 85.28%
Batch 10, Loss: 0.2395
Batch 20, Loss: 0.2308
Batch 30, Loss: 0.2364
Batch 40, Loss: 0.2430
Batch 50, Loss: 0.1944
Batch 60, Loss: 0.2094
Batch 70, Loss: 0.2112
Batch 80, Loss: 0.2329
Batch 90, Loss: 0.2160
Batch 100, Loss: 0.2083
Batch 110, Loss: 0.2173
Batch 120, Loss: 0.2298
Batch 130, Loss: 0.2215
Batch 140, Loss: 0.2219
Batch 150, Loss: 0.2369
Batch 160, Loss: 0.2174
Batch 170, Loss: 0.2271
Batch 180, Loss: 0.2425
Batch 190, Loss: 0.2599
Batch 200, Loss: 0.2412
Batch 210, Loss: 0.2477
Batch 220, Loss: 0.2401
Batch 230, Loss: 0.2189
Batch 240, Loss: 0.2325
Batch 250, Loss: 0.2385
Batch 260, Loss: 0.2350
Batch 270, Loss: 0.2051
Batch 280, Loss: 0.2456
Batch 290, Loss: 0.2189
Batch 300, Loss: 0.2384
Batch 310, Loss: 0.2244
Batch 320, Loss: 0.2298
Batch 330, Loss: 0.2483
Batch 340, Loss: 0.2281
Batch 350, Loss: 0.2141
Batch 360, Loss: 0.2411
Batch 370, Loss: 0.2290
Batch 380, Loss: 0.2261
Batch 390, Loss: 0.2428
Epoch 23 learning rate: 0.056266661678215216
Epoch 23 time: 236.25895881652832 seconds
Epoch 23 accuracy: 86.38%
Batch 10, Loss: 0.2008
Batch 20, Loss: 0.2192
Batch 30, Loss: 0.1995
Batch 40, Loss: 0.2134
Batch 50, Loss: 0.2023
Batch 60, Loss: 0.2387
Batch 70, Loss: 0.1885
Batch 80, Loss: 0.1993
Batch 90, Loss: 0.2068
Batch 100, Loss: 0.1884
Batch 110, Loss: 0.2171
Batch 120, Loss: 0.2165
Batch 130, Loss: 0.2142
Batch 140, Loss: 0.2091
Batch 150, Loss: 0.2333
Batch 160, Loss: 0.2241
Batch 170, Loss: 0.2268
Batch 180, Loss: 0.2333
Batch 190, Loss: 0.2274
Batch 200, Loss: 0.2235
Batch 210, Loss: 0.1991
Batch 220, Loss: 0.2369
Batch 230, Loss: 0.1957
Batch 240, Loss: 0.2326
Batch 250, Loss: 0.2064
Batch 260, Loss: 0.2141
Batch 270, Loss: 0.2237
Batch 280, Loss: 0.2342
Batch 290, Loss: 0.2325
Batch 300, Loss: 0.2127
Batch 310, Loss: 0.2222
Batch 320, Loss: 0.2454
Batch 330, Loss: 0.2331
Batch 340, Loss: 0.2370
Batch 350, Loss: 0.2022
Batch 360, Loss: 0.2377
Batch 370, Loss: 0.2643
Batch 380, Loss: 0.2333
Batch 390, Loss: 0.2409
Epoch 24 learning rate: 0.053139525976465665
Epoch 24 time: 236.20719647407532 seconds
Epoch 24 accuracy: 87.87%
Batch 10, Loss: 0.2159
Batch 20, Loss: 0.2159
Batch 30, Loss: 0.1670
Batch 40, Loss: 0.2024
Batch 50, Loss: 0.2278
Batch 60, Loss: 0.2107
Batch 70, Loss: 0.2050
Batch 80, Loss: 0.1851
Batch 90, Loss: 0.2114
Batch 100, Loss: 0.2153
Batch 110, Loss: 0.1940
Batch 120, Loss: 0.2277
Batch 130, Loss: 0.2110
Batch 140, Loss: 0.2215
Batch 150, Loss: 0.2021
Batch 160, Loss: 0.2068
Batch 170, Loss: 0.1960
Batch 180, Loss: 0.1754
Batch 190, Loss: 0.2152
Batch 200, Loss: 0.2222
Batch 210, Loss: 0.2131
Batch 220, Loss: 0.1997
Batch 230, Loss: 0.1959
Batch 240, Loss: 0.2011
Batch 250, Loss: 0.2094
Batch 260, Loss: 0.2071
Batch 270, Loss: 0.2242
Batch 280, Loss: 0.1931
Batch 290, Loss: 0.2062
Batch 300, Loss: 0.2506
Batch 310, Loss: 0.2041
Batch 320, Loss: 0.2210
Batch 330, Loss: 0.2171
Batch 340, Loss: 0.2179
Batch 350, Loss: 0.2170
Batch 360, Loss: 0.2072
Batch 370, Loss: 0.2092
Batch 380, Loss: 0.2097
Batch 390, Loss: 0.2086
Epoch 25 learning rate: 0.049999999999999996
Epoch 25 time: 236.41819524765015 seconds
Epoch 25 accuracy: 87.98%
Batch 10, Loss: 0.1904
Batch 20, Loss: 0.1863
Batch 30, Loss: 0.1889
Batch 40, Loss: 0.2161
Batch 50, Loss: 0.2073
Batch 60, Loss: 0.1723
Batch 70, Loss: 0.1945
Batch 80, Loss: 0.1937
Batch 90, Loss: 0.1735
Batch 100, Loss: 0.1825
Batch 110, Loss: 0.1869
Batch 120, Loss: 0.1971
Batch 130, Loss: 0.1814
Batch 140, Loss: 0.1941
Batch 150, Loss: 0.2028
Batch 160, Loss: 0.2193
Batch 170, Loss: 0.1833
Batch 180, Loss: 0.1957
Batch 190, Loss: 0.1797
Batch 200, Loss: 0.1783
Batch 210, Loss: 0.2108
Batch 220, Loss: 0.2404
Batch 230, Loss: 0.2191
Batch 240, Loss: 0.2132
Batch 250, Loss: 0.2173
Batch 260, Loss: 0.2017
Batch 270, Loss: 0.1834
Batch 280, Loss: 0.2232
Batch 290, Loss: 0.2008
Batch 300, Loss: 0.2124
Batch 310, Loss: 0.2173
Batch 320, Loss: 0.2083
Batch 330, Loss: 0.1869
Batch 340, Loss: 0.1854
Batch 350, Loss: 0.1903
Batch 360, Loss: 0.1805
Batch 370, Loss: 0.2131
Batch 380, Loss: 0.2106
Batch 390, Loss: 0.2171
Epoch 26 learning rate: 0.046860474023534326
Epoch 26 time: 236.34907746315002 seconds
Epoch 26 accuracy: 87.36%
Batch 10, Loss: 0.1895
Batch 20, Loss: 0.1598
Batch 30, Loss: 0.1666
Batch 40, Loss: 0.1792
Batch 50, Loss: 0.1850
Batch 60, Loss: 0.1781
Batch 70, Loss: 0.1808
Batch 80, Loss: 0.1780
Batch 90, Loss: 0.1728
Batch 100, Loss: 0.1846
Batch 110, Loss: 0.1813
Batch 120, Loss: 0.1839
Batch 130, Loss: 0.1595
Batch 140, Loss: 0.1833
Batch 150, Loss: 0.1524
Batch 160, Loss: 0.1886
Batch 170, Loss: 0.1982
Batch 180, Loss: 0.2159
Batch 190, Loss: 0.1868
Batch 200, Loss: 0.1928
Batch 210, Loss: 0.2017
Batch 220, Loss: 0.1890
Batch 230, Loss: 0.1804
Batch 240, Loss: 0.2129
Batch 250, Loss: 0.1925
Batch 260, Loss: 0.1686
Batch 270, Loss: 0.2086
Batch 280, Loss: 0.1970
Batch 290, Loss: 0.2188
Batch 300, Loss: 0.2201
Batch 310, Loss: 0.1768
Batch 320, Loss: 0.1714
Batch 330, Loss: 0.1831
Batch 340, Loss: 0.1906
Batch 350, Loss: 0.2004
Batch 360, Loss: 0.1985
Batch 370, Loss: 0.1815
Batch 380, Loss: 0.2216
Batch 390, Loss: 0.1894
Epoch 27 learning rate: 0.04373333832178478
Epoch 27 time: 236.2694969177246 seconds
Epoch 27 accuracy: 88.98%
Batch 10, Loss: 0.1877
Batch 20, Loss: 0.1680
Batch 30, Loss: 0.1639
Batch 40, Loss: 0.1615
Batch 50, Loss: 0.1606
Batch 60, Loss: 0.1558
Batch 70, Loss: 0.1890
Batch 80, Loss: 0.1610
Batch 90, Loss: 0.1582
Batch 100, Loss: 0.1815
Batch 110, Loss: 0.1630
Batch 120, Loss: 0.1773
Batch 130, Loss: 0.1609
Batch 140, Loss: 0.1709
Batch 150, Loss: 0.1745
Batch 160, Loss: 0.1711
Batch 170, Loss: 0.1819
Batch 180, Loss: 0.1788
Batch 190, Loss: 0.1762
Batch 200, Loss: 0.2039
Batch 210, Loss: 0.2028
Batch 220, Loss: 0.1837
Batch 230, Loss: 0.1739
Batch 240, Loss: 0.2028
Batch 250, Loss: 0.1657
Batch 260, Loss: 0.1946
Batch 270, Loss: 0.1871
Batch 280, Loss: 0.1887
Batch 290, Loss: 0.1574
Batch 300, Loss: 0.1665
Batch 310, Loss: 0.1593
Batch 320, Loss: 0.1851
Batch 330, Loss: 0.1737
Batch 340, Loss: 0.1985
Batch 350, Loss: 0.1914
Batch 360, Loss: 0.1882
Batch 370, Loss: 0.1902
Batch 380, Loss: 0.1945
Batch 390, Loss: 0.1947
Epoch 28 learning rate: 0.040630934270713764
Epoch 28 time: 236.25834584236145 seconds
Epoch 28 accuracy: 89.41%
Batch 10, Loss: 0.1647
Batch 20, Loss: 0.1724
Batch 30, Loss: 0.1465
Batch 40, Loss: 0.1502
Batch 50, Loss: 0.1441
Batch 60, Loss: 0.1687
Batch 70, Loss: 0.1508
Batch 80, Loss: 0.1743
Batch 90, Loss: 0.1459
Batch 100, Loss: 0.1615
Batch 110, Loss: 0.1762
Batch 120, Loss: 0.1600
Batch 130, Loss: 0.1639
Batch 140, Loss: 0.1781
Batch 150, Loss: 0.1310
Batch 160, Loss: 0.1604
Batch 170, Loss: 0.1523
Batch 180, Loss: 0.1558
Batch 190, Loss: 0.1467
Batch 200, Loss: 0.1493
Batch 210, Loss: 0.1930
Batch 220, Loss: 0.1807
Batch 230, Loss: 0.1744
Batch 240, Loss: 0.1828
Batch 250, Loss: 0.1556
Batch 260, Loss: 0.1390
Batch 270, Loss: 0.1720
Batch 280, Loss: 0.1740
Batch 290, Loss: 0.1500
Batch 300, Loss: 0.1687
Batch 310, Loss: 0.1923
Batch 320, Loss: 0.1545
Batch 330, Loss: 0.1725
Batch 340, Loss: 0.1794
Batch 350, Loss: 0.1839
Batch 360, Loss: 0.1577
Batch 370, Loss: 0.1656
Batch 380, Loss: 0.2025
Batch 390, Loss: 0.1910
Epoch 29 learning rate: 0.03756550564175726
Epoch 29 time: 236.18817973136902 seconds
Epoch 29 accuracy: 89.6%
Batch 10, Loss: 0.1457
Batch 20, Loss: 0.1476
Batch 30, Loss: 0.1397
Batch 40, Loss: 0.1673
Batch 50, Loss: 0.1529
Batch 60, Loss: 0.1585
Batch 70, Loss: 0.1351
Batch 80, Loss: 0.1337
Batch 90, Loss: 0.1458
Batch 100, Loss: 0.1561
Batch 110, Loss: 0.1325
Batch 120, Loss: 0.1450
Batch 130, Loss: 0.1434
Batch 140, Loss: 0.1603
Batch 150, Loss: 0.1411
Batch 160, Loss: 0.1608
Batch 170, Loss: 0.1732
Batch 180, Loss: 0.1623
Batch 190, Loss: 0.1595
Batch 200, Loss: 0.1533
Batch 210, Loss: 0.1509
Batch 220, Loss: 0.1600
Batch 230, Loss: 0.1562
Batch 240, Loss: 0.1454
Batch 250, Loss: 0.1608
Batch 260, Loss: 0.1773
Batch 270, Loss: 0.1511
Batch 280, Loss: 0.1314
Batch 290, Loss: 0.1732
Batch 300, Loss: 0.1690
Batch 310, Loss: 0.1764
Batch 320, Loss: 0.1642
Batch 330, Loss: 0.1652
Batch 340, Loss: 0.1650
Batch 350, Loss: 0.1668
Batch 360, Loss: 0.1354
Batch 370, Loss: 0.1729
Batch 380, Loss: 0.1597
Batch 390, Loss: 0.1582
Epoch 30 learning rate: 0.03454915028125265
Epoch 30 time: 236.10210824012756 seconds
Epoch 30 accuracy: 89.94%
Batch 10, Loss: 0.1193
Batch 20, Loss: 0.1379
Batch 30, Loss: 0.1368
Batch 40, Loss: 0.1250
Batch 50, Loss: 0.1458
Batch 60, Loss: 0.1388
Batch 70, Loss: 0.1343
Batch 80, Loss: 0.1115
Batch 90, Loss: 0.1356
Batch 100, Loss: 0.1445
Batch 110, Loss: 0.1366
Batch 120, Loss: 0.1494
Batch 130, Loss: 0.1401
Batch 140, Loss: 0.1462
Batch 150, Loss: 0.1410
Batch 160, Loss: 0.1559
Batch 170, Loss: 0.1663
Batch 180, Loss: 0.1714
Batch 190, Loss: 0.1317
Batch 200, Loss: 0.1584
Batch 210, Loss: 0.1534
Batch 220, Loss: 0.1522
Batch 230, Loss: 0.1570
Batch 240, Loss: 0.1483
Batch 250, Loss: 0.1403
Batch 260, Loss: 0.1405
Batch 270, Loss: 0.1489
Batch 280, Loss: 0.1458
Batch 290, Loss: 0.1509
Batch 300, Loss: 0.1300
Batch 310, Loss: 0.1507
Batch 320, Loss: 0.1533
Batch 330, Loss: 0.1544
Batch 340, Loss: 0.1503
Batch 350, Loss: 0.1541
Batch 360, Loss: 0.1584
Batch 370, Loss: 0.1267
Batch 380, Loss: 0.1627
Batch 390, Loss: 0.1448
Epoch 31 learning rate: 0.03159377236576612
Epoch 31 time: 236.20239877700806 seconds
Epoch 31 accuracy: 89.82%
Batch 10, Loss: 0.1290
Batch 20, Loss: 0.1249
Batch 30, Loss: 0.1123
Batch 40, Loss: 0.1195
Batch 50, Loss: 0.1344
Batch 60, Loss: 0.1167
Batch 70, Loss: 0.1181
Batch 80, Loss: 0.1101
Batch 90, Loss: 0.1151
Batch 100, Loss: 0.1273
Batch 110, Loss: 0.1380
Batch 120, Loss: 0.1210
Batch 130, Loss: 0.1101
Batch 140, Loss: 0.1304
Batch 150, Loss: 0.1240
Batch 160, Loss: 0.1325
Batch 170, Loss: 0.1356
Batch 180, Loss: 0.1430
Batch 190, Loss: 0.1344
Batch 200, Loss: 0.1508
Batch 210, Loss: 0.1306
Batch 220, Loss: 0.1289
Batch 230, Loss: 0.1291
Batch 240, Loss: 0.1279
Batch 250, Loss: 0.1174
Batch 260, Loss: 0.1314
Batch 270, Loss: 0.1217
Batch 280, Loss: 0.1466
Batch 290, Loss: 0.1481
Batch 300, Loss: 0.1678
Batch 310, Loss: 0.1371
Batch 320, Loss: 0.1454
Batch 330, Loss: 0.1341
Batch 340, Loss: 0.1357
Batch 350, Loss: 0.1300
Batch 360, Loss: 0.1509
Batch 370, Loss: 0.1403
Batch 380, Loss: 0.1695
Batch 390, Loss: 0.1376
Epoch 32 learning rate: 0.028711035421746366
Epoch 32 time: 236.2580907344818 seconds
Epoch 32 accuracy: 90.75%
Batch 10, Loss: 0.1292
Batch 20, Loss: 0.1175
Batch 30, Loss: 0.0997
Batch 40, Loss: 0.1075
Batch 50, Loss: 0.1253
Batch 60, Loss: 0.1122
Batch 70, Loss: 0.1051
Batch 80, Loss: 0.1026
Batch 90, Loss: 0.1121
Batch 100, Loss: 0.1202
Batch 110, Loss: 0.1213
Batch 120, Loss: 0.1165
Batch 130, Loss: 0.1218
Batch 140, Loss: 0.1347
Batch 150, Loss: 0.1096
Batch 160, Loss: 0.1217
Batch 170, Loss: 0.1378
Batch 180, Loss: 0.1079
Batch 190, Loss: 0.1173
Batch 200, Loss: 0.1298
Batch 210, Loss: 0.1311
Batch 220, Loss: 0.1298
Batch 230, Loss: 0.1199
Batch 240, Loss: 0.1082
Batch 250, Loss: 0.1245
Batch 260, Loss: 0.1261
Batch 270, Loss: 0.1196
Batch 280, Loss: 0.1206
Batch 290, Loss: 0.1377
Batch 300, Loss: 0.1387
Batch 310, Loss: 0.1313
Batch 320, Loss: 0.1208
Batch 330, Loss: 0.1414
Batch 340, Loss: 0.1176
Batch 350, Loss: 0.1111
Batch 360, Loss: 0.1148
Batch 370, Loss: 0.1393
Batch 380, Loss: 0.1225
Batch 390, Loss: 0.1421
Epoch 33 learning rate: 0.025912316294914226
Epoch 33 time: 236.21743249893188 seconds
Epoch 33 accuracy: 91.1%
Batch 10, Loss: 0.1064
Batch 20, Loss: 0.1489
Batch 30, Loss: 0.0998
Batch 40, Loss: 0.1046
Batch 50, Loss: 0.1144
Batch 60, Loss: 0.1008
Batch 70, Loss: 0.1006
Batch 80, Loss: 0.1157
Batch 90, Loss: 0.0952
Batch 100, Loss: 0.1068
Batch 110, Loss: 0.1078
Batch 120, Loss: 0.0991
Batch 130, Loss: 0.0975
Batch 140, Loss: 0.0937
Batch 150, Loss: 0.1022
Batch 160, Loss: 0.1013
Batch 170, Loss: 0.1062
Batch 180, Loss: 0.1308
Batch 190, Loss: 0.1178
Batch 200, Loss: 0.1165
Batch 210, Loss: 0.1039
Batch 220, Loss: 0.1046
Batch 230, Loss: 0.1152
Batch 240, Loss: 0.0882
Batch 250, Loss: 0.0980
Batch 260, Loss: 0.1085
Batch 270, Loss: 0.1193
Batch 280, Loss: 0.1204
Batch 290, Loss: 0.1102
Batch 300, Loss: 0.1058
Batch 310, Loss: 0.1068
Batch 320, Loss: 0.1049
Batch 330, Loss: 0.1216
Batch 340, Loss: 0.1080
Batch 350, Loss: 0.1332
Batch 360, Loss: 0.1215
Batch 370, Loss: 0.1064
Batch 380, Loss: 0.1101
Batch 390, Loss: 0.1321
Epoch 34 learning rate: 0.023208660251050155
Epoch 34 time: 236.34246611595154 seconds
Epoch 34 accuracy: 90.15%
Batch 10, Loss: 0.0933
Batch 20, Loss: 0.0873
Batch 30, Loss: 0.0971
Batch 40, Loss: 0.0867
Batch 50, Loss: 0.1070
Batch 60, Loss: 0.0934
Batch 70, Loss: 0.1035
Batch 80, Loss: 0.0924
Batch 90, Loss: 0.1044
Batch 100, Loss: 0.1020
Batch 110, Loss: 0.0890
Batch 120, Loss: 0.1004
Batch 130, Loss: 0.0997
Batch 140, Loss: 0.0761
Batch 150, Loss: 0.0980
Batch 160, Loss: 0.0948
Batch 170, Loss: 0.0895
Batch 180, Loss: 0.1162
Batch 190, Loss: 0.0828
Batch 200, Loss: 0.1067
Batch 210, Loss: 0.0992
Batch 220, Loss: 0.1130
Batch 230, Loss: 0.0943
Batch 240, Loss: 0.0983
Batch 250, Loss: 0.1056
Batch 260, Loss: 0.1129
Batch 270, Loss: 0.1119
Batch 280, Loss: 0.1173
Batch 290, Loss: 0.1038
Batch 300, Loss: 0.0908
Batch 310, Loss: 0.1060
Batch 320, Loss: 0.0812
Batch 330, Loss: 0.1244
Batch 340, Loss: 0.1006
Batch 350, Loss: 0.1093
Batch 360, Loss: 0.0925
Batch 370, Loss: 0.0930
Batch 380, Loss: 0.0832
Batch 390, Loss: 0.1095
Epoch 35 learning rate: 0.020610737385376346
Epoch 35 time: 236.19270420074463 seconds
Epoch 35 accuracy: 91.68%
Batch 10, Loss: 0.1001
Batch 20, Loss: 0.0963
Batch 30, Loss: 0.0858
Batch 40, Loss: 0.0734
Batch 50, Loss: 0.0862
Batch 60, Loss: 0.0845
Batch 70, Loss: 0.0769
Batch 80, Loss: 0.0760
Batch 90, Loss: 0.0806
Batch 100, Loss: 0.0766
Batch 110, Loss: 0.0893
Batch 120, Loss: 0.1149
Batch 130, Loss: 0.0950
Batch 140, Loss: 0.0890
Batch 150, Loss: 0.0794
Batch 160, Loss: 0.0896
Batch 170, Loss: 0.0879
Batch 180, Loss: 0.0887
Batch 190, Loss: 0.0868
Batch 200, Loss: 0.0794
Batch 210, Loss: 0.0724
Batch 220, Loss: 0.0856
Batch 230, Loss: 0.0666
Batch 240, Loss: 0.0964
Batch 250, Loss: 0.1010
Batch 260, Loss: 0.0636
Batch 270, Loss: 0.0936
Batch 280, Loss: 0.0903
Batch 290, Loss: 0.1027
Batch 300, Loss: 0.0817
Batch 310, Loss: 0.0784
Batch 320, Loss: 0.0998
Batch 330, Loss: 0.0798
Batch 340, Loss: 0.0799
Batch 350, Loss: 0.0802
Batch 360, Loss: 0.0759
Batch 370, Loss: 0.0859
Batch 380, Loss: 0.0923
Batch 390, Loss: 0.0864
Epoch 36 learning rate: 0.01812880051256551
Epoch 36 time: 236.1502935886383 seconds
Epoch 36 accuracy: 92.25%
Batch 10, Loss: 0.0684
Batch 20, Loss: 0.0768
Batch 30, Loss: 0.0725
Batch 40, Loss: 0.0603
Batch 50, Loss: 0.0828
Batch 60, Loss: 0.0674
Batch 70, Loss: 0.0703
Batch 80, Loss: 0.0678
Batch 90, Loss: 0.0760
Batch 100, Loss: 0.0640
Batch 110, Loss: 0.0716
Batch 120, Loss: 0.0737
Batch 130, Loss: 0.0671
Batch 140, Loss: 0.0665
Batch 150, Loss: 0.0639
Batch 160, Loss: 0.0833
Batch 170, Loss: 0.0814
Batch 180, Loss: 0.0675
Batch 190, Loss: 0.0676
Batch 200, Loss: 0.0775
Batch 210, Loss: 0.0700
Batch 220, Loss: 0.0748
Batch 230, Loss: 0.0813
Batch 240, Loss: 0.0762
Batch 250, Loss: 0.0796
Batch 260, Loss: 0.0667
Batch 270, Loss: 0.0740
Batch 280, Loss: 0.0699
Batch 290, Loss: 0.0730
Batch 300, Loss: 0.0658
Batch 310, Loss: 0.0806
Batch 320, Loss: 0.0776
Batch 330, Loss: 0.0765
Batch 340, Loss: 0.0662
Batch 350, Loss: 0.0745
Batch 360, Loss: 0.0768
Batch 370, Loss: 0.0686
Batch 380, Loss: 0.0731
Batch 390, Loss: 0.0738
Epoch 37 learning rate: 0.015772644703565562
Epoch 37 time: 236.25422716140747 seconds
Epoch 37 accuracy: 92.64%
Batch 10, Loss: 0.0627
Batch 20, Loss: 0.0766
Batch 30, Loss: 0.0633
Batch 40, Loss: 0.0545
Batch 50, Loss: 0.0554
Batch 60, Loss: 0.0600
Batch 70, Loss: 0.0702
Batch 80, Loss: 0.0643
Batch 90, Loss: 0.0686
Batch 100, Loss: 0.0638
Batch 110, Loss: 0.0635
Batch 120, Loss: 0.0803
Batch 130, Loss: 0.0561
Batch 140, Loss: 0.0574
Batch 150, Loss: 0.0558
Batch 160, Loss: 0.0590
Batch 170, Loss: 0.0603
Batch 180, Loss: 0.0641
Batch 190, Loss: 0.0664
Batch 200, Loss: 0.0568
Batch 210, Loss: 0.0719
Batch 220, Loss: 0.0626
Batch 230, Loss: 0.0639
Batch 240, Loss: 0.0611
Batch 250, Loss: 0.0604
Batch 260, Loss: 0.0758
Batch 270, Loss: 0.0600
Batch 280, Loss: 0.0706
Batch 290, Loss: 0.0660
Batch 300, Loss: 0.0484
Batch 310, Loss: 0.0653
Batch 320, Loss: 0.0612
Batch 330, Loss: 0.0680
Batch 340, Loss: 0.0642
Batch 350, Loss: 0.0697
Batch 360, Loss: 0.0660
Batch 370, Loss: 0.0704
Batch 380, Loss: 0.0810
Batch 390, Loss: 0.0729
Epoch 38 learning rate: 0.013551568628929433
Epoch 38 time: 236.166841506958 seconds
Epoch 38 accuracy: 92.47%
Batch 10, Loss: 0.0523
Batch 20, Loss: 0.0558
Batch 30, Loss: 0.0602
Batch 40, Loss: 0.0529
Batch 50, Loss: 0.0546
Batch 60, Loss: 0.0513
Batch 70, Loss: 0.0498
Batch 80, Loss: 0.0464
Batch 90, Loss: 0.0669
Batch 100, Loss: 0.0508
Batch 110, Loss: 0.0631
Batch 120, Loss: 0.0614
Batch 130, Loss: 0.0662
Batch 140, Loss: 0.0484
Batch 150, Loss: 0.0480
Batch 160, Loss: 0.0429
Batch 170, Loss: 0.0524
Batch 180, Loss: 0.0461
Batch 190, Loss: 0.0530
Batch 200, Loss: 0.0453
Batch 210, Loss: 0.0445
Batch 220, Loss: 0.0448
Batch 230, Loss: 0.0577
Batch 240, Loss: 0.0522
Batch 250, Loss: 0.0547
Batch 260, Loss: 0.0576
Batch 270, Loss: 0.0508
Batch 280, Loss: 0.0550
Batch 290, Loss: 0.0649
Batch 300, Loss: 0.0560
Batch 310, Loss: 0.0522
Batch 320, Loss: 0.0631
Batch 330, Loss: 0.0622
Batch 340, Loss: 0.0505
Batch 350, Loss: 0.0551
Batch 360, Loss: 0.0603
Batch 370, Loss: 0.0562
Batch 380, Loss: 0.0535
Batch 390, Loss: 0.0531
Epoch 39 learning rate: 0.011474337861210542
Epoch 39 time: 236.13166856765747 seconds
Epoch 39 accuracy: 93.24%
Batch 10, Loss: 0.0445
Batch 20, Loss: 0.0491
Batch 30, Loss: 0.0456
Batch 40, Loss: 0.0389
Batch 50, Loss: 0.0534
Batch 60, Loss: 0.0418
Batch 70, Loss: 0.0441
Batch 80, Loss: 0.0423
Batch 90, Loss: 0.0489
Batch 100, Loss: 0.0415
Batch 110, Loss: 0.0417
Batch 120, Loss: 0.0380
Batch 130, Loss: 0.0403
Batch 140, Loss: 0.0453
Batch 150, Loss: 0.0460
Batch 160, Loss: 0.0423
Batch 170, Loss: 0.0395
Batch 180, Loss: 0.0400
Batch 190, Loss: 0.0488
Batch 200, Loss: 0.0401
Batch 210, Loss: 0.0337
Batch 220, Loss: 0.0507
Batch 230, Loss: 0.0505
Batch 240, Loss: 0.0418
Batch 250, Loss: 0.0464
Batch 260, Loss: 0.0473
Batch 270, Loss: 0.0527
Batch 280, Loss: 0.0605
Batch 290, Loss: 0.0522
Batch 300, Loss: 0.0402
Batch 310, Loss: 0.0382
Batch 320, Loss: 0.0461
Batch 330, Loss: 0.0508
Batch 340, Loss: 0.0392
Batch 350, Loss: 0.0544
Batch 360, Loss: 0.0383
Batch 370, Loss: 0.0478
Batch 380, Loss: 0.0530
Batch 390, Loss: 0.0414
Epoch 40 learning rate: 0.009549150281252632
Epoch 40 time: 236.2761332988739 seconds
Epoch 40 accuracy: 93.35%
Batch 10, Loss: 0.0469
Batch 20, Loss: 0.0425
Batch 30, Loss: 0.0314
Batch 40, Loss: 0.0514
Batch 50, Loss: 0.0491
Batch 60, Loss: 0.0353
Batch 70, Loss: 0.0362
Batch 80, Loss: 0.0395
Batch 90, Loss: 0.0326
Batch 100, Loss: 0.0390
Batch 110, Loss: 0.0324
Batch 120, Loss: 0.0355
Batch 130, Loss: 0.0243
Batch 140, Loss: 0.0308
Batch 150, Loss: 0.0446
Batch 160, Loss: 0.0387
Batch 170, Loss: 0.0407
Batch 180, Loss: 0.0324
Batch 190, Loss: 0.0333
Batch 200, Loss: 0.0365
Batch 210, Loss: 0.0426
Batch 220, Loss: 0.0350
Batch 230, Loss: 0.0402
Batch 240, Loss: 0.0372
Batch 250, Loss: 0.0389
Batch 260, Loss: 0.0463
Batch 270, Loss: 0.0315
Batch 280, Loss: 0.0355
Batch 290, Loss: 0.0422
Batch 300, Loss: 0.0419
Batch 310, Loss: 0.0420
Batch 320, Loss: 0.0394
Batch 330, Loss: 0.0394
Batch 340, Loss: 0.0493
Batch 350, Loss: 0.0402
Batch 360, Loss: 0.0387
Batch 370, Loss: 0.0354
Batch 380, Loss: 0.0341
Batch 390, Loss: 0.0514
Epoch 41 learning rate: 0.007783603724899257
Epoch 41 time: 236.37152886390686 seconds
Epoch 41 accuracy: 93.74%
Batch 10, Loss: 0.0272
Batch 20, Loss: 0.0283
Batch 30, Loss: 0.0236
Batch 40, Loss: 0.0320
Batch 50, Loss: 0.0248
Batch 60, Loss: 0.0336
Batch 70, Loss: 0.0280
Batch 80, Loss: 0.0237
Batch 90, Loss: 0.0317
Batch 100, Loss: 0.0233
Batch 110, Loss: 0.0333
Batch 120, Loss: 0.0351
Batch 130, Loss: 0.0368
Batch 140, Loss: 0.0246
Batch 150, Loss: 0.0331
Batch 160, Loss: 0.0258
Batch 170, Loss: 0.0258
Batch 180, Loss: 0.0268
Batch 190, Loss: 0.0338
Batch 200, Loss: 0.0282
Batch 210, Loss: 0.0261
Batch 220, Loss: 0.0285
Batch 230, Loss: 0.0263
Batch 240, Loss: 0.0245
Batch 250, Loss: 0.0294
Batch 260, Loss: 0.0389
Batch 270, Loss: 0.0299
Batch 280, Loss: 0.0339
Batch 290, Loss: 0.0384
Batch 300, Loss: 0.0304
Batch 310, Loss: 0.0303
Batch 320, Loss: 0.0380
Batch 330, Loss: 0.0305
Batch 340, Loss: 0.0316
Batch 350, Loss: 0.0384
Batch 360, Loss: 0.0282
Batch 370, Loss: 0.0302
Batch 380, Loss: 0.0320
Batch 390, Loss: 0.0330
Epoch 42 learning rate: 0.0061846659978068205
Epoch 42 time: 236.26597452163696 seconds
Epoch 42 accuracy: 93.46%
Batch 10, Loss: 0.0274
Batch 20, Loss: 0.0333
Batch 30, Loss: 0.0256
Batch 40, Loss: 0.0270
Batch 50, Loss: 0.0266
Batch 60, Loss: 0.0355
Batch 70, Loss: 0.0260
Batch 80, Loss: 0.0241
Batch 90, Loss: 0.0223
Batch 100, Loss: 0.0284
Batch 110, Loss: 0.0232
Batch 120, Loss: 0.0262
Batch 130, Loss: 0.0227
Batch 140, Loss: 0.0232
Batch 150, Loss: 0.0211
Batch 160, Loss: 0.0213
Batch 170, Loss: 0.0215
Batch 180, Loss: 0.0284
Batch 190, Loss: 0.0256
Batch 200, Loss: 0.0173
Batch 210, Loss: 0.0248
Batch 220, Loss: 0.0350
Batch 230, Loss: 0.0293
Batch 240, Loss: 0.0245
Batch 250, Loss: 0.0231
Batch 260, Loss: 0.0246
Batch 270, Loss: 0.0288
Batch 280, Loss: 0.0249
Batch 290, Loss: 0.0236
Batch 300, Loss: 0.0291
Batch 310, Loss: 0.0261
Batch 320, Loss: 0.0221
Batch 330, Loss: 0.0215
Batch 340, Loss: 0.0237
Batch 350, Loss: 0.0274
Batch 360, Loss: 0.0227
Batch 370, Loss: 0.0195
Batch 380, Loss: 0.0222
Batch 390, Loss: 0.0341
Epoch 43 learning rate: 0.004758647376699033
Epoch 43 time: 236.29042768478394 seconds
Epoch 43 accuracy: 93.66%
Batch 10, Loss: 0.0182
Batch 20, Loss: 0.0250
Batch 30, Loss: 0.0213
Batch 40, Loss: 0.0232
Batch 50, Loss: 0.0182
Batch 60, Loss: 0.0248
Batch 70, Loss: 0.0275
Batch 80, Loss: 0.0221
Batch 90, Loss: 0.0175
Batch 100, Loss: 0.0201
Batch 110, Loss: 0.0244
Batch 120, Loss: 0.0162
Batch 130, Loss: 0.0200
Batch 140, Loss: 0.0237
Batch 150, Loss: 0.0304
Batch 160, Loss: 0.0191
Batch 170, Loss: 0.0175
Batch 180, Loss: 0.0200
Batch 190, Loss: 0.0205
Batch 200, Loss: 0.0196
Batch 210, Loss: 0.0231
Batch 220, Loss: 0.0273
Batch 230, Loss: 0.0193
Batch 240, Loss: 0.0209
Batch 250, Loss: 0.0201
Batch 260, Loss: 0.0219
Batch 270, Loss: 0.0203
Batch 280, Loss: 0.0251
Batch 290, Loss: 0.0214
Batch 300, Loss: 0.0221
Batch 310, Loss: 0.0194
Batch 320, Loss: 0.0230
Batch 330, Loss: 0.0234
Batch 340, Loss: 0.0283
Batch 350, Loss: 0.0207
Batch 360, Loss: 0.0171
Batch 370, Loss: 0.0226
Batch 380, Loss: 0.0282
Batch 390, Loss: 0.0222
Epoch 44 learning rate: 0.0035111757055874327
Epoch 44 time: 236.27147340774536 seconds
Epoch 44 accuracy: 94.01%
Batch 10, Loss: 0.0107
Batch 20, Loss: 0.0180
Batch 30, Loss: 0.0161
Batch 40, Loss: 0.0173
Batch 50, Loss: 0.0147
Batch 60, Loss: 0.0159
Batch 70, Loss: 0.0181
Batch 80, Loss: 0.0212
Batch 90, Loss: 0.0165
Batch 100, Loss: 0.0203
Batch 110, Loss: 0.0230
Batch 120, Loss: 0.0189
Batch 130, Loss: 0.0171
Batch 140, Loss: 0.0215
Batch 150, Loss: 0.0167
Batch 160, Loss: 0.0142
Batch 170, Loss: 0.0234
Batch 180, Loss: 0.0129
Batch 190, Loss: 0.0201
Batch 200, Loss: 0.0196
Batch 210, Loss: 0.0194
Batch 220, Loss: 0.0163
Batch 230, Loss: 0.0154
Batch 240, Loss: 0.0169
Batch 250, Loss: 0.0197
Batch 260, Loss: 0.0171
Batch 270, Loss: 0.0235
Batch 280, Loss: 0.0254
Batch 290, Loss: 0.0173
Batch 300, Loss: 0.0235
Batch 310, Loss: 0.0186
Batch 320, Loss: 0.0161
Batch 330, Loss: 0.0260
Batch 340, Loss: 0.0197
Batch 350, Loss: 0.0193
Batch 360, Loss: 0.0200
Batch 370, Loss: 0.0210
Batch 380, Loss: 0.0182
Batch 390, Loss: 0.0222
Epoch 45 learning rate: 0.0024471741852423235
Epoch 45 time: 236.3047263622284 seconds
Epoch 45 accuracy: 94.12%
Batch 10, Loss: 0.0182
Batch 20, Loss: 0.0174
Batch 30, Loss: 0.0169
Batch 40, Loss: 0.0196
Batch 50, Loss: 0.0135
Batch 60, Loss: 0.0164
Batch 70, Loss: 0.0151
Batch 80, Loss: 0.0121
Batch 90, Loss: 0.0173
Batch 100, Loss: 0.0165
Batch 110, Loss: 0.0175
Batch 120, Loss: 0.0128
Batch 130, Loss: 0.0154
Batch 140, Loss: 0.0193
Batch 150, Loss: 0.0137
Batch 160, Loss: 0.0176
Batch 170, Loss: 0.0130
Batch 180, Loss: 0.0188
Batch 190, Loss: 0.0164
Batch 200, Loss: 0.0149
Batch 210, Loss: 0.0115
Batch 220, Loss: 0.0140
Batch 230, Loss: 0.0224
Batch 240, Loss: 0.0150
Batch 250, Loss: 0.0147
Batch 260, Loss: 0.0190
Batch 270, Loss: 0.0144
Batch 280, Loss: 0.0149
Batch 290, Loss: 0.0147
Batch 300, Loss: 0.0176
Batch 310, Loss: 0.0168
Batch 320, Loss: 0.0126
Batch 330, Loss: 0.0138
Batch 340, Loss: 0.0126
Batch 350, Loss: 0.0140
Batch 360, Loss: 0.0222
Batch 370, Loss: 0.0126
Batch 380, Loss: 0.0166
Batch 390, Loss: 0.0170
Epoch 46 learning rate: 0.0015708419435684518
Epoch 46 time: 236.26703691482544 seconds
Epoch 46 accuracy: 94.47%
Batch 10, Loss: 0.0119
Batch 20, Loss: 0.0152
Batch 30, Loss: 0.0163
Batch 40, Loss: 0.0176
Batch 50, Loss: 0.0120
Batch 60, Loss: 0.0139
Batch 70, Loss: 0.0153
Batch 80, Loss: 0.0151
Batch 90, Loss: 0.0134
Batch 100, Loss: 0.0200
Batch 110, Loss: 0.0129
Batch 120, Loss: 0.0165
Batch 130, Loss: 0.0134
Batch 140, Loss: 0.0138
Batch 150, Loss: 0.0137
Batch 160, Loss: 0.0120
Batch 170, Loss: 0.0171
Batch 180, Loss: 0.0109
Batch 190, Loss: 0.0135
Batch 200, Loss: 0.0171
Batch 210, Loss: 0.0106
Batch 220, Loss: 0.0156
Batch 230, Loss: 0.0134
Batch 240, Loss: 0.0116
Batch 250, Loss: 0.0156
Batch 260, Loss: 0.0150
Batch 270, Loss: 0.0121
Batch 280, Loss: 0.0129
Batch 290, Loss: 0.0130
Batch 300, Loss: 0.0186
Batch 310, Loss: 0.0119
Batch 320, Loss: 0.0171
Batch 330, Loss: 0.0149
Batch 340, Loss: 0.0143
Batch 350, Loss: 0.0156
Batch 360, Loss: 0.0142
Batch 370, Loss: 0.0132
Batch 380, Loss: 0.0159
Batch 390, Loss: 0.0214
Epoch 47 learning rate: 0.000885637463565564
Epoch 47 time: 236.33692598342896 seconds
Epoch 47 accuracy: 94.31%
Batch 10, Loss: 0.0130
Batch 20, Loss: 0.0144
Batch 30, Loss: 0.0169
Batch 40, Loss: 0.0167
Batch 50, Loss: 0.0138
Batch 60, Loss: 0.0130
Batch 70, Loss: 0.0114
Batch 80, Loss: 0.0131
Batch 90, Loss: 0.0137
Batch 100, Loss: 0.0145
Batch 110, Loss: 0.0163
Batch 120, Loss: 0.0155
Batch 130, Loss: 0.0108
Batch 140, Loss: 0.0181
Batch 150, Loss: 0.0161
Batch 160, Loss: 0.0131
Batch 170, Loss: 0.0134
Batch 180, Loss: 0.0112
Batch 190, Loss: 0.0104
Batch 200, Loss: 0.0129
Batch 210, Loss: 0.0165
Batch 220, Loss: 0.0125
Batch 230, Loss: 0.0116
Batch 240, Loss: 0.0104
Batch 250, Loss: 0.0152
Batch 260, Loss: 0.0183
Batch 270, Loss: 0.0125
Batch 280, Loss: 0.0149
Batch 290, Loss: 0.0129
Batch 300, Loss: 0.0132
Batch 310, Loss: 0.0133
Batch 320, Loss: 0.0134
Batch 330, Loss: 0.0128
Batch 340, Loss: 0.0152
Batch 350, Loss: 0.0143
Batch 360, Loss: 0.0163
Batch 370, Loss: 0.0153
Batch 380, Loss: 0.0119
Batch 390, Loss: 0.0130
Epoch 48 learning rate: 0.00039426493427611173
Epoch 48 time: 236.24257731437683 seconds
Epoch 48 accuracy: 94.45%
Batch 10, Loss: 0.0136
Batch 20, Loss: 0.0181
Batch 30, Loss: 0.0117
Batch 40, Loss: 0.0155
Batch 50, Loss: 0.0125
Batch 60, Loss: 0.0141
Batch 70, Loss: 0.0098
Batch 80, Loss: 0.0162
Batch 90, Loss: 0.0109
Batch 100, Loss: 0.0165
Batch 110, Loss: 0.0158
Batch 120, Loss: 0.0151
Batch 130, Loss: 0.0138
Batch 140, Loss: 0.0150
Batch 150, Loss: 0.0133
Batch 160, Loss: 0.0143
Batch 170, Loss: 0.0125
Batch 180, Loss: 0.0123
Batch 190, Loss: 0.0131
Batch 200, Loss: 0.0128
Batch 210, Loss: 0.0143
Batch 220, Loss: 0.0151
Batch 230, Loss: 0.0129
Batch 240, Loss: 0.0114
Batch 250, Loss: 0.0130
Batch 260, Loss: 0.0132
Batch 270, Loss: 0.0108
Batch 280, Loss: 0.0100
Batch 290, Loss: 0.0103
Batch 300, Loss: 0.0117
Batch 310, Loss: 0.0111
Batch 320, Loss: 0.0120
Batch 330, Loss: 0.0154
Batch 340, Loss: 0.0161
Batch 350, Loss: 0.0111
Batch 360, Loss: 0.0113
Batch 370, Loss: 0.0137
Batch 380, Loss: 0.0157
Batch 390, Loss: 0.0144
Epoch 49 learning rate: 9.866357858642205e-05
Epoch 49 time: 236.28534054756165 seconds
Epoch 49 accuracy: 94.42%
Batch 10, Loss: 0.0164
Batch 20, Loss: 0.0137
Batch 30, Loss: 0.0165
Batch 40, Loss: 0.0109
Batch 50, Loss: 0.0132
Batch 60, Loss: 0.0121
Batch 70, Loss: 0.0116
Batch 80, Loss: 0.0136
Batch 90, Loss: 0.0110
Batch 100, Loss: 0.0136
Batch 110, Loss: 0.0088
Batch 120, Loss: 0.0093
Batch 130, Loss: 0.0103
Batch 140, Loss: 0.0110
Batch 150, Loss: 0.0107
Batch 160, Loss: 0.0153
Batch 170, Loss: 0.0135
Batch 180, Loss: 0.0147
Batch 190, Loss: 0.0104
Batch 200, Loss: 0.0149
Batch 210, Loss: 0.0102
Batch 220, Loss: 0.0113
Batch 230, Loss: 0.0133
Batch 240, Loss: 0.0124
Batch 250, Loss: 0.0135
Batch 260, Loss: 0.0151
Batch 270, Loss: 0.0119
Batch 280, Loss: 0.0111
Batch 290, Loss: 0.0125
Batch 300, Loss: 0.0126
Batch 310, Loss: 0.0105
Batch 320, Loss: 0.0170
Batch 330, Loss: 0.0112
Batch 340, Loss: 0.0112
Batch 350, Loss: 0.0150
Batch 360, Loss: 0.0118
Batch 370, Loss: 0.0094
Batch 380, Loss: 0.0117
Batch 390, Loss: 0.0177
Epoch 50 learning rate: 0.0
Epoch 50 time: 236.2854790687561 seconds
Epoch 50 accuracy: 94.42%
rho:  0.04 , alpha:  0.3
Total training time: 11823.175447940826 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
The top Hessian eigenvalue of this model is 85.2085
Norm of the Gradient: 1.2239471674e+00
