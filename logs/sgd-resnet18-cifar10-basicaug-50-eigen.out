The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:216: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/SGD/basicaug/lr-0.1/batchsize-128/2024-08-04-19:31:30
Batch 10, Loss: 2.7813
Batch 20, Loss: 2.0452
Batch 30, Loss: 1.7162
Batch 40, Loss: 1.5852
Batch 50, Loss: 1.5891
Batch 60, Loss: 1.5298
Batch 70, Loss: 1.5022
Batch 80, Loss: 1.4887
Batch 90, Loss: 1.4666
Batch 100, Loss: 1.4799
Batch 110, Loss: 1.4385
Batch 120, Loss: 1.4587
Batch 130, Loss: 1.4015
Batch 140, Loss: 1.3950
Batch 150, Loss: 1.3704
Batch 160, Loss: 1.3385
Batch 170, Loss: 1.3766
Batch 180, Loss: 1.3385
Batch 190, Loss: 1.3168
Batch 200, Loss: 1.3426
Batch 210, Loss: 1.2880
Batch 220, Loss: 1.3027
Batch 230, Loss: 1.2930
Batch 240, Loss: 1.2926
Batch 250, Loss: 1.2644
Batch 260, Loss: 1.2526
Batch 270, Loss: 1.2228
Batch 280, Loss: 1.2309
Batch 290, Loss: 1.1903
Batch 300, Loss: 1.2326
Batch 310, Loss: 1.2311
Batch 320, Loss: 1.2212
Batch 330, Loss: 1.1771
Batch 340, Loss: 1.2095
Batch 350, Loss: 1.2096
Batch 360, Loss: 1.2209
Batch 370, Loss: 1.1517
Batch 380, Loss: 1.1243
Batch 390, Loss: 1.1616
Epoch 1 learning rate: 0.09990133642141358
Epoch 1 time: 30.951081037521362 seconds
Epoch 1 accuracy: 41.09%
Batch 10, Loss: 1.1381
Batch 20, Loss: 1.1294
Batch 30, Loss: 1.1060
Batch 40, Loss: 1.1352
Batch 50, Loss: 1.1145
Batch 60, Loss: 1.1138
Batch 70, Loss: 1.0891
Batch 80, Loss: 1.1017
Batch 90, Loss: 1.1255
Batch 100, Loss: 1.0587
Batch 110, Loss: 1.1111
Batch 120, Loss: 1.0603
Batch 130, Loss: 1.0484
Batch 140, Loss: 1.0319
Batch 150, Loss: 1.0571
Batch 160, Loss: 1.0480
Batch 170, Loss: 1.0350
Batch 180, Loss: 1.0376
Batch 190, Loss: 0.9944
Batch 200, Loss: 1.0605
Batch 210, Loss: 1.0422
Batch 220, Loss: 1.0195
Batch 230, Loss: 1.0170
Batch 240, Loss: 1.0370
Batch 250, Loss: 0.9747
Batch 260, Loss: 0.9767
Batch 270, Loss: 1.0041
Batch 280, Loss: 1.0228
Batch 290, Loss: 0.9635
Batch 300, Loss: 0.9539
Batch 310, Loss: 0.9719
Batch 320, Loss: 0.9465
Batch 330, Loss: 0.9143
Batch 340, Loss: 0.9108
Batch 350, Loss: 0.9397
Batch 360, Loss: 0.8735
Batch 370, Loss: 0.8975
Batch 380, Loss: 0.9146
Batch 390, Loss: 0.8963
Epoch 2 learning rate: 0.0996057350657239
Epoch 2 time: 24.968130826950073 seconds
Epoch 2 accuracy: 56.36%
Batch 10, Loss: 0.8726
Batch 20, Loss: 0.8505
Batch 30, Loss: 0.8936
Batch 40, Loss: 0.8685
Batch 50, Loss: 0.8772
Batch 60, Loss: 0.8831
Batch 70, Loss: 0.8593
Batch 80, Loss: 0.8728
Batch 90, Loss: 0.8893
Batch 100, Loss: 0.8367
Batch 110, Loss: 0.8642
Batch 120, Loss: 0.8552
Batch 130, Loss: 0.8389
Batch 140, Loss: 0.8583
Batch 150, Loss: 0.8561
Batch 160, Loss: 0.8545
Batch 170, Loss: 0.8340
Batch 180, Loss: 0.8216
Batch 190, Loss: 0.8417
Batch 200, Loss: 0.8326
Batch 210, Loss: 0.8308
Batch 220, Loss: 0.7920
Batch 230, Loss: 0.8139
Batch 240, Loss: 0.7899
Batch 250, Loss: 0.7764
Batch 260, Loss: 0.7651
Batch 270, Loss: 0.7945
Batch 280, Loss: 0.7582
Batch 290, Loss: 0.8073
Batch 300, Loss: 0.7730
Batch 310, Loss: 0.7253
Batch 320, Loss: 0.8019
Batch 330, Loss: 0.7850
Batch 340, Loss: 0.7510
Batch 350, Loss: 0.7966
Batch 360, Loss: 0.7323
Batch 370, Loss: 0.7304
Batch 380, Loss: 0.7114
Batch 390, Loss: 0.7362
Epoch 3 learning rate: 0.09911436253643444
Epoch 3 time: 24.899861574172974 seconds
Epoch 3 accuracy: 58.8%
Batch 10, Loss: 0.7911
Batch 20, Loss: 0.7512
Batch 30, Loss: 0.7666
Batch 40, Loss: 0.7371
Batch 50, Loss: 0.7432
Batch 60, Loss: 0.7574
Batch 70, Loss: 0.7127
Batch 80, Loss: 0.7329
Batch 90, Loss: 0.7244
Batch 100, Loss: 0.7308
Batch 110, Loss: 0.7047
Batch 120, Loss: 0.7355
Batch 130, Loss: 0.7099
Batch 140, Loss: 0.7872
Batch 150, Loss: 0.6927
Batch 160, Loss: 0.6770
Batch 170, Loss: 0.6861
Batch 180, Loss: 0.6912
Batch 190, Loss: 0.6968
Batch 200, Loss: 0.7238
Batch 210, Loss: 0.7160
Batch 220, Loss: 0.7061
Batch 230, Loss: 0.6743
Batch 240, Loss: 0.6933
Batch 250, Loss: 0.6518
Batch 260, Loss: 0.6787
Batch 270, Loss: 0.6991
Batch 280, Loss: 0.6748
Batch 290, Loss: 0.6793
Batch 300, Loss: 0.6807
Batch 310, Loss: 0.6879
Batch 320, Loss: 0.6856
Batch 330, Loss: 0.6962
Batch 340, Loss: 0.6972
Batch 350, Loss: 0.6426
Batch 360, Loss: 0.6174
Batch 370, Loss: 0.6466
Batch 380, Loss: 0.6411
Batch 390, Loss: 0.6427
Epoch 4 learning rate: 0.09842915805643154
Epoch 4 time: 24.906134366989136 seconds
Epoch 4 accuracy: 67.71%
Batch 10, Loss: 0.6607
Batch 20, Loss: 0.6107
Batch 30, Loss: 0.6182
Batch 40, Loss: 0.6298
Batch 50, Loss: 0.6011
Batch 60, Loss: 0.6102
Batch 70, Loss: 0.6080
Batch 80, Loss: 0.5827
Batch 90, Loss: 0.5702
Batch 100, Loss: 0.6153
Batch 110, Loss: 0.5803
Batch 120, Loss: 0.6347
Batch 130, Loss: 0.5980
Batch 140, Loss: 0.6015
Batch 150, Loss: 0.5745
Batch 160, Loss: 0.6189
Batch 170, Loss: 0.6274
Batch 180, Loss: 0.6124
Batch 190, Loss: 0.5769
Batch 200, Loss: 0.6263
Batch 210, Loss: 0.5535
Batch 220, Loss: 0.6141
Batch 230, Loss: 0.5716
Batch 240, Loss: 0.5725
Batch 250, Loss: 0.6054
Batch 260, Loss: 0.6192
Batch 270, Loss: 0.6015
Batch 280, Loss: 0.6363
Batch 290, Loss: 0.5706
Batch 300, Loss: 0.6032
Batch 310, Loss: 0.5600
Batch 320, Loss: 0.5666
Batch 330, Loss: 0.5469
Batch 340, Loss: 0.5340
Batch 350, Loss: 0.5841
Batch 360, Loss: 0.5639
Batch 370, Loss: 0.5306
Batch 380, Loss: 0.5433
Batch 390, Loss: 0.5666
Epoch 5 learning rate: 0.09755282581475767
Epoch 5 time: 24.944720029830933 seconds
Epoch 5 accuracy: 71.8%
Batch 10, Loss: 0.5239
Batch 20, Loss: 0.5459
Batch 30, Loss: 0.5133
Batch 40, Loss: 0.5064
Batch 50, Loss: 0.5760
Batch 60, Loss: 0.5351
Batch 70, Loss: 0.5441
Batch 80, Loss: 0.5274
Batch 90, Loss: 0.4841
Batch 100, Loss: 0.5067
Batch 110, Loss: 0.5325
Batch 120, Loss: 0.5140
Batch 130, Loss: 0.4889
Batch 140, Loss: 0.4939
Batch 150, Loss: 0.5485
Batch 160, Loss: 0.5357
Batch 170, Loss: 0.4977
Batch 180, Loss: 0.5170
Batch 190, Loss: 0.5028
Batch 200, Loss: 0.5417
Batch 210, Loss: 0.5236
Batch 220, Loss: 0.5084
Batch 230, Loss: 0.5191
Batch 240, Loss: 0.5493
Batch 250, Loss: 0.5022
Batch 260, Loss: 0.5152
Batch 270, Loss: 0.4717
Batch 280, Loss: 0.4934
Batch 290, Loss: 0.4711
Batch 300, Loss: 0.4929
Batch 310, Loss: 0.5087
Batch 320, Loss: 0.5084
Batch 330, Loss: 0.4643
Batch 340, Loss: 0.4902
Batch 350, Loss: 0.4959
Batch 360, Loss: 0.4698
Batch 370, Loss: 0.4611
Batch 380, Loss: 0.4863
Batch 390, Loss: 0.4914
Epoch 6 learning rate: 0.09648882429441256
Epoch 6 time: 24.921167850494385 seconds
Epoch 6 accuracy: 76.16%
Batch 10, Loss: 0.4865
Batch 20, Loss: 0.4866
Batch 30, Loss: 0.4845
Batch 40, Loss: 0.4958
Batch 50, Loss: 0.4857
Batch 60, Loss: 0.4640
Batch 70, Loss: 0.4627
Batch 80, Loss: 0.4852
Batch 90, Loss: 0.4956
Batch 100, Loss: 0.4594
Batch 110, Loss: 0.4652
Batch 120, Loss: 0.4638
Batch 130, Loss: 0.4549
Batch 140, Loss: 0.4846
Batch 150, Loss: 0.4451
Batch 160, Loss: 0.4461
Batch 170, Loss: 0.4323
Batch 180, Loss: 0.4959
Batch 190, Loss: 0.4547
Batch 200, Loss: 0.4306
Batch 210, Loss: 0.4710
Batch 220, Loss: 0.4454
Batch 230, Loss: 0.4479
Batch 240, Loss: 0.4427
Batch 250, Loss: 0.4444
Batch 260, Loss: 0.4434
Batch 270, Loss: 0.4417
Batch 280, Loss: 0.4759
Batch 290, Loss: 0.4572
Batch 300, Loss: 0.4248
Batch 310, Loss: 0.4344
Batch 320, Loss: 0.4677
Batch 330, Loss: 0.4207
Batch 340, Loss: 0.4542
Batch 350, Loss: 0.4553
Batch 360, Loss: 0.4374
Batch 370, Loss: 0.4590
Batch 380, Loss: 0.4108
Batch 390, Loss: 0.4522
Epoch 7 learning rate: 0.09524135262330098
Epoch 7 time: 24.90893793106079 seconds
Epoch 7 accuracy: 77.07%
Batch 10, Loss: 0.4444
Batch 20, Loss: 0.4144
Batch 30, Loss: 0.4312
Batch 40, Loss: 0.4160
Batch 50, Loss: 0.3978
Batch 60, Loss: 0.3812
Batch 70, Loss: 0.4322
Batch 80, Loss: 0.4266
Batch 90, Loss: 0.4346
Batch 100, Loss: 0.4270
Batch 110, Loss: 0.4649
Batch 120, Loss: 0.3944
Batch 130, Loss: 0.4115
Batch 140, Loss: 0.4086
Batch 150, Loss: 0.3873
Batch 160, Loss: 0.4270
Batch 170, Loss: 0.4275
Batch 180, Loss: 0.4609
Batch 190, Loss: 0.3946
Batch 200, Loss: 0.3945
Batch 210, Loss: 0.4432
Batch 220, Loss: 0.4495
Batch 230, Loss: 0.4398
Batch 240, Loss: 0.4219
Batch 250, Loss: 0.4240
Batch 260, Loss: 0.4707
Batch 270, Loss: 0.4182
Batch 280, Loss: 0.4437
Batch 290, Loss: 0.4241
Batch 300, Loss: 0.3812
Batch 310, Loss: 0.4160
Batch 320, Loss: 0.4072
Batch 330, Loss: 0.4434
Batch 340, Loss: 0.4291
Batch 350, Loss: 0.4045
Batch 360, Loss: 0.4366
Batch 370, Loss: 0.3676
Batch 380, Loss: 0.4015
Batch 390, Loss: 0.4196
Epoch 8 learning rate: 0.09381533400219318
Epoch 8 time: 24.91828966140747 seconds
Epoch 8 accuracy: 75.42%
Batch 10, Loss: 0.3828
Batch 20, Loss: 0.3524
Batch 30, Loss: 0.3977
Batch 40, Loss: 0.3745
Batch 50, Loss: 0.4154
Batch 60, Loss: 0.3756
Batch 70, Loss: 0.3656
Batch 80, Loss: 0.4109
Batch 90, Loss: 0.4023
Batch 100, Loss: 0.3998
Batch 110, Loss: 0.4081
Batch 120, Loss: 0.4094
Batch 130, Loss: 0.3865
Batch 140, Loss: 0.3764
Batch 150, Loss: 0.4147
Batch 160, Loss: 0.4120
Batch 170, Loss: 0.4177
Batch 180, Loss: 0.3556
Batch 190, Loss: 0.3923
Batch 200, Loss: 0.4180
Batch 210, Loss: 0.3568
Batch 220, Loss: 0.3487
Batch 230, Loss: 0.3772
Batch 240, Loss: 0.4175
Batch 250, Loss: 0.4080
Batch 260, Loss: 0.3736
Batch 270, Loss: 0.3696
Batch 280, Loss: 0.4342
Batch 290, Loss: 0.3642
Batch 300, Loss: 0.3868
Batch 310, Loss: 0.4183
Batch 320, Loss: 0.3954
Batch 330, Loss: 0.3866
Batch 340, Loss: 0.3923
Batch 350, Loss: 0.4006
Batch 360, Loss: 0.3922
Batch 370, Loss: 0.4250
Batch 380, Loss: 0.3982
Batch 390, Loss: 0.4139
Epoch 9 learning rate: 0.09221639627510075
Epoch 9 time: 24.966856718063354 seconds
Epoch 9 accuracy: 77.57%
Batch 10, Loss: 0.3543
Batch 20, Loss: 0.3656
Batch 30, Loss: 0.3662
Batch 40, Loss: 0.3943
Batch 50, Loss: 0.3497
Batch 60, Loss: 0.3700
Batch 70, Loss: 0.3546
Batch 80, Loss: 0.3633
Batch 90, Loss: 0.3359
Batch 100, Loss: 0.3663
Batch 110, Loss: 0.3531
Batch 120, Loss: 0.4077
Batch 130, Loss: 0.3841
Batch 140, Loss: 0.4228
Batch 150, Loss: 0.3754
Batch 160, Loss: 0.3694
Batch 170, Loss: 0.3705
Batch 180, Loss: 0.3830
Batch 190, Loss: 0.3674
Batch 200, Loss: 0.3815
Batch 210, Loss: 0.4250
Batch 220, Loss: 0.3652
Batch 230, Loss: 0.3696
Batch 240, Loss: 0.3555
Batch 250, Loss: 0.3833
Batch 260, Loss: 0.3696
Batch 270, Loss: 0.3694
Batch 280, Loss: 0.3917
Batch 290, Loss: 0.3508
Batch 300, Loss: 0.3773
Batch 310, Loss: 0.3592
Batch 320, Loss: 0.4027
Batch 330, Loss: 0.3613
Batch 340, Loss: 0.4081
Batch 350, Loss: 0.3781
Batch 360, Loss: 0.3859
Batch 370, Loss: 0.3408
Batch 380, Loss: 0.3715
Batch 390, Loss: 0.3660
Epoch 10 learning rate: 0.09045084971874737
Epoch 10 time: 24.875112295150757 seconds
Epoch 10 accuracy: 78.24%
Batch 10, Loss: 0.3969
Batch 20, Loss: 0.3088
Batch 30, Loss: 0.3882
Batch 40, Loss: 0.3420
Batch 50, Loss: 0.4022
Batch 60, Loss: 0.3267
Batch 70, Loss: 0.3559
Batch 80, Loss: 0.3153
Batch 90, Loss: 0.3457
Batch 100, Loss: 0.3313
Batch 110, Loss: 0.3770
Batch 120, Loss: 0.3307
Batch 130, Loss: 0.3535
Batch 140, Loss: 0.3752
Batch 150, Loss: 0.3396
Batch 160, Loss: 0.3900
Batch 170, Loss: 0.3685
Batch 180, Loss: 0.3824
Batch 190, Loss: 0.3261
Batch 200, Loss: 0.3337
Batch 210, Loss: 0.3572
Batch 220, Loss: 0.3800
Batch 230, Loss: 0.3646
Batch 240, Loss: 0.3736
Batch 250, Loss: 0.3628
Batch 260, Loss: 0.3499
Batch 270, Loss: 0.3545
Batch 280, Loss: 0.3660
Batch 290, Loss: 0.3643
Batch 300, Loss: 0.3548
Batch 310, Loss: 0.3541
Batch 320, Loss: 0.3510
Batch 330, Loss: 0.3445
Batch 340, Loss: 0.3717
Batch 350, Loss: 0.3803
Batch 360, Loss: 0.3847
Batch 370, Loss: 0.3593
Batch 380, Loss: 0.3328
Batch 390, Loss: 0.3328
Epoch 11 learning rate: 0.08852566213878946
Epoch 11 time: 24.910998582839966 seconds
Epoch 11 accuracy: 59.41%
Batch 10, Loss: 0.3568
Batch 20, Loss: 0.3412
Batch 30, Loss: 0.3281
Batch 40, Loss: 0.3182
Batch 50, Loss: 0.3289
Batch 60, Loss: 0.3396
Batch 70, Loss: 0.3450
Batch 80, Loss: 0.3127
Batch 90, Loss: 0.3611
Batch 100, Loss: 0.3418
Batch 110, Loss: 0.3367
Batch 120, Loss: 0.3593
Batch 130, Loss: 0.3478
Batch 140, Loss: 0.3262
Batch 150, Loss: 0.3610
Batch 160, Loss: 0.3496
Batch 170, Loss: 0.3561
Batch 180, Loss: 0.3331
Batch 190, Loss: 0.3620
Batch 200, Loss: 0.3437
Batch 210, Loss: 0.3599
Batch 220, Loss: 0.3665
Batch 230, Loss: 0.3447
Batch 240, Loss: 0.3444
Batch 250, Loss: 0.3580
Batch 260, Loss: 0.3327
Batch 270, Loss: 0.3520
Batch 280, Loss: 0.3589
Batch 290, Loss: 0.3242
Batch 300, Loss: 0.3435
Batch 310, Loss: 0.3358
Batch 320, Loss: 0.3472
Batch 330, Loss: 0.3632
Batch 340, Loss: 0.3198
Batch 350, Loss: 0.3763
Batch 360, Loss: 0.3289
Batch 370, Loss: 0.3593
Batch 380, Loss: 0.3236
Batch 390, Loss: 0.3439
Epoch 12 learning rate: 0.08644843137107057
Epoch 12 time: 24.85053014755249 seconds
Epoch 12 accuracy: 82.04%
Batch 10, Loss: 0.3293
Batch 20, Loss: 0.3116
Batch 30, Loss: 0.3020
Batch 40, Loss: 0.3290
Batch 50, Loss: 0.3227
Batch 60, Loss: 0.3253
Batch 70, Loss: 0.3303
Batch 80, Loss: 0.3269
Batch 90, Loss: 0.3552
Batch 100, Loss: 0.3307
Batch 110, Loss: 0.3480
Batch 120, Loss: 0.3216
Batch 130, Loss: 0.2975
Batch 140, Loss: 0.3646
Batch 150, Loss: 0.3141
Batch 160, Loss: 0.3202
Batch 170, Loss: 0.3640
Batch 180, Loss: 0.3832
Batch 190, Loss: 0.3075
Batch 200, Loss: 0.3592
Batch 210, Loss: 0.3370
Batch 220, Loss: 0.3790
Batch 230, Loss: 0.3339
Batch 240, Loss: 0.2951
Batch 250, Loss: 0.3386
Batch 260, Loss: 0.3570
Batch 270, Loss: 0.3327
Batch 280, Loss: 0.3351
Batch 290, Loss: 0.3166
Batch 300, Loss: 0.3120
Batch 310, Loss: 0.3269
Batch 320, Loss: 0.3390
Batch 330, Loss: 0.3054
Batch 340, Loss: 0.3288
Batch 350, Loss: 0.3248
Batch 360, Loss: 0.3569
Batch 370, Loss: 0.3343
Batch 380, Loss: 0.3738
Batch 390, Loss: 0.3234
Epoch 13 learning rate: 0.08422735529643442
Epoch 13 time: 24.893415212631226 seconds
Epoch 13 accuracy: 81.65%
Batch 10, Loss: 0.3055
Batch 20, Loss: 0.3034
Batch 30, Loss: 0.3353
Batch 40, Loss: 0.3346
Batch 50, Loss: 0.3270
Batch 60, Loss: 0.2867
Batch 70, Loss: 0.3297
Batch 80, Loss: 0.2949
Batch 90, Loss: 0.3140
Batch 100, Loss: 0.2621
Batch 110, Loss: 0.2882
Batch 120, Loss: 0.3265
Batch 130, Loss: 0.3022
Batch 140, Loss: 0.3056
Batch 150, Loss: 0.3210
Batch 160, Loss: 0.3421
Batch 170, Loss: 0.3296
Batch 180, Loss: 0.3269
Batch 190, Loss: 0.2994
Batch 200, Loss: 0.3076
Batch 210, Loss: 0.3204
Batch 220, Loss: 0.3247
Batch 230, Loss: 0.3264
Batch 240, Loss: 0.3733
Batch 250, Loss: 0.3457
Batch 260, Loss: 0.3275
Batch 270, Loss: 0.3308
Batch 280, Loss: 0.3300
Batch 290, Loss: 0.3175
Batch 300, Loss: 0.2961
Batch 310, Loss: 0.2981
Batch 320, Loss: 0.2967
Batch 330, Loss: 0.3364
Batch 340, Loss: 0.3076
Batch 350, Loss: 0.3286
Batch 360, Loss: 0.3387
Batch 370, Loss: 0.3300
Batch 380, Loss: 0.3326
Batch 390, Loss: 0.3228
Epoch 14 learning rate: 0.08187119948743447
Epoch 14 time: 24.899442195892334 seconds
Epoch 14 accuracy: 81.43%
Batch 10, Loss: 0.3255
Batch 20, Loss: 0.3096
Batch 30, Loss: 0.2923
Batch 40, Loss: 0.3106
Batch 50, Loss: 0.3058
Batch 60, Loss: 0.3069
Batch 70, Loss: 0.3017
Batch 80, Loss: 0.2732
Batch 90, Loss: 0.3258
Batch 100, Loss: 0.2772
Batch 110, Loss: 0.3067
Batch 120, Loss: 0.3063
Batch 130, Loss: 0.3083
Batch 140, Loss: 0.3210
Batch 150, Loss: 0.3195
Batch 160, Loss: 0.3252
Batch 170, Loss: 0.3042
Batch 180, Loss: 0.3045
Batch 190, Loss: 0.2730
Batch 200, Loss: 0.3310
Batch 210, Loss: 0.3087
Batch 220, Loss: 0.3131
Batch 230, Loss: 0.3391
Batch 240, Loss: 0.2973
Batch 250, Loss: 0.2912
Batch 260, Loss: 0.2912
Batch 270, Loss: 0.3038
Batch 280, Loss: 0.3421
Batch 290, Loss: 0.3154
Batch 300, Loss: 0.3054
Batch 310, Loss: 0.3085
Batch 320, Loss: 0.2911
Batch 330, Loss: 0.2866
Batch 340, Loss: 0.2964
Batch 350, Loss: 0.3380
Batch 360, Loss: 0.3215
Batch 370, Loss: 0.3310
Batch 380, Loss: 0.3165
Batch 390, Loss: 0.2938
Epoch 15 learning rate: 0.07938926261462366
Epoch 15 time: 24.890634536743164 seconds
Epoch 15 accuracy: 83.12%
Batch 10, Loss: 0.2978
Batch 20, Loss: 0.2860
Batch 30, Loss: 0.2675
Batch 40, Loss: 0.2719
Batch 50, Loss: 0.2734
Batch 60, Loss: 0.2744
Batch 70, Loss: 0.2993
Batch 80, Loss: 0.2997
Batch 90, Loss: 0.3170
Batch 100, Loss: 0.2779
Batch 110, Loss: 0.3000
Batch 120, Loss: 0.2897
Batch 130, Loss: 0.3044
Batch 140, Loss: 0.3070
Batch 150, Loss: 0.2748
Batch 160, Loss: 0.3168
Batch 170, Loss: 0.3071
Batch 180, Loss: 0.3157
Batch 190, Loss: 0.2860
Batch 200, Loss: 0.3264
Batch 210, Loss: 0.3232
Batch 220, Loss: 0.3160
Batch 230, Loss: 0.2851
Batch 240, Loss: 0.2881
Batch 250, Loss: 0.3250
Batch 260, Loss: 0.3285
Batch 270, Loss: 0.2947
Batch 280, Loss: 0.2757
Batch 290, Loss: 0.3000
Batch 300, Loss: 0.2765
Batch 310, Loss: 0.2675
Batch 320, Loss: 0.2890
Batch 330, Loss: 0.2940
Batch 340, Loss: 0.3020
Batch 350, Loss: 0.2809
Batch 360, Loss: 0.3389
Batch 370, Loss: 0.3300
Batch 380, Loss: 0.2990
Batch 390, Loss: 0.3232
Epoch 16 learning rate: 0.07679133974894982
Epoch 16 time: 24.916393280029297 seconds
Epoch 16 accuracy: 83.46%
Batch 10, Loss: 0.2571
Batch 20, Loss: 0.2776
Batch 30, Loss: 0.2813
Batch 40, Loss: 0.3074
Batch 50, Loss: 0.2835
Batch 60, Loss: 0.2564
Batch 70, Loss: 0.2914
Batch 80, Loss: 0.2796
Batch 90, Loss: 0.2919
Batch 100, Loss: 0.3045
Batch 110, Loss: 0.2804
Batch 120, Loss: 0.2924
Batch 130, Loss: 0.3144
Batch 140, Loss: 0.2680
Batch 150, Loss: 0.2761
Batch 160, Loss: 0.2918
Batch 170, Loss: 0.2928
Batch 180, Loss: 0.2906
Batch 190, Loss: 0.2921
Batch 200, Loss: 0.2951
Batch 210, Loss: 0.3162
Batch 220, Loss: 0.2864
Batch 230, Loss: 0.3106
Batch 240, Loss: 0.2803
Batch 250, Loss: 0.3403
Batch 260, Loss: 0.2914
Batch 270, Loss: 0.2430
Batch 280, Loss: 0.2592
Batch 290, Loss: 0.3172
Batch 300, Loss: 0.3084
Batch 310, Loss: 0.2822
Batch 320, Loss: 0.2705
Batch 330, Loss: 0.3173
Batch 340, Loss: 0.2872
Batch 350, Loss: 0.3083
Batch 360, Loss: 0.2804
Batch 370, Loss: 0.3070
Batch 380, Loss: 0.3073
Batch 390, Loss: 0.2887
Epoch 17 learning rate: 0.07408768370508576
Epoch 17 time: 24.92951774597168 seconds
Epoch 17 accuracy: 82.98%
Batch 10, Loss: 0.2697
Batch 20, Loss: 0.2400
Batch 30, Loss: 0.2663
Batch 40, Loss: 0.2631
Batch 50, Loss: 0.2623
Batch 60, Loss: 0.2831
Batch 70, Loss: 0.2610
Batch 80, Loss: 0.2652
Batch 90, Loss: 0.2568
Batch 100, Loss: 0.2314
Batch 110, Loss: 0.3108
Batch 120, Loss: 0.2560
Batch 130, Loss: 0.2387
Batch 140, Loss: 0.2496
Batch 150, Loss: 0.2946
Batch 160, Loss: 0.2975
Batch 170, Loss: 0.2793
Batch 180, Loss: 0.3001
Batch 190, Loss: 0.3036
Batch 200, Loss: 0.2590
Batch 210, Loss: 0.2817
Batch 220, Loss: 0.2906
Batch 230, Loss: 0.2680
Batch 240, Loss: 0.2797
Batch 250, Loss: 0.2801
Batch 260, Loss: 0.2655
Batch 270, Loss: 0.2945
Batch 280, Loss: 0.3001
Batch 290, Loss: 0.2782
Batch 300, Loss: 0.2832
Batch 310, Loss: 0.2942
Batch 320, Loss: 0.2310
Batch 330, Loss: 0.2602
Batch 340, Loss: 0.2442
Batch 350, Loss: 0.2708
Batch 360, Loss: 0.2878
Batch 370, Loss: 0.2978
Batch 380, Loss: 0.3164
Batch 390, Loss: 0.3072
Epoch 18 learning rate: 0.07128896457825362
Epoch 18 time: 24.897377967834473 seconds
Epoch 18 accuracy: 85.96%
Batch 10, Loss: 0.2537
Batch 20, Loss: 0.2565
Batch 30, Loss: 0.2506
Batch 40, Loss: 0.2372
Batch 50, Loss: 0.2417
Batch 60, Loss: 0.2457
Batch 70, Loss: 0.2724
Batch 80, Loss: 0.2460
Batch 90, Loss: 0.2708
Batch 100, Loss: 0.2687
Batch 110, Loss: 0.2173
Batch 120, Loss: 0.2436
Batch 130, Loss: 0.2802
Batch 140, Loss: 0.2661
Batch 150, Loss: 0.2302
Batch 160, Loss: 0.2919
Batch 170, Loss: 0.2279
Batch 180, Loss: 0.2564
Batch 190, Loss: 0.2725
Batch 200, Loss: 0.2441
Batch 210, Loss: 0.2795
Batch 220, Loss: 0.2559
Batch 230, Loss: 0.3017
Batch 240, Loss: 0.2368
Batch 250, Loss: 0.2691
Batch 260, Loss: 0.2599
Batch 270, Loss: 0.2691
Batch 280, Loss: 0.2763
Batch 290, Loss: 0.2555
Batch 300, Loss: 0.2840
Batch 310, Loss: 0.2667
Batch 320, Loss: 0.2705
Batch 330, Loss: 0.2543
Batch 340, Loss: 0.2694
Batch 350, Loss: 0.2491
Batch 360, Loss: 0.2804
Batch 370, Loss: 0.2968
Batch 380, Loss: 0.2541
Batch 390, Loss: 0.2771
Epoch 19 learning rate: 0.06840622763423389
Epoch 19 time: 24.89049530029297 seconds
Epoch 19 accuracy: 82.95%
Batch 10, Loss: 0.2420
Batch 20, Loss: 0.2268
Batch 30, Loss: 0.2538
Batch 40, Loss: 0.2889
Batch 50, Loss: 0.2486
Batch 60, Loss: 0.2565
Batch 70, Loss: 0.2501
Batch 80, Loss: 0.2239
Batch 90, Loss: 0.2152
Batch 100, Loss: 0.2311
Batch 110, Loss: 0.2353
Batch 120, Loss: 0.2719
Batch 130, Loss: 0.2440
Batch 140, Loss: 0.2723
Batch 150, Loss: 0.2499
Batch 160, Loss: 0.2160
Batch 170, Loss: 0.2796
Batch 180, Loss: 0.2295
Batch 190, Loss: 0.2743
Batch 200, Loss: 0.2786
Batch 210, Loss: 0.2742
Batch 220, Loss: 0.2919
Batch 230, Loss: 0.2450
Batch 240, Loss: 0.2267
Batch 250, Loss: 0.2264
Batch 260, Loss: 0.2531
Batch 270, Loss: 0.2630
Batch 280, Loss: 0.2980
Batch 290, Loss: 0.2780
Batch 300, Loss: 0.2743
Batch 310, Loss: 0.2966
Batch 320, Loss: 0.2562
Batch 330, Loss: 0.2449
Batch 340, Loss: 0.2234
Batch 350, Loss: 0.2609
Batch 360, Loss: 0.2508
Batch 370, Loss: 0.2750
Batch 380, Loss: 0.2686
Batch 390, Loss: 0.3018
Epoch 20 learning rate: 0.06545084971874736
Epoch 20 time: 24.929786443710327 seconds
Epoch 20 accuracy: 83.01%
Batch 10, Loss: 0.2665
Batch 20, Loss: 0.2261
Batch 30, Loss: 0.2419
Batch 40, Loss: 0.2660
Batch 50, Loss: 0.2382
Batch 60, Loss: 0.2172
Batch 70, Loss: 0.2458
Batch 80, Loss: 0.2210
Batch 90, Loss: 0.2356
Batch 100, Loss: 0.2390
Batch 110, Loss: 0.2186
Batch 120, Loss: 0.2524
Batch 130, Loss: 0.2358
Batch 140, Loss: 0.2421
Batch 150, Loss: 0.2657
Batch 160, Loss: 0.2762
Batch 170, Loss: 0.2457
Batch 180, Loss: 0.2376
Batch 190, Loss: 0.2228
Batch 200, Loss: 0.2165
Batch 210, Loss: 0.2445
Batch 220, Loss: 0.2675
Batch 230, Loss: 0.2611
Batch 240, Loss: 0.2312
Batch 250, Loss: 0.2235
Batch 260, Loss: 0.2363
Batch 270, Loss: 0.2386
Batch 280, Loss: 0.2405
Batch 290, Loss: 0.2415
Batch 300, Loss: 0.2409
Batch 310, Loss: 0.2396
Batch 320, Loss: 0.2614
Batch 330, Loss: 0.2587
Batch 340, Loss: 0.2677
Batch 350, Loss: 0.2302
Batch 360, Loss: 0.2402
Batch 370, Loss: 0.2201
Batch 380, Loss: 0.2370
Batch 390, Loss: 0.2472
Epoch 21 learning rate: 0.06243449435824272
Epoch 21 time: 24.867815494537354 seconds
Epoch 21 accuracy: 86.12%
Batch 10, Loss: 0.2304
Batch 20, Loss: 0.2255
Batch 30, Loss: 0.2355
Batch 40, Loss: 0.2028
Batch 50, Loss: 0.2421
Batch 60, Loss: 0.2439
Batch 70, Loss: 0.2338
Batch 80, Loss: 0.2295
Batch 90, Loss: 0.2355
Batch 100, Loss: 0.2467
Batch 110, Loss: 0.2213
Batch 120, Loss: 0.2541
Batch 130, Loss: 0.2616
Batch 140, Loss: 0.2282
Batch 150, Loss: 0.2372
Batch 160, Loss: 0.2251
Batch 170, Loss: 0.2083
Batch 180, Loss: 0.2127
Batch 190, Loss: 0.2504
Batch 200, Loss: 0.2288
Batch 210, Loss: 0.2366
Batch 220, Loss: 0.2434
Batch 230, Loss: 0.2486
Batch 240, Loss: 0.2600
Batch 250, Loss: 0.2523
Batch 260, Loss: 0.2495
Batch 270, Loss: 0.2310
Batch 280, Loss: 0.2286
Batch 290, Loss: 0.2455
Batch 300, Loss: 0.2049
Batch 310, Loss: 0.2434
Batch 320, Loss: 0.2541
Batch 330, Loss: 0.2357
Batch 340, Loss: 0.2203
Batch 350, Loss: 0.2376
Batch 360, Loss: 0.2566
Batch 370, Loss: 0.2803
Batch 380, Loss: 0.2355
Batch 390, Loss: 0.2783
Epoch 22 learning rate: 0.05936906572928623
Epoch 22 time: 24.870697498321533 seconds
Epoch 22 accuracy: 85.56%
Batch 10, Loss: 0.1942
Batch 20, Loss: 0.2350
Batch 30, Loss: 0.2084
Batch 40, Loss: 0.1914
Batch 50, Loss: 0.2201
Batch 60, Loss: 0.2030
Batch 70, Loss: 0.2000
Batch 80, Loss: 0.2299
Batch 90, Loss: 0.2390
Batch 100, Loss: 0.2114
Batch 110, Loss: 0.2127
Batch 120, Loss: 0.1751
Batch 130, Loss: 0.2354
Batch 140, Loss: 0.2304
Batch 150, Loss: 0.2349
Batch 160, Loss: 0.2601
Batch 170, Loss: 0.2165
Batch 180, Loss: 0.2158
Batch 190, Loss: 0.2025
Batch 200, Loss: 0.2240
Batch 210, Loss: 0.2350
Batch 220, Loss: 0.1973
Batch 230, Loss: 0.2217
Batch 240, Loss: 0.2267
Batch 250, Loss: 0.2436
Batch 260, Loss: 0.2228
Batch 270, Loss: 0.2235
Batch 280, Loss: 0.2476
Batch 290, Loss: 0.2445
Batch 300, Loss: 0.2290
Batch 310, Loss: 0.2219
Batch 320, Loss: 0.2338
Batch 330, Loss: 0.2289
Batch 340, Loss: 0.2407
Batch 350, Loss: 0.2555
Batch 360, Loss: 0.2276
Batch 370, Loss: 0.2527
Batch 380, Loss: 0.2464
Batch 390, Loss: 0.2081
Epoch 23 learning rate: 0.056266661678215216
Epoch 23 time: 24.939048051834106 seconds
Epoch 23 accuracy: 86.07%
Batch 10, Loss: 0.2117
Batch 20, Loss: 0.2026
Batch 30, Loss: 0.2145
Batch 40, Loss: 0.2006
Batch 50, Loss: 0.2006
Batch 60, Loss: 0.1892
Batch 70, Loss: 0.2044
Batch 80, Loss: 0.2142
Batch 90, Loss: 0.2132
Batch 100, Loss: 0.2616
Batch 110, Loss: 0.2267
Batch 120, Loss: 0.2057
Batch 130, Loss: 0.1928
Batch 140, Loss: 0.1991
Batch 150, Loss: 0.1986
Batch 160, Loss: 0.2063
Batch 170, Loss: 0.2040
Batch 180, Loss: 0.2136
Batch 190, Loss: 0.2145
Batch 200, Loss: 0.2065
Batch 210, Loss: 0.1993
Batch 220, Loss: 0.2013
Batch 230, Loss: 0.1803
Batch 240, Loss: 0.2045
Batch 250, Loss: 0.2409
Batch 260, Loss: 0.2308
Batch 270, Loss: 0.2136
Batch 280, Loss: 0.2007
Batch 290, Loss: 0.2005
Batch 300, Loss: 0.1798
Batch 310, Loss: 0.2122
Batch 320, Loss: 0.2189
Batch 330, Loss: 0.1909
Batch 340, Loss: 0.2225
Batch 350, Loss: 0.2343
Batch 360, Loss: 0.2327
Batch 370, Loss: 0.2140
Batch 380, Loss: 0.2442
Batch 390, Loss: 0.2161
Epoch 24 learning rate: 0.053139525976465665
Epoch 24 time: 24.967766761779785 seconds
Epoch 24 accuracy: 83.3%
Batch 10, Loss: 0.2178
Batch 20, Loss: 0.2038
Batch 30, Loss: 0.1956
Batch 40, Loss: 0.1845
Batch 50, Loss: 0.1983
Batch 60, Loss: 0.2120
Batch 70, Loss: 0.2030
Batch 80, Loss: 0.2129
Batch 90, Loss: 0.2115
Batch 100, Loss: 0.2058
Batch 110, Loss: 0.1947
Batch 120, Loss: 0.2096
Batch 130, Loss: 0.2129
Batch 140, Loss: 0.1977
Batch 150, Loss: 0.2031
Batch 160, Loss: 0.2058
Batch 170, Loss: 0.1934
Batch 180, Loss: 0.1910
Batch 190, Loss: 0.1996
Batch 200, Loss: 0.2046
Batch 210, Loss: 0.1978
Batch 220, Loss: 0.2321
Batch 230, Loss: 0.2136
Batch 240, Loss: 0.2170
Batch 250, Loss: 0.2258
Batch 260, Loss: 0.2348
Batch 270, Loss: 0.2130
Batch 280, Loss: 0.2134
Batch 290, Loss: 0.2022
Batch 300, Loss: 0.2360
Batch 310, Loss: 0.2136
Batch 320, Loss: 0.1964
Batch 330, Loss: 0.2214
Batch 340, Loss: 0.2149
Batch 350, Loss: 0.1893
Batch 360, Loss: 0.1916
Batch 370, Loss: 0.1928
Batch 380, Loss: 0.1767
Batch 390, Loss: 0.2066
Epoch 25 learning rate: 0.049999999999999996
Epoch 25 time: 24.86658215522766 seconds
Epoch 25 accuracy: 85.36%
Batch 10, Loss: 0.1885
Batch 20, Loss: 0.1920
Batch 30, Loss: 0.1708
Batch 40, Loss: 0.1584
Batch 50, Loss: 0.2119
Batch 60, Loss: 0.1777
Batch 70, Loss: 0.1778
Batch 80, Loss: 0.1757
Batch 90, Loss: 0.1912
Batch 100, Loss: 0.1711
Batch 110, Loss: 0.1939
Batch 120, Loss: 0.1846
Batch 130, Loss: 0.1723
Batch 140, Loss: 0.1964
Batch 150, Loss: 0.1910
Batch 160, Loss: 0.2017
Batch 170, Loss: 0.1890
Batch 180, Loss: 0.1912
Batch 190, Loss: 0.1783
Batch 200, Loss: 0.2021
Batch 210, Loss: 0.1859
Batch 220, Loss: 0.2157
Batch 230, Loss: 0.1851
Batch 240, Loss: 0.1889
Batch 250, Loss: 0.2053
Batch 260, Loss: 0.1962
Batch 270, Loss: 0.2033
Batch 280, Loss: 0.1916
Batch 290, Loss: 0.1919
Batch 300, Loss: 0.1744
Batch 310, Loss: 0.2009
Batch 320, Loss: 0.2012
Batch 330, Loss: 0.2059
Batch 340, Loss: 0.2147
Batch 350, Loss: 0.2048
Batch 360, Loss: 0.1928
Batch 370, Loss: 0.1858
Batch 380, Loss: 0.2528
Batch 390, Loss: 0.1823
Epoch 26 learning rate: 0.046860474023534326
Epoch 26 time: 24.876306772232056 seconds
Epoch 26 accuracy: 87.32%
Batch 10, Loss: 0.1561
Batch 20, Loss: 0.1648
Batch 30, Loss: 0.1876
Batch 40, Loss: 0.1607
Batch 50, Loss: 0.1718
Batch 60, Loss: 0.1918
Batch 70, Loss: 0.1836
Batch 80, Loss: 0.1881
Batch 90, Loss: 0.2060
Batch 100, Loss: 0.1608
Batch 110, Loss: 0.1559
Batch 120, Loss: 0.1900
Batch 130, Loss: 0.1788
Batch 140, Loss: 0.2055
Batch 150, Loss: 0.1860
Batch 160, Loss: 0.1754
Batch 170, Loss: 0.1772
Batch 180, Loss: 0.1645
Batch 190, Loss: 0.1991
Batch 200, Loss: 0.1832
Batch 210, Loss: 0.1730
Batch 220, Loss: 0.1846
Batch 230, Loss: 0.1676
Batch 240, Loss: 0.1983
Batch 250, Loss: 0.1697
Batch 260, Loss: 0.1841
Batch 270, Loss: 0.1876
Batch 280, Loss: 0.2105
Batch 290, Loss: 0.2051
Batch 300, Loss: 0.1915
Batch 310, Loss: 0.1744
Batch 320, Loss: 0.1764
Batch 330, Loss: 0.1957
Batch 340, Loss: 0.2026
Batch 350, Loss: 0.1949
Batch 360, Loss: 0.1906
Batch 370, Loss: 0.1943
Batch 380, Loss: 0.2080
Batch 390, Loss: 0.1830
Epoch 27 learning rate: 0.04373333832178478
Epoch 27 time: 24.8926739692688 seconds
Epoch 27 accuracy: 89.41%
Batch 10, Loss: 0.1639
Batch 20, Loss: 0.1664
Batch 30, Loss: 0.1452
Batch 40, Loss: 0.1748
Batch 50, Loss: 0.1621
Batch 60, Loss: 0.1665
Batch 70, Loss: 0.1873
Batch 80, Loss: 0.1586
Batch 90, Loss: 0.1766
Batch 100, Loss: 0.1837
Batch 110, Loss: 0.1934
Batch 120, Loss: 0.1765
Batch 130, Loss: 0.1574
Batch 140, Loss: 0.1625
Batch 150, Loss: 0.1709
Batch 160, Loss: 0.1801
Batch 170, Loss: 0.1779
Batch 180, Loss: 0.1760
Batch 190, Loss: 0.1763
Batch 200, Loss: 0.1651
Batch 210, Loss: 0.1574
Batch 220, Loss: 0.1665
Batch 230, Loss: 0.1608
Batch 240, Loss: 0.1752
Batch 250, Loss: 0.1848
Batch 260, Loss: 0.1762
Batch 270, Loss: 0.1806
Batch 280, Loss: 0.1532
Batch 290, Loss: 0.2186
Batch 300, Loss: 0.1649
Batch 310, Loss: 0.1698
Batch 320, Loss: 0.1713
Batch 330, Loss: 0.1758
Batch 340, Loss: 0.1633
Batch 350, Loss: 0.1915
Batch 360, Loss: 0.1813
Batch 370, Loss: 0.1829
Batch 380, Loss: 0.1986
Batch 390, Loss: 0.1780
Epoch 28 learning rate: 0.040630934270713764
Epoch 28 time: 24.92272114753723 seconds
Epoch 28 accuracy: 87.68%
Batch 10, Loss: 0.1523
Batch 20, Loss: 0.1591
Batch 30, Loss: 0.1488
Batch 40, Loss: 0.1624
Batch 50, Loss: 0.1446
Batch 60, Loss: 0.1402
Batch 70, Loss: 0.1433
Batch 80, Loss: 0.1750
Batch 90, Loss: 0.1616
Batch 100, Loss: 0.1643
Batch 110, Loss: 0.1767
Batch 120, Loss: 0.1476
Batch 130, Loss: 0.1601
Batch 140, Loss: 0.1609
Batch 150, Loss: 0.1534
Batch 160, Loss: 0.1755
Batch 170, Loss: 0.1619
Batch 180, Loss: 0.1641
Batch 190, Loss: 0.1600
Batch 200, Loss: 0.1620
Batch 210, Loss: 0.1609
Batch 220, Loss: 0.1557
Batch 230, Loss: 0.1728
Batch 240, Loss: 0.1727
Batch 250, Loss: 0.1726
Batch 260, Loss: 0.1542
Batch 270, Loss: 0.1891
Batch 280, Loss: 0.1769
Batch 290, Loss: 0.1605
Batch 300, Loss: 0.1668
Batch 310, Loss: 0.1553
Batch 320, Loss: 0.1675
Batch 330, Loss: 0.1667
Batch 340, Loss: 0.1712
Batch 350, Loss: 0.1680
Batch 360, Loss: 0.1676
Batch 370, Loss: 0.2010
Batch 380, Loss: 0.1590
Batch 390, Loss: 0.1685
Epoch 29 learning rate: 0.03756550564175726
Epoch 29 time: 24.915722131729126 seconds
Epoch 29 accuracy: 87.9%
Batch 10, Loss: 0.1511
Batch 20, Loss: 0.1620
Batch 30, Loss: 0.1232
Batch 40, Loss: 0.1463
Batch 50, Loss: 0.1579
Batch 60, Loss: 0.1423
Batch 70, Loss: 0.1372
Batch 80, Loss: 0.1629
Batch 90, Loss: 0.1375
Batch 100, Loss: 0.1521
Batch 110, Loss: 0.1527
Batch 120, Loss: 0.1393
Batch 130, Loss: 0.1458
Batch 140, Loss: 0.1458
Batch 150, Loss: 0.1149
Batch 160, Loss: 0.1467
Batch 170, Loss: 0.1573
Batch 180, Loss: 0.1575
Batch 190, Loss: 0.1592
Batch 200, Loss: 0.1513
Batch 210, Loss: 0.1502
Batch 220, Loss: 0.1610
Batch 230, Loss: 0.1524
Batch 240, Loss: 0.1586
Batch 250, Loss: 0.1702
Batch 260, Loss: 0.1695
Batch 270, Loss: 0.1450
Batch 280, Loss: 0.1891
Batch 290, Loss: 0.1340
Batch 300, Loss: 0.1455
Batch 310, Loss: 0.1622
Batch 320, Loss: 0.1703
Batch 330, Loss: 0.1604
Batch 340, Loss: 0.1598
Batch 350, Loss: 0.1870
Batch 360, Loss: 0.1547
Batch 370, Loss: 0.1629
Batch 380, Loss: 0.1645
Batch 390, Loss: 0.1835
Epoch 30 learning rate: 0.03454915028125265
Epoch 30 time: 24.882968187332153 seconds
Epoch 30 accuracy: 88.46%
Batch 10, Loss: 0.1559
Batch 20, Loss: 0.1247
Batch 30, Loss: 0.1526
Batch 40, Loss: 0.1374
Batch 50, Loss: 0.1228
Batch 60, Loss: 0.1471
Batch 70, Loss: 0.1247
Batch 80, Loss: 0.1074
Batch 90, Loss: 0.1329
Batch 100, Loss: 0.1245
Batch 110, Loss: 0.1443
Batch 120, Loss: 0.1203
Batch 130, Loss: 0.1330
Batch 140, Loss: 0.1483
Batch 150, Loss: 0.1613
Batch 160, Loss: 0.1424
Batch 170, Loss: 0.1447
Batch 180, Loss: 0.1454
Batch 190, Loss: 0.1434
Batch 200, Loss: 0.1298
Batch 210, Loss: 0.1607
Batch 220, Loss: 0.1462
Batch 230, Loss: 0.1243
Batch 240, Loss: 0.1277
Batch 250, Loss: 0.1591
Batch 260, Loss: 0.1288
Batch 270, Loss: 0.1573
Batch 280, Loss: 0.1353
Batch 290, Loss: 0.1628
Batch 300, Loss: 0.1409
Batch 310, Loss: 0.1500
Batch 320, Loss: 0.1460
Batch 330, Loss: 0.1644
Batch 340, Loss: 0.1499
Batch 350, Loss: 0.1389
Batch 360, Loss: 0.1576
Batch 370, Loss: 0.1521
Batch 380, Loss: 0.1774
Batch 390, Loss: 0.1357
Epoch 31 learning rate: 0.03159377236576612
Epoch 31 time: 24.906801223754883 seconds
Epoch 31 accuracy: 88.2%
Batch 10, Loss: 0.1404
Batch 20, Loss: 0.1366
Batch 30, Loss: 0.1474
Batch 40, Loss: 0.1316
Batch 50, Loss: 0.1105
Batch 60, Loss: 0.1270
Batch 70, Loss: 0.1205
Batch 80, Loss: 0.1184
Batch 90, Loss: 0.1497
Batch 100, Loss: 0.1416
Batch 110, Loss: 0.1346
Batch 120, Loss: 0.1377
Batch 130, Loss: 0.1017
Batch 140, Loss: 0.1237
Batch 150, Loss: 0.1251
Batch 160, Loss: 0.1269
Batch 170, Loss: 0.1125
Batch 180, Loss: 0.1364
Batch 190, Loss: 0.1091
Batch 200, Loss: 0.1222
Batch 210, Loss: 0.1226
Batch 220, Loss: 0.1190
Batch 230, Loss: 0.1293
Batch 240, Loss: 0.1410
Batch 250, Loss: 0.1429
Batch 260, Loss: 0.1433
Batch 270, Loss: 0.1407
Batch 280, Loss: 0.1262
Batch 290, Loss: 0.1392
Batch 300, Loss: 0.1325
Batch 310, Loss: 0.1436
Batch 320, Loss: 0.1438
Batch 330, Loss: 0.1349
Batch 340, Loss: 0.1349
Batch 350, Loss: 0.1284
Batch 360, Loss: 0.1390
Batch 370, Loss: 0.1385
Batch 380, Loss: 0.1431
Batch 390, Loss: 0.1562
Epoch 32 learning rate: 0.028711035421746366
Epoch 32 time: 24.902544498443604 seconds
Epoch 32 accuracy: 89.95%
Batch 10, Loss: 0.1248
Batch 20, Loss: 0.1201
Batch 30, Loss: 0.0951
Batch 40, Loss: 0.0963
Batch 50, Loss: 0.1038
Batch 60, Loss: 0.1109
Batch 70, Loss: 0.1036
Batch 80, Loss: 0.1040
Batch 90, Loss: 0.0984
Batch 100, Loss: 0.0917
Batch 110, Loss: 0.0960
Batch 120, Loss: 0.1125
Batch 130, Loss: 0.1230
Batch 140, Loss: 0.1275
Batch 150, Loss: 0.1065
Batch 160, Loss: 0.1300
Batch 170, Loss: 0.1245
Batch 180, Loss: 0.1109
Batch 190, Loss: 0.1292
Batch 200, Loss: 0.1103
Batch 210, Loss: 0.1131
Batch 220, Loss: 0.1328
Batch 230, Loss: 0.1540
Batch 240, Loss: 0.1395
Batch 250, Loss: 0.1279
Batch 260, Loss: 0.1206
Batch 270, Loss: 0.1293
Batch 280, Loss: 0.1469
Batch 290, Loss: 0.1254
Batch 300, Loss: 0.1358
Batch 310, Loss: 0.1220
Batch 320, Loss: 0.1280
Batch 330, Loss: 0.1451
Batch 340, Loss: 0.1362
Batch 350, Loss: 0.1201
Batch 360, Loss: 0.1357
Batch 370, Loss: 0.1264
Batch 380, Loss: 0.1576
Batch 390, Loss: 0.1628
Epoch 33 learning rate: 0.025912316294914226
Epoch 33 time: 24.878561973571777 seconds
Epoch 33 accuracy: 88.64%
Batch 10, Loss: 0.1288
Batch 20, Loss: 0.0891
Batch 30, Loss: 0.0910
Batch 40, Loss: 0.1129
Batch 50, Loss: 0.0978
Batch 60, Loss: 0.0855
Batch 70, Loss: 0.1134
Batch 80, Loss: 0.1029
Batch 90, Loss: 0.1164
Batch 100, Loss: 0.0939
Batch 110, Loss: 0.0999
Batch 120, Loss: 0.1062
Batch 130, Loss: 0.1134
Batch 140, Loss: 0.1175
Batch 150, Loss: 0.1042
Batch 160, Loss: 0.0975
Batch 170, Loss: 0.0904
Batch 180, Loss: 0.0937
Batch 190, Loss: 0.0888
Batch 200, Loss: 0.1248
Batch 210, Loss: 0.1111
Batch 220, Loss: 0.0828
Batch 230, Loss: 0.1083
Batch 240, Loss: 0.1004
Batch 250, Loss: 0.1103
Batch 260, Loss: 0.1268
Batch 270, Loss: 0.1154
Batch 280, Loss: 0.1170
Batch 290, Loss: 0.1018
Batch 300, Loss: 0.1139
Batch 310, Loss: 0.1237
Batch 320, Loss: 0.1181
Batch 330, Loss: 0.1186
Batch 340, Loss: 0.1322
Batch 350, Loss: 0.1096
Batch 360, Loss: 0.1158
Batch 370, Loss: 0.1013
Batch 380, Loss: 0.1104
Batch 390, Loss: 0.0996
Epoch 34 learning rate: 0.023208660251050155
Epoch 34 time: 24.89159607887268 seconds
Epoch 34 accuracy: 90.66%
Batch 10, Loss: 0.0994
Batch 20, Loss: 0.0899
Batch 30, Loss: 0.0895
Batch 40, Loss: 0.1048
Batch 50, Loss: 0.0955
Batch 60, Loss: 0.0993
Batch 70, Loss: 0.0905
Batch 80, Loss: 0.0741
Batch 90, Loss: 0.0794
Batch 100, Loss: 0.0945
Batch 110, Loss: 0.0990
Batch 120, Loss: 0.0881
Batch 130, Loss: 0.0708
Batch 140, Loss: 0.1056
Batch 150, Loss: 0.0784
Batch 160, Loss: 0.1044
Batch 170, Loss: 0.1007
Batch 180, Loss: 0.1016
Batch 190, Loss: 0.1033
Batch 200, Loss: 0.0878
Batch 210, Loss: 0.0838
Batch 220, Loss: 0.0983
Batch 230, Loss: 0.1018
Batch 240, Loss: 0.1059
Batch 250, Loss: 0.1051
Batch 260, Loss: 0.0977
Batch 270, Loss: 0.1024
Batch 280, Loss: 0.1081
Batch 290, Loss: 0.0992
Batch 300, Loss: 0.1080
Batch 310, Loss: 0.0990
Batch 320, Loss: 0.1153
Batch 330, Loss: 0.1003
Batch 340, Loss: 0.0937
Batch 350, Loss: 0.1098
Batch 360, Loss: 0.1031
Batch 370, Loss: 0.0952
Batch 380, Loss: 0.1022
Batch 390, Loss: 0.1068
Epoch 35 learning rate: 0.020610737385376346
Epoch 35 time: 24.904430627822876 seconds
Epoch 35 accuracy: 90.6%
Batch 10, Loss: 0.1136
Batch 20, Loss: 0.0643
Batch 30, Loss: 0.0762
Batch 40, Loss: 0.0688
Batch 50, Loss: 0.0809
Batch 60, Loss: 0.0727
Batch 70, Loss: 0.0836
Batch 80, Loss: 0.0763
Batch 90, Loss: 0.0732
Batch 100, Loss: 0.0793
Batch 110, Loss: 0.0780
Batch 120, Loss: 0.0736
Batch 130, Loss: 0.0799
Batch 140, Loss: 0.0841
Batch 150, Loss: 0.0906
Batch 160, Loss: 0.0792
Batch 170, Loss: 0.0854
Batch 180, Loss: 0.0820
Batch 190, Loss: 0.0885
Batch 200, Loss: 0.0772
Batch 210, Loss: 0.0729
Batch 220, Loss: 0.0838
Batch 230, Loss: 0.0909
Batch 240, Loss: 0.0868
Batch 250, Loss: 0.0829
Batch 260, Loss: 0.0952
Batch 270, Loss: 0.0786
Batch 280, Loss: 0.0696
Batch 290, Loss: 0.0973
Batch 300, Loss: 0.0870
Batch 310, Loss: 0.0960
Batch 320, Loss: 0.1017
Batch 330, Loss: 0.1024
Batch 340, Loss: 0.0937
Batch 350, Loss: 0.0937
Batch 360, Loss: 0.0933
Batch 370, Loss: 0.1047
Batch 380, Loss: 0.0995
Batch 390, Loss: 0.0791
Epoch 36 learning rate: 0.01812880051256551
Epoch 36 time: 24.874428749084473 seconds
Epoch 36 accuracy: 91.55%
Batch 10, Loss: 0.0630
Batch 20, Loss: 0.0760
Batch 30, Loss: 0.0841
Batch 40, Loss: 0.0673
Batch 50, Loss: 0.0643
Batch 60, Loss: 0.0543
Batch 70, Loss: 0.0759
Batch 80, Loss: 0.0671
Batch 90, Loss: 0.0753
Batch 100, Loss: 0.0529
Batch 110, Loss: 0.0710
Batch 120, Loss: 0.0681
Batch 130, Loss: 0.0753
Batch 140, Loss: 0.0743
Batch 150, Loss: 0.0708
Batch 160, Loss: 0.0744
Batch 170, Loss: 0.0771
Batch 180, Loss: 0.0783
Batch 190, Loss: 0.0695
Batch 200, Loss: 0.0863
Batch 210, Loss: 0.0654
Batch 220, Loss: 0.0723
Batch 230, Loss: 0.0665
Batch 240, Loss: 0.0713
Batch 250, Loss: 0.0675
Batch 260, Loss: 0.0661
Batch 270, Loss: 0.0739
Batch 280, Loss: 0.0732
Batch 290, Loss: 0.0732
Batch 300, Loss: 0.0675
Batch 310, Loss: 0.0796
Batch 320, Loss: 0.0889
Batch 330, Loss: 0.0855
Batch 340, Loss: 0.0842
Batch 350, Loss: 0.0799
Batch 360, Loss: 0.0727
Batch 370, Loss: 0.0935
Batch 380, Loss: 0.0690
Batch 390, Loss: 0.0899
Epoch 37 learning rate: 0.015772644703565562
Epoch 37 time: 24.91407084465027 seconds
Epoch 37 accuracy: 91.71%
Batch 10, Loss: 0.0692
Batch 20, Loss: 0.0704
Batch 30, Loss: 0.0627
Batch 40, Loss: 0.0468
Batch 50, Loss: 0.0613
Batch 60, Loss: 0.0524
Batch 70, Loss: 0.0569
Batch 80, Loss: 0.0585
Batch 90, Loss: 0.0618
Batch 100, Loss: 0.0523
Batch 110, Loss: 0.0718
Batch 120, Loss: 0.0492
Batch 130, Loss: 0.0536
Batch 140, Loss: 0.0664
Batch 150, Loss: 0.0718
Batch 160, Loss: 0.0600
Batch 170, Loss: 0.0629
Batch 180, Loss: 0.0562
Batch 190, Loss: 0.0599
Batch 200, Loss: 0.0579
Batch 210, Loss: 0.0775
Batch 220, Loss: 0.0534
Batch 230, Loss: 0.0621
Batch 240, Loss: 0.0457
Batch 250, Loss: 0.0587
Batch 260, Loss: 0.0524
Batch 270, Loss: 0.0613
Batch 280, Loss: 0.0611
Batch 290, Loss: 0.0599
Batch 300, Loss: 0.0537
Batch 310, Loss: 0.0433
Batch 320, Loss: 0.0724
Batch 330, Loss: 0.0817
Batch 340, Loss: 0.0616
Batch 350, Loss: 0.0811
Batch 360, Loss: 0.0596
Batch 370, Loss: 0.0797
Batch 380, Loss: 0.0802
Batch 390, Loss: 0.0567
Epoch 38 learning rate: 0.013551568628929433
Epoch 38 time: 24.874032974243164 seconds
Epoch 38 accuracy: 91.5%
Batch 10, Loss: 0.0580
Batch 20, Loss: 0.0541
Batch 30, Loss: 0.0442
Batch 40, Loss: 0.0513
Batch 50, Loss: 0.0473
Batch 60, Loss: 0.0454
Batch 70, Loss: 0.0408
Batch 80, Loss: 0.0480
Batch 90, Loss: 0.0389
Batch 100, Loss: 0.0482
Batch 110, Loss: 0.0487
Batch 120, Loss: 0.0443
Batch 130, Loss: 0.0500
Batch 140, Loss: 0.0364
Batch 150, Loss: 0.0405
Batch 160, Loss: 0.0485
Batch 170, Loss: 0.0469
Batch 180, Loss: 0.0446
Batch 190, Loss: 0.0461
Batch 200, Loss: 0.0576
Batch 210, Loss: 0.0589
Batch 220, Loss: 0.0568
Batch 230, Loss: 0.0479
Batch 240, Loss: 0.0427
Batch 250, Loss: 0.0493
Batch 260, Loss: 0.0553
Batch 270, Loss: 0.0541
Batch 280, Loss: 0.0515
Batch 290, Loss: 0.0371
Batch 300, Loss: 0.0590
Batch 310, Loss: 0.0520
Batch 320, Loss: 0.0569
Batch 330, Loss: 0.0594
Batch 340, Loss: 0.0561
Batch 350, Loss: 0.0506
Batch 360, Loss: 0.0577
Batch 370, Loss: 0.0524
Batch 380, Loss: 0.0443
Batch 390, Loss: 0.0474
Epoch 39 learning rate: 0.011474337861210542
Epoch 39 time: 24.893024444580078 seconds
Epoch 39 accuracy: 92.22%
Batch 10, Loss: 0.0351
Batch 20, Loss: 0.0380
Batch 30, Loss: 0.0424
Batch 40, Loss: 0.0359
Batch 50, Loss: 0.0490
Batch 60, Loss: 0.0374
Batch 70, Loss: 0.0397
Batch 80, Loss: 0.0473
Batch 90, Loss: 0.0345
Batch 100, Loss: 0.0492
Batch 110, Loss: 0.0435
Batch 120, Loss: 0.0357
Batch 130, Loss: 0.0468
Batch 140, Loss: 0.0516
Batch 150, Loss: 0.0319
Batch 160, Loss: 0.0532
Batch 170, Loss: 0.0324
Batch 180, Loss: 0.0461
Batch 190, Loss: 0.0384
Batch 200, Loss: 0.0442
Batch 210, Loss: 0.0402
Batch 220, Loss: 0.0330
Batch 230, Loss: 0.0348
Batch 240, Loss: 0.0348
Batch 250, Loss: 0.0385
Batch 260, Loss: 0.0454
Batch 270, Loss: 0.0483
Batch 280, Loss: 0.0346
Batch 290, Loss: 0.0445
Batch 300, Loss: 0.0416
Batch 310, Loss: 0.0358
Batch 320, Loss: 0.0388
Batch 330, Loss: 0.0429
Batch 340, Loss: 0.0504
Batch 350, Loss: 0.0398
Batch 360, Loss: 0.0398
Batch 370, Loss: 0.0422
Batch 380, Loss: 0.0362
Batch 390, Loss: 0.0437
Epoch 40 learning rate: 0.009549150281252632
Epoch 40 time: 24.887461185455322 seconds
Epoch 40 accuracy: 92.91%
Batch 10, Loss: 0.0441
Batch 20, Loss: 0.0274
Batch 30, Loss: 0.0248
Batch 40, Loss: 0.0291
Batch 50, Loss: 0.0254
Batch 60, Loss: 0.0295
Batch 70, Loss: 0.0293
Batch 80, Loss: 0.0273
Batch 90, Loss: 0.0261
Batch 100, Loss: 0.0263
Batch 110, Loss: 0.0355
Batch 120, Loss: 0.0275
Batch 130, Loss: 0.0340
Batch 140, Loss: 0.0369
Batch 150, Loss: 0.0377
Batch 160, Loss: 0.0318
Batch 170, Loss: 0.0350
Batch 180, Loss: 0.0262
Batch 190, Loss: 0.0243
Batch 200, Loss: 0.0398
Batch 210, Loss: 0.0311
Batch 220, Loss: 0.0359
Batch 230, Loss: 0.0246
Batch 240, Loss: 0.0248
Batch 250, Loss: 0.0330
Batch 260, Loss: 0.0280
Batch 270, Loss: 0.0336
Batch 280, Loss: 0.0362
Batch 290, Loss: 0.0413
Batch 300, Loss: 0.0282
Batch 310, Loss: 0.0280
Batch 320, Loss: 0.0308
Batch 330, Loss: 0.0261
Batch 340, Loss: 0.0379
Batch 350, Loss: 0.0366
Batch 360, Loss: 0.0286
Batch 370, Loss: 0.0401
Batch 380, Loss: 0.0359
Batch 390, Loss: 0.0362
Epoch 41 learning rate: 0.007783603724899257
Epoch 41 time: 24.88596510887146 seconds
Epoch 41 accuracy: 92.61%
Batch 10, Loss: 0.0251
Batch 20, Loss: 0.0324
Batch 30, Loss: 0.0171
Batch 40, Loss: 0.0280
Batch 50, Loss: 0.0274
Batch 60, Loss: 0.0205
Batch 70, Loss: 0.0209
Batch 80, Loss: 0.0229
Batch 90, Loss: 0.0265
Batch 100, Loss: 0.0252
Batch 110, Loss: 0.0227
Batch 120, Loss: 0.0204
Batch 130, Loss: 0.0207
Batch 140, Loss: 0.0292
Batch 150, Loss: 0.0279
Batch 160, Loss: 0.0266
Batch 170, Loss: 0.0241
Batch 180, Loss: 0.0265
Batch 190, Loss: 0.0245
Batch 200, Loss: 0.0241
Batch 210, Loss: 0.0175
Batch 220, Loss: 0.0265
Batch 230, Loss: 0.0191
Batch 240, Loss: 0.0226
Batch 250, Loss: 0.0407
Batch 260, Loss: 0.0270
Batch 270, Loss: 0.0223
Batch 280, Loss: 0.0340
Batch 290, Loss: 0.0275
Batch 300, Loss: 0.0241
Batch 310, Loss: 0.0292
Batch 320, Loss: 0.0252
Batch 330, Loss: 0.0252
Batch 340, Loss: 0.0225
Batch 350, Loss: 0.0270
Batch 360, Loss: 0.0215
Batch 370, Loss: 0.0191
Batch 380, Loss: 0.0244
Batch 390, Loss: 0.0307
Epoch 42 learning rate: 0.0061846659978068205
Epoch 42 time: 24.90381121635437 seconds
Epoch 42 accuracy: 93.45%
Batch 10, Loss: 0.0135
Batch 20, Loss: 0.0230
Batch 30, Loss: 0.0235
Batch 40, Loss: 0.0233
Batch 50, Loss: 0.0137
Batch 60, Loss: 0.0200
Batch 70, Loss: 0.0169
Batch 80, Loss: 0.0150
Batch 90, Loss: 0.0177
Batch 100, Loss: 0.0113
Batch 110, Loss: 0.0178
Batch 120, Loss: 0.0191
Batch 130, Loss: 0.0184
Batch 140, Loss: 0.0202
Batch 150, Loss: 0.0173
Batch 160, Loss: 0.0220
Batch 170, Loss: 0.0195
Batch 180, Loss: 0.0196
Batch 190, Loss: 0.0151
Batch 200, Loss: 0.0256
Batch 210, Loss: 0.0160
Batch 220, Loss: 0.0213
Batch 230, Loss: 0.0181
Batch 240, Loss: 0.0145
Batch 250, Loss: 0.0200
Batch 260, Loss: 0.0129
Batch 270, Loss: 0.0164
Batch 280, Loss: 0.0236
Batch 290, Loss: 0.0219
Batch 300, Loss: 0.0201
Batch 310, Loss: 0.0136
Batch 320, Loss: 0.0176
Batch 330, Loss: 0.0180
Batch 340, Loss: 0.0215
Batch 350, Loss: 0.0163
Batch 360, Loss: 0.0148
Batch 370, Loss: 0.0190
Batch 380, Loss: 0.0192
Batch 390, Loss: 0.0190
Epoch 43 learning rate: 0.004758647376699033
Epoch 43 time: 24.92866826057434 seconds
Epoch 43 accuracy: 93.63%
Batch 10, Loss: 0.0125
Batch 20, Loss: 0.0184
Batch 30, Loss: 0.0191
Batch 40, Loss: 0.0134
Batch 50, Loss: 0.0098
Batch 60, Loss: 0.0142
Batch 70, Loss: 0.0264
Batch 80, Loss: 0.0125
Batch 90, Loss: 0.0109
Batch 100, Loss: 0.0135
Batch 110, Loss: 0.0154
Batch 120, Loss: 0.0107
Batch 130, Loss: 0.0131
Batch 140, Loss: 0.0198
Batch 150, Loss: 0.0141
Batch 160, Loss: 0.0148
Batch 170, Loss: 0.0152
Batch 180, Loss: 0.0130
Batch 190, Loss: 0.0154
Batch 200, Loss: 0.0150
Batch 210, Loss: 0.0115
Batch 220, Loss: 0.0119
Batch 230, Loss: 0.0159
Batch 240, Loss: 0.0157
Batch 250, Loss: 0.0181
Batch 260, Loss: 0.0120
Batch 270, Loss: 0.0111
Batch 280, Loss: 0.0098
Batch 290, Loss: 0.0089
Batch 300, Loss: 0.0181
Batch 310, Loss: 0.0096
Batch 320, Loss: 0.0058
Batch 330, Loss: 0.0182
Batch 340, Loss: 0.0137
Batch 350, Loss: 0.0104
Batch 360, Loss: 0.0179
Batch 370, Loss: 0.0110
Batch 380, Loss: 0.0117
Batch 390, Loss: 0.0161
Epoch 44 learning rate: 0.0035111757055874327
Epoch 44 time: 24.90764856338501 seconds
Epoch 44 accuracy: 94.16%
Batch 10, Loss: 0.0111
Batch 20, Loss: 0.0112
Batch 30, Loss: 0.0100
Batch 40, Loss: 0.0131
Batch 50, Loss: 0.0105
Batch 60, Loss: 0.0119
Batch 70, Loss: 0.0073
Batch 80, Loss: 0.0207
Batch 90, Loss: 0.0113
Batch 100, Loss: 0.0091
Batch 110, Loss: 0.0113
Batch 120, Loss: 0.0093
Batch 130, Loss: 0.0136
Batch 140, Loss: 0.0094
Batch 150, Loss: 0.0138
Batch 160, Loss: 0.0122
Batch 170, Loss: 0.0096
Batch 180, Loss: 0.0091
Batch 190, Loss: 0.0127
Batch 200, Loss: 0.0108
Batch 210, Loss: 0.0073
Batch 220, Loss: 0.0110
Batch 230, Loss: 0.0088
Batch 240, Loss: 0.0102
Batch 250, Loss: 0.0124
Batch 260, Loss: 0.0063
Batch 270, Loss: 0.0134
Batch 280, Loss: 0.0101
Batch 290, Loss: 0.0087
Batch 300, Loss: 0.0149
Batch 310, Loss: 0.0175
Batch 320, Loss: 0.0125
Batch 330, Loss: 0.0103
Batch 340, Loss: 0.0085
Batch 350, Loss: 0.0109
Batch 360, Loss: 0.0110
Batch 370, Loss: 0.0136
Batch 380, Loss: 0.0160
Batch 390, Loss: 0.0127
Epoch 45 learning rate: 0.0024471741852423235
Epoch 45 time: 24.899510622024536 seconds
Epoch 45 accuracy: 94.03%
Batch 10, Loss: 0.0143
Batch 20, Loss: 0.0117
Batch 30, Loss: 0.0104
Batch 40, Loss: 0.0078
Batch 50, Loss: 0.0120
Batch 60, Loss: 0.0081
Batch 70, Loss: 0.0111
Batch 80, Loss: 0.0062
Batch 90, Loss: 0.0130
Batch 100, Loss: 0.0098
Batch 110, Loss: 0.0084
Batch 120, Loss: 0.0072
Batch 130, Loss: 0.0081
Batch 140, Loss: 0.0076
Batch 150, Loss: 0.0114
Batch 160, Loss: 0.0073
Batch 170, Loss: 0.0089
Batch 180, Loss: 0.0096
Batch 190, Loss: 0.0106
Batch 200, Loss: 0.0108
Batch 210, Loss: 0.0127
Batch 220, Loss: 0.0047
Batch 230, Loss: 0.0099
Batch 240, Loss: 0.0086
Batch 250, Loss: 0.0082
Batch 260, Loss: 0.0100
Batch 270, Loss: 0.0073
Batch 280, Loss: 0.0077
Batch 290, Loss: 0.0120
Batch 300, Loss: 0.0079
Batch 310, Loss: 0.0118
Batch 320, Loss: 0.0095
Batch 330, Loss: 0.0064
Batch 340, Loss: 0.0075
Batch 350, Loss: 0.0080
Batch 360, Loss: 0.0063
Batch 370, Loss: 0.0093
Batch 380, Loss: 0.0095
Batch 390, Loss: 0.0094
Epoch 46 learning rate: 0.0015708419435684518
Epoch 46 time: 24.90446376800537 seconds
Epoch 46 accuracy: 94.12%
Batch 10, Loss: 0.0071
Batch 20, Loss: 0.0064
Batch 30, Loss: 0.0078
Batch 40, Loss: 0.0085
Batch 50, Loss: 0.0080
Batch 60, Loss: 0.0059
Batch 70, Loss: 0.0112
Batch 80, Loss: 0.0075
Batch 90, Loss: 0.0057
Batch 100, Loss: 0.0060
Batch 110, Loss: 0.0102
Batch 120, Loss: 0.0061
Batch 130, Loss: 0.0112
Batch 140, Loss: 0.0093
Batch 150, Loss: 0.0098
Batch 160, Loss: 0.0081
Batch 170, Loss: 0.0074
Batch 180, Loss: 0.0060
Batch 190, Loss: 0.0092
Batch 200, Loss: 0.0061
Batch 210, Loss: 0.0071
Batch 220, Loss: 0.0099
Batch 230, Loss: 0.0078
Batch 240, Loss: 0.0086
Batch 250, Loss: 0.0049
Batch 260, Loss: 0.0061
Batch 270, Loss: 0.0050
Batch 280, Loss: 0.0085
Batch 290, Loss: 0.0076
Batch 300, Loss: 0.0054
Batch 310, Loss: 0.0112
Batch 320, Loss: 0.0068
Batch 330, Loss: 0.0092
Batch 340, Loss: 0.0058
Batch 350, Loss: 0.0102
Batch 360, Loss: 0.0059
Batch 370, Loss: 0.0062
Batch 380, Loss: 0.0095
Batch 390, Loss: 0.0072
Epoch 47 learning rate: 0.000885637463565564
Epoch 47 time: 24.871752977371216 seconds
Epoch 47 accuracy: 94.13%
Batch 10, Loss: 0.0057
Batch 20, Loss: 0.0091
Batch 30, Loss: 0.0057
Batch 40, Loss: 0.0075
Batch 50, Loss: 0.0063
Batch 60, Loss: 0.0058
Batch 70, Loss: 0.0078
Batch 80, Loss: 0.0083
Batch 90, Loss: 0.0078
Batch 100, Loss: 0.0062
Batch 110, Loss: 0.0065
Batch 120, Loss: 0.0105
Batch 130, Loss: 0.0055
Batch 140, Loss: 0.0095
Batch 150, Loss: 0.0059
Batch 160, Loss: 0.0062
Batch 170, Loss: 0.0079
Batch 180, Loss: 0.0069
Batch 190, Loss: 0.0070
Batch 200, Loss: 0.0055
Batch 210, Loss: 0.0096
Batch 220, Loss: 0.0070
Batch 230, Loss: 0.0098
Batch 240, Loss: 0.0057
Batch 250, Loss: 0.0074
Batch 260, Loss: 0.0075
Batch 270, Loss: 0.0084
Batch 280, Loss: 0.0073
Batch 290, Loss: 0.0069
Batch 300, Loss: 0.0048
Batch 310, Loss: 0.0065
Batch 320, Loss: 0.0057
Batch 330, Loss: 0.0066
Batch 340, Loss: 0.0067
Batch 350, Loss: 0.0062
Batch 360, Loss: 0.0052
Batch 370, Loss: 0.0077
Batch 380, Loss: 0.0063
Batch 390, Loss: 0.0040
Epoch 48 learning rate: 0.00039426493427611173
Epoch 48 time: 24.876906871795654 seconds
Epoch 48 accuracy: 94.29%
Batch 10, Loss: 0.0069
Batch 20, Loss: 0.0063
Batch 30, Loss: 0.0090
Batch 40, Loss: 0.0081
Batch 50, Loss: 0.0065
Batch 60, Loss: 0.0059
Batch 70, Loss: 0.0060
Batch 80, Loss: 0.0120
Batch 90, Loss: 0.0072
Batch 100, Loss: 0.0073
Batch 110, Loss: 0.0047
Batch 120, Loss: 0.0071
Batch 130, Loss: 0.0083
Batch 140, Loss: 0.0043
Batch 150, Loss: 0.0049
Batch 160, Loss: 0.0065
Batch 170, Loss: 0.0054
Batch 180, Loss: 0.0063
Batch 190, Loss: 0.0067
Batch 200, Loss: 0.0075
Batch 210, Loss: 0.0072
Batch 220, Loss: 0.0111
Batch 230, Loss: 0.0052
Batch 240, Loss: 0.0054
Batch 250, Loss: 0.0070
Batch 260, Loss: 0.0072
Batch 270, Loss: 0.0035
Batch 280, Loss: 0.0078
Batch 290, Loss: 0.0075
Batch 300, Loss: 0.0074
Batch 310, Loss: 0.0044
Batch 320, Loss: 0.0072
Batch 330, Loss: 0.0087
Batch 340, Loss: 0.0085
Batch 350, Loss: 0.0083
Batch 360, Loss: 0.0045
Batch 370, Loss: 0.0049
Batch 380, Loss: 0.0063
Batch 390, Loss: 0.0081
Epoch 49 learning rate: 9.866357858642205e-05
Epoch 49 time: 24.8956081867218 seconds
Epoch 49 accuracy: 94.28%
Batch 10, Loss: 0.0061
Batch 20, Loss: 0.0057
Batch 30, Loss: 0.0082
Batch 40, Loss: 0.0069
Batch 50, Loss: 0.0079
Batch 60, Loss: 0.0085
Batch 70, Loss: 0.0070
Batch 80, Loss: 0.0039
Batch 90, Loss: 0.0064
Batch 100, Loss: 0.0068
Batch 110, Loss: 0.0111
Batch 120, Loss: 0.0063
Batch 130, Loss: 0.0078
Batch 140, Loss: 0.0053
Batch 150, Loss: 0.0053
Batch 160, Loss: 0.0053
Batch 170, Loss: 0.0062
Batch 180, Loss: 0.0062
Batch 190, Loss: 0.0042
Batch 200, Loss: 0.0073
Batch 210, Loss: 0.0058
Batch 220, Loss: 0.0073
Batch 230, Loss: 0.0050
Batch 240, Loss: 0.0100
Batch 250, Loss: 0.0073
Batch 260, Loss: 0.0108
Batch 270, Loss: 0.0047
Batch 280, Loss: 0.0062
Batch 290, Loss: 0.0053
Batch 300, Loss: 0.0092
Batch 310, Loss: 0.0052
Batch 320, Loss: 0.0062
Batch 330, Loss: 0.0069
Batch 340, Loss: 0.0067
Batch 350, Loss: 0.0044
Batch 360, Loss: 0.0048
Batch 370, Loss: 0.0056
Batch 380, Loss: 0.0060
Batch 390, Loss: 0.0068
Epoch 50 learning rate: 0.0
Epoch 50 time: 24.942672967910767 seconds
Epoch 50 accuracy: 94.25%
rho:  0.04 , alpha:  0.3
Total training time: 1251.2412040233612 seconds
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
The top Hessian eigenvalue of this model is 157.5169
Norm of the Gradient: 5.7547390461e-01
