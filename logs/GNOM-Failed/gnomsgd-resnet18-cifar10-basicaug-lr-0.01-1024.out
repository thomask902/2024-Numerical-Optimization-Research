The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py", line 389, in <module>
    main()
  File "/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py", line 200, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py", line 360, in main_worker
    train_epoch_gam(model, train_loader, optimizer, gpu, args.print_freq)
  File "/project/6070520/tkleinkn/Vanilla-GAM/utils/train_utils_gam.py", line 26, in train_epoch_gam
    predictions, loss = optimizer.step()
                        ^^^^^^^^^^^^^^^^
  File "/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/project/6070520/tkleinkn/Vanilla-GAM/utils/gnom.py", line 121, in step
    g = self.grad_norm_grad()
        ^^^^^^^^^^^^^^^^^^^^^
  File "/project/6070520/tkleinkn/Vanilla-GAM/utils/gnom.py", line 51, in grad_norm_grad
    hessian_vec_prod_dict = torch.autograd.grad(
                            ^^^^^^^^^^^^^^^^^^^^
  File "/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/__init__.py", line 412, in grad
    result = _engine_run_backward(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 
