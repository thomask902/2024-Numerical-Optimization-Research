The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR100/GAMNonAccelerated
Using non-accelerated GAM
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 3.6530
Batch 200, Loss: 3.3175
Batch 300, Loss: 3.2210
Epoch 1 learning rate: 0.09999383162408304
Epoch 1 time: 245.6713309288025 seconds
Epoch 1 accuracy: 12.46%
Batch 100, Loss: 3.0707
Batch 200, Loss: 2.9385
Batch 300, Loss: 2.8587
Epoch 2 learning rate: 0.09997532801828658
Epoch 2 time: 236.32312631607056 seconds
Epoch 2 accuracy: 21.22%
Batch 100, Loss: 2.6669
Batch 200, Loss: 2.6178
Batch 300, Loss: 2.5324
Epoch 3 learning rate: 0.09994449374809851
Epoch 3 time: 236.07580733299255 seconds
Epoch 3 accuracy: 27.29%
Batch 100, Loss: 2.3507
Batch 200, Loss: 2.2965
Batch 300, Loss: 2.2316
Epoch 4 learning rate: 0.09990133642141359
Epoch 4 time: 236.24686431884766 seconds
Epoch 4 accuracy: 35.05%
Batch 100, Loss: 2.0626
Batch 200, Loss: 2.0379
Batch 300, Loss: 1.9972
Epoch 5 learning rate: 0.09984586668665642
Epoch 5 time: 236.17495107650757 seconds
Epoch 5 accuracy: 39.94%
Batch 100, Loss: 1.8453
Batch 200, Loss: 1.8283
Batch 300, Loss: 1.8231
Epoch 6 learning rate: 0.09977809823015402
Epoch 6 time: 236.21371364593506 seconds
Epoch 6 accuracy: 41.92%
Batch 100, Loss: 1.6986
Batch 200, Loss: 1.7072
Batch 300, Loss: 1.6692
Epoch 7 learning rate: 0.09969804777275901
Epoch 7 time: 236.13483905792236 seconds
Epoch 7 accuracy: 48.11%
Batch 100, Loss: 1.5618
Batch 200, Loss: 1.5771
Batch 300, Loss: 1.5707
Epoch 8 learning rate: 0.09960573506572391
Epoch 8 time: 235.98541736602783 seconds
Epoch 8 accuracy: 51.51%
Batch 100, Loss: 1.4857
Batch 200, Loss: 1.5048
Batch 300, Loss: 1.5041
Epoch 9 learning rate: 0.09950118288582789
Epoch 9 time: 236.01296877861023 seconds
Epoch 9 accuracy: 53.81%
Batch 100, Loss: 1.4059
Batch 200, Loss: 1.4340
Batch 300, Loss: 1.4270
Epoch 10 learning rate: 0.09938441702975691
Epoch 10 time: 235.926255941391 seconds
Epoch 10 accuracy: 53.15%
Batch 100, Loss: 1.3661
Batch 200, Loss: 1.3732
Batch 300, Loss: 1.3598
Epoch 11 learning rate: 0.09925546630773871
Epoch 11 time: 235.94253063201904 seconds
Epoch 11 accuracy: 52.14%
Batch 100, Loss: 1.3184
Batch 200, Loss: 1.3265
Batch 300, Loss: 1.3492
Epoch 12 learning rate: 0.09911436253643445
Epoch 12 time: 235.79043865203857 seconds
Epoch 12 accuracy: 53.18%
Batch 100, Loss: 1.2749
Batch 200, Loss: 1.2654
Batch 300, Loss: 1.3095
Epoch 13 learning rate: 0.0989611405310883
Epoch 13 time: 235.97953534126282 seconds
Epoch 13 accuracy: 54.37%
Batch 100, Loss: 1.2260
Batch 200, Loss: 1.2624
Batch 300, Loss: 1.2738
Epoch 14 learning rate: 0.09879583809693739
Epoch 14 time: 236.06847548484802 seconds
Epoch 14 accuracy: 54.96%
Batch 100, Loss: 1.2013
Batch 200, Loss: 1.2210
Batch 300, Loss: 1.2520
Epoch 15 learning rate: 0.09861849601988384
Epoch 15 time: 235.9484281539917 seconds
Epoch 15 accuracy: 56.76%
Batch 100, Loss: 1.1625
Batch 200, Loss: 1.2057
Batch 300, Loss: 1.2071
Epoch 16 learning rate: 0.09842915805643157
Epoch 16 time: 236.0607283115387 seconds
Epoch 16 accuracy: 55.4%
Batch 100, Loss: 1.1517
Batch 200, Loss: 1.1863
Batch 300, Loss: 1.1987
Epoch 17 learning rate: 0.09822787092288993
Epoch 17 time: 236.0530858039856 seconds
Epoch 17 accuracy: 57.45%
Batch 100, Loss: 1.1354
Batch 200, Loss: 1.1544
Batch 300, Loss: 1.1813
Epoch 18 learning rate: 0.09801468428384717
Epoch 18 time: 236.12110018730164 seconds
Epoch 18 accuracy: 58.6%
Batch 100, Loss: 1.1097
Batch 200, Loss: 1.1526
Batch 300, Loss: 1.1601
Epoch 19 learning rate: 0.09778965073991652
Epoch 19 time: 236.0643594264984 seconds
Epoch 19 accuracy: 56.45%
Batch 100, Loss: 1.1036
Batch 200, Loss: 1.1067
Batch 300, Loss: 1.1481
Epoch 20 learning rate: 0.0975528258147577
Epoch 20 time: 236.0219156742096 seconds
Epoch 20 accuracy: 59.23%
Batch 100, Loss: 1.0760
Batch 200, Loss: 1.1041
Batch 300, Loss: 1.1278
Epoch 21 learning rate: 0.09730426794137728
Epoch 21 time: 235.95463299751282 seconds
Epoch 21 accuracy: 58.82%
Batch 100, Loss: 1.0481
Batch 200, Loss: 1.0809
Batch 300, Loss: 1.1103
Epoch 22 learning rate: 0.09704403844771128
Epoch 22 time: 236.06170630455017 seconds
Epoch 22 accuracy: 57.65%
Batch 100, Loss: 1.0523
Batch 200, Loss: 1.0844
Batch 300, Loss: 1.0997
Epoch 23 learning rate: 0.09677220154149338
Epoch 23 time: 236.1193504333496 seconds
Epoch 23 accuracy: 57.52%
Batch 100, Loss: 1.0298
Batch 200, Loss: 1.0624
Batch 300, Loss: 1.0903
Epoch 24 learning rate: 0.09648882429441258
Epoch 24 time: 236.12086534500122 seconds
Epoch 24 accuracy: 60.99%
Batch 100, Loss: 1.0175
Batch 200, Loss: 1.0688
Batch 300, Loss: 1.0585
Epoch 25 learning rate: 0.09619397662556435
Epoch 25 time: 236.1019127368927 seconds
Epoch 25 accuracy: 59.11%
Batch 100, Loss: 1.0130
Batch 200, Loss: 1.0407
Batch 300, Loss: 1.0691
Epoch 26 learning rate: 0.09588773128419906
Epoch 26 time: 236.09246158599854 seconds
Epoch 26 accuracy: 60.89%
Batch 100, Loss: 1.0078
Batch 200, Loss: 1.0224
Batch 300, Loss: 1.0557
Epoch 27 learning rate: 0.09557016383177226
Epoch 27 time: 235.83340334892273 seconds
Epoch 27 accuracy: 61.16%
Batch 100, Loss: 0.9694
Batch 200, Loss: 1.0174
Batch 300, Loss: 1.0421
Epoch 28 learning rate: 0.09524135262330098
Epoch 28 time: 236.03128957748413 seconds
Epoch 28 accuracy: 59.15%
Batch 100, Loss: 0.9931
Batch 200, Loss: 1.0056
Batch 300, Loss: 1.0434
Epoch 29 learning rate: 0.09490137878803079
Epoch 29 time: 235.9835925102234 seconds
Epoch 29 accuracy: 61.47%
Batch 100, Loss: 0.9571
Batch 200, Loss: 1.0028
Batch 300, Loss: 1.0474
Epoch 30 learning rate: 0.0945503262094184
Epoch 30 time: 236.0818109512329 seconds
Epoch 30 accuracy: 59.38%
Batch 100, Loss: 0.9749
Batch 200, Loss: 0.9789
Batch 300, Loss: 1.0126
Epoch 31 learning rate: 0.0941882815044347
Epoch 31 time: 236.0901746749878 seconds
Epoch 31 accuracy: 61.61%
Batch 100, Loss: 0.9401
Batch 200, Loss: 0.9995
Batch 300, Loss: 1.0174
Epoch 32 learning rate: 0.09381533400219319
Epoch 32 time: 236.0221962928772 seconds
Epoch 32 accuracy: 61.17%
Batch 100, Loss: 0.9395
Batch 200, Loss: 0.9858
Batch 300, Loss: 1.0045
Epoch 33 learning rate: 0.09343157572190958
Epoch 33 time: 236.1859896183014 seconds
Epoch 33 accuracy: 59.4%
Batch 100, Loss: 0.9343
Batch 200, Loss: 0.9662
Batch 300, Loss: 0.9967
Epoch 34 learning rate: 0.0930371013501972
Epoch 34 time: 236.068683385849 seconds
Epoch 34 accuracy: 63.04%
Batch 100, Loss: 0.9275
Batch 200, Loss: 0.9652
Batch 300, Loss: 0.9825
Epoch 35 learning rate: 0.09263200821770463
Epoch 35 time: 237.82823133468628 seconds
Epoch 35 accuracy: 61.44%
Batch 100, Loss: 0.9007
Batch 200, Loss: 0.9760
Batch 300, Loss: 0.9767
Epoch 36 learning rate: 0.09221639627510078
Epoch 36 time: 236.08303117752075 seconds
Epoch 36 accuracy: 59.97%
Batch 100, Loss: 0.9338
Batch 200, Loss: 0.9181
Batch 300, Loss: 0.9951
Epoch 37 learning rate: 0.09179036806841355
Epoch 37 time: 235.95172309875488 seconds
Epoch 37 accuracy: 61.61%
Batch 100, Loss: 0.8978
Batch 200, Loss: 0.9512
Batch 300, Loss: 0.9649
Epoch 38 learning rate: 0.09135402871372812
Epoch 38 time: 235.91228604316711 seconds
Epoch 38 accuracy: 61.7%
Batch 100, Loss: 0.9095
Batch 200, Loss: 0.9390
Batch 300, Loss: 0.9595
Epoch 39 learning rate: 0.0909074858712512
Epoch 39 time: 235.95886278152466 seconds
Epoch 39 accuracy: 60.86%
Batch 100, Loss: 0.8762
Batch 200, Loss: 0.9440
Batch 300, Loss: 0.9522
Epoch 40 learning rate: 0.09045084971874741
Epoch 40 time: 236.06715869903564 seconds
Epoch 40 accuracy: 60.87%
Batch 100, Loss: 0.8729
Batch 200, Loss: 0.9161
Batch 300, Loss: 0.9486
Epoch 41 learning rate: 0.08998423292435458
Epoch 41 time: 235.9153869152069 seconds
Epoch 41 accuracy: 60.81%
Batch 100, Loss: 0.8919
Batch 200, Loss: 0.9060
Batch 300, Loss: 0.9488
Epoch 42 learning rate: 0.08950775061878455
Epoch 42 time: 235.98694443702698 seconds
Epoch 42 accuracy: 61.77%
Batch 100, Loss: 0.8818
Batch 200, Loss: 0.9188
Batch 300, Loss: 0.9330
Epoch 43 learning rate: 0.08902152036691653
Epoch 43 time: 235.94053292274475 seconds
Epoch 43 accuracy: 63.78%
Batch 100, Loss: 0.8783
Batch 200, Loss: 0.9066
Batch 300, Loss: 0.9273
Epoch 44 learning rate: 0.08852566213878951
Epoch 44 time: 235.9682319164276 seconds
Epoch 44 accuracy: 60.6%
Batch 100, Loss: 0.8493
Batch 200, Loss: 0.8943
Batch 300, Loss: 0.9438
Epoch 45 learning rate: 0.0880202982800016
Epoch 45 time: 236.11644554138184 seconds
Epoch 45 accuracy: 63.09%
Batch 100, Loss: 0.8549
Batch 200, Loss: 0.9037
Batch 300, Loss: 0.9208
Epoch 46 learning rate: 0.08750555348152303
Epoch 46 time: 236.0386290550232 seconds
Epoch 46 accuracy: 63.97%
Batch 100, Loss: 0.8362
Batch 200, Loss: 0.8911
Batch 300, Loss: 0.9159
Epoch 47 learning rate: 0.08698155474893052
Epoch 47 time: 235.97752404212952 seconds
Epoch 47 accuracy: 64.57%
Batch 100, Loss: 0.8439
Batch 200, Loss: 0.8562
Batch 300, Loss: 0.9093
Epoch 48 learning rate: 0.08644843137107061
Epoch 48 time: 236.02207612991333 seconds
Epoch 48 accuracy: 60.75%
Batch 100, Loss: 0.8272
Batch 200, Loss: 0.8856
Batch 300, Loss: 0.9053
Epoch 49 learning rate: 0.08590631488815947
Epoch 49 time: 235.94046592712402 seconds
Epoch 49 accuracy: 62.91%
Batch 100, Loss: 0.8384
Batch 200, Loss: 0.8787
Batch 300, Loss: 0.8904
Epoch 50 learning rate: 0.0853553390593274
Epoch 50 time: 236.01175379753113 seconds
Epoch 50 accuracy: 59.6%
Batch 100, Loss: 0.8298
Batch 200, Loss: 0.8603
Batch 300, Loss: 0.8875
Epoch 51 learning rate: 0.08479563982961574
Epoch 51 time: 236.0267333984375 seconds
Epoch 51 accuracy: 64.53%
Batch 100, Loss: 0.8253
Batch 200, Loss: 0.8768
Batch 300, Loss: 0.8704
Epoch 52 learning rate: 0.08422735529643446
Epoch 52 time: 235.89603209495544 seconds
Epoch 52 accuracy: 66.35%
Batch 100, Loss: 0.8005
Batch 200, Loss: 0.8727
Batch 300, Loss: 0.8850
Epoch 53 learning rate: 0.08365062567548869
Epoch 53 time: 235.8811559677124 seconds
Epoch 53 accuracy: 63.15%
Batch 100, Loss: 0.7980
Batch 200, Loss: 0.8435
Batch 300, Loss: 0.8604
Epoch 54 learning rate: 0.08306559326618261
Epoch 54 time: 236.0702519416809 seconds
Epoch 54 accuracy: 64.93%
Batch 100, Loss: 0.7913
Batch 200, Loss: 0.8350
Batch 300, Loss: 0.8732
Epoch 55 learning rate: 0.0824724024165092
Epoch 55 time: 235.98737740516663 seconds
Epoch 55 accuracy: 63.84%
Batch 100, Loss: 0.7856
Batch 200, Loss: 0.8141
Batch 300, Loss: 0.8515
Epoch 56 learning rate: 0.0818711994874345
Epoch 56 time: 235.94102787971497 seconds
Epoch 56 accuracy: 63.19%
Batch 100, Loss: 0.7923
Batch 200, Loss: 0.8290
Batch 300, Loss: 0.8553
Epoch 57 learning rate: 0.08126213281678528
Epoch 57 time: 236.03485989570618 seconds
Epoch 57 accuracy: 65.68%
Batch 100, Loss: 0.7728
Batch 200, Loss: 0.8140
Batch 300, Loss: 0.8584
Epoch 58 learning rate: 0.08064535268264884
Epoch 58 time: 236.17140364646912 seconds
Epoch 58 accuracy: 64.49%
Batch 100, Loss: 0.7687
Batch 200, Loss: 0.8194
Batch 300, Loss: 0.8313
Epoch 59 learning rate: 0.08002101126629421
Epoch 59 time: 236.09747529029846 seconds
Epoch 59 accuracy: 64.89%
Batch 100, Loss: 0.7640
Batch 200, Loss: 0.8023
Batch 300, Loss: 0.8212
Epoch 60 learning rate: 0.07938926261462367
Epoch 60 time: 236.02341032028198 seconds
Epoch 60 accuracy: 64.62%
Batch 100, Loss: 0.7561
Batch 200, Loss: 0.8071
Batch 300, Loss: 0.8330
Epoch 61 learning rate: 0.07875026260216395
Epoch 61 time: 236.14921355247498 seconds
Epoch 61 accuracy: 65.4%
Batch 100, Loss: 0.7531
Batch 200, Loss: 0.7964
Batch 300, Loss: 0.8276
Epoch 62 learning rate: 0.07810416889260656
Epoch 62 time: 236.05560779571533 seconds
Epoch 62 accuracy: 66.23%
Batch 100, Loss: 0.7673
Batch 200, Loss: 0.8012
Batch 300, Loss: 0.8047
Epoch 63 learning rate: 0.07745114089990661
Epoch 63 time: 236.00965642929077 seconds
Epoch 63 accuracy: 66.75%
Batch 100, Loss: 0.7427
Batch 200, Loss: 0.7790
Batch 300, Loss: 0.8087
Epoch 64 learning rate: 0.07679133974894985
Epoch 64 time: 236.1077060699463 seconds
Epoch 64 accuracy: 65.75%
Batch 100, Loss: 0.7437
Batch 200, Loss: 0.7612
Batch 300, Loss: 0.8216
Epoch 65 learning rate: 0.07612492823579746
Epoch 65 time: 236.50429034233093 seconds
Epoch 65 accuracy: 66.23%
Batch 100, Loss: 0.7330
Batch 200, Loss: 0.7602
Batch 300, Loss: 0.7835
Epoch 66 learning rate: 0.07545207078751859
Epoch 66 time: 236.02551651000977 seconds
Epoch 66 accuracy: 61.79%
Batch 100, Loss: 0.7283
Batch 200, Loss: 0.7585
Batch 300, Loss: 0.7883
Epoch 67 learning rate: 0.0747729334216204
Epoch 67 time: 235.97863268852234 seconds
Epoch 67 accuracy: 65.5%
Batch 100, Loss: 0.7294
Batch 200, Loss: 0.7573
Batch 300, Loss: 0.7679
Epoch 68 learning rate: 0.07408768370508578
Epoch 68 time: 236.13529014587402 seconds
Epoch 68 accuracy: 66.35%
Batch 100, Loss: 0.7083
Batch 200, Loss: 0.7451
Batch 300, Loss: 0.7830
Epoch 69 learning rate: 0.0733964907130287
Epoch 69 time: 235.84619760513306 seconds
Epoch 69 accuracy: 66.11%
Batch 100, Loss: 0.6866
Batch 200, Loss: 0.7575
Batch 300, Loss: 0.7631
Epoch 70 learning rate: 0.07269952498697736
Epoch 70 time: 236.02555298805237 seconds
Epoch 70 accuracy: 65.25%
Batch 100, Loss: 0.7035
Batch 200, Loss: 0.7112
Batch 300, Loss: 0.7736
Epoch 71 learning rate: 0.07199695849279578
Epoch 71 time: 235.93094205856323 seconds
Epoch 71 accuracy: 65.91%
Batch 100, Loss: 0.6823
Batch 200, Loss: 0.7392
Batch 300, Loss: 0.7592
Epoch 72 learning rate: 0.07128896457825366
Epoch 72 time: 235.98687028884888 seconds
Epoch 72 accuracy: 64.02%
Batch 100, Loss: 0.6963
Batch 200, Loss: 0.7209
Batch 300, Loss: 0.7602
Epoch 73 learning rate: 0.07057571793025548
Epoch 73 time: 236.02003955841064 seconds
Epoch 73 accuracy: 67.73%
Batch 100, Loss: 0.6754
Batch 200, Loss: 0.7062
Batch 300, Loss: 0.7545
Epoch 74 learning rate: 0.06985739453173906
Epoch 74 time: 235.89694786071777 seconds
Epoch 74 accuracy: 66.61%
Batch 100, Loss: 0.6800
Batch 200, Loss: 0.7029
Batch 300, Loss: 0.7492
Epoch 75 learning rate: 0.06913417161825453
Epoch 75 time: 236.0578739643097 seconds
Epoch 75 accuracy: 64.9%
Batch 100, Loss: 0.6599
Batch 200, Loss: 0.7103
Batch 300, Loss: 0.7380
Epoch 76 learning rate: 0.06840622763423394
Epoch 76 time: 236.10234665870667 seconds
Epoch 76 accuracy: 64.05%
Batch 100, Loss: 0.6642
Batch 200, Loss: 0.6940
Batch 300, Loss: 0.7285
Epoch 77 learning rate: 0.0676737421889629
Epoch 77 time: 236.14521551132202 seconds
Epoch 77 accuracy: 65.62%
Batch 100, Loss: 0.6411
Batch 200, Loss: 0.6819
Batch 300, Loss: 0.7053
Epoch 78 learning rate: 0.06693689601226462
Epoch 78 time: 236.0431866645813 seconds
Epoch 78 accuracy: 66.57%
Batch 100, Loss: 0.6474
Batch 200, Loss: 0.6729
Batch 300, Loss: 0.6987
Epoch 79 learning rate: 0.06619587090990751
Epoch 79 time: 236.0356764793396 seconds
Epoch 79 accuracy: 65.54%
Batch 100, Loss: 0.6346
Batch 200, Loss: 0.6784
Batch 300, Loss: 0.7130
Epoch 80 learning rate: 0.06545084971874741
Epoch 80 time: 236.05835127830505 seconds
Epoch 80 accuracy: 67.09%
Batch 100, Loss: 0.6411
Batch 200, Loss: 0.6775
Batch 300, Loss: 0.6798
Epoch 81 learning rate: 0.06470201626161524
Epoch 81 time: 235.9983892440796 seconds
Epoch 81 accuracy: 67.15%
Batch 100, Loss: 0.6175
Batch 200, Loss: 0.6570
Batch 300, Loss: 0.7039
Epoch 82 learning rate: 0.06394955530196152
Epoch 82 time: 235.93312001228333 seconds
Epoch 82 accuracy: 67.52%
Batch 100, Loss: 0.6302
Batch 200, Loss: 0.6405
Batch 300, Loss: 0.6911
Epoch 83 learning rate: 0.06319365249826868
Epoch 83 time: 236.02238249778748 seconds
Epoch 83 accuracy: 66.43%
Batch 100, Loss: 0.6092
Batch 200, Loss: 0.6390
Batch 300, Loss: 0.6726
Epoch 84 learning rate: 0.06243449435824277
Epoch 84 time: 235.97874450683594 seconds
Epoch 84 accuracy: 68.67%
Batch 100, Loss: 0.6014
Batch 200, Loss: 0.6323
Batch 300, Loss: 0.6585
Epoch 85 learning rate: 0.06167226819279532
Epoch 85 time: 235.9498257637024 seconds
Epoch 85 accuracy: 67.44%
Batch 100, Loss: 0.6017
Batch 200, Loss: 0.6412
Batch 300, Loss: 0.6369
Epoch 86 learning rate: 0.06090716206982718
Epoch 86 time: 235.96565914154053 seconds
Epoch 86 accuracy: 65.88%
Batch 100, Loss: 0.5914
Batch 200, Loss: 0.6158
Batch 300, Loss: 0.6533
Epoch 87 learning rate: 0.06013936476782568
Epoch 87 time: 235.9623999595642 seconds
Epoch 87 accuracy: 67.73%
Batch 100, Loss: 0.5651
Batch 200, Loss: 0.6087
Batch 300, Loss: 0.6479
Epoch 88 learning rate: 0.05936906572928629
Epoch 88 time: 235.97362899780273 seconds
Epoch 88 accuracy: 68.92%
Batch 100, Loss: 0.5645
Batch 200, Loss: 0.6162
Batch 300, Loss: 0.6367
Epoch 89 learning rate: 0.05859645501397052
Epoch 89 time: 236.06240916252136 seconds
Epoch 89 accuracy: 68.13%
