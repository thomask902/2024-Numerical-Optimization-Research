The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-06-11:12:11
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 93.6520
Batch 50, Loss: 29.6478
Batch 75, Loss: 12.7537
Batch 100, Loss: 9.2122
Batch 125, Loss: 7.5662
Batch 150, Loss: 6.4490
Batch 175, Loss: 5.7259
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 290.3527367115021 seconds
Epoch 1 accuracy: 10.82%
Batch 25, Loss: 5.0184
Batch 50, Loss: 4.7494
Batch 75, Loss: 4.5397
Batch 100, Loss: 4.3672
Batch 125, Loss: 4.2194
Batch 150, Loss: 4.0903
Batch 175, Loss: 3.9761
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 322.6486976146698 seconds
Epoch 2 accuracy: 10.79%
Batch 25, Loss: 3.8101
Batch 50, Loss: 3.7245
Batch 75, Loss: 3.6468
Batch 100, Loss: 3.5761
Batch 125, Loss: 3.5115
Batch 150, Loss: 3.4522
Batch 175, Loss: 3.3975
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 307.3468244075775 seconds
Epoch 3 accuracy: 11.57%
Batch 25, Loss: 3.3151
Batch 50, Loss: 3.2711
Batch 75, Loss: 3.2301
Batch 100, Loss: 3.1918
Batch 125, Loss: 3.1559
Batch 150, Loss: 3.1222
Batch 175, Loss: 3.0904
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 310.03311038017273 seconds
Epoch 4 accuracy: 11.82%
Batch 25, Loss: 3.0404
Batch 50, Loss: 3.0126
Batch 75, Loss: 2.9861
Batch 100, Loss: 2.9607
Batch 125, Loss: 2.9362
Batch 150, Loss: 2.9125
Batch 175, Loss: 2.8897
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 336.64856243133545 seconds
Epoch 5 accuracy: 12.02%
Batch 25, Loss: 2.8527
Batch 50, Loss: 2.8314
Batch 75, Loss: 2.8108
Batch 100, Loss: 2.7907
Batch 125, Loss: 2.7711
Batch 150, Loss: 2.7518
Batch 175, Loss: 2.7331
Noise applied in 147 out of 1152 batches, 12.76
Epoch 6 learning rate: 0.01
Epoch 6 time: 476.80855536460876 seconds
Epoch 6 accuracy: 13.92%
Batch 25, Loss: 2.7026
Batch 50, Loss: 2.6851
Batch 75, Loss: 2.6679
Batch 100, Loss: 2.6511
Batch 125, Loss: 2.6345
Batch 150, Loss: 2.6184
Batch 175, Loss: 2.6025
Noise applied in 339 out of 1344 batches, 25.22
Epoch 7 learning rate: 0.01
Epoch 7 time: 413.01996636390686 seconds
Epoch 7 accuracy: 14.55%
Batch 25, Loss: 2.5765
Batch 50, Loss: 2.5614
Batch 75, Loss: 2.5466
Batch 100, Loss: 2.5319
Batch 125, Loss: 2.5175
Batch 150, Loss: 2.5033
Batch 175, Loss: 2.4893
Noise applied in 531 out of 1536 batches, 34.57
Epoch 8 learning rate: 0.01
Epoch 8 time: 436.980366230011 seconds
Epoch 8 accuracy: 14.59%
Batch 25, Loss: 2.4663
Batch 50, Loss: 2.4528
Batch 75, Loss: 2.4395
Batch 100, Loss: 2.4263
Batch 125, Loss: 2.4133
Batch 150, Loss: 2.4006
Batch 175, Loss: 2.3879
Noise applied in 723 out of 1728 batches, 41.84
Epoch 9 learning rate: 0.01
Epoch 9 time: 545.2466831207275 seconds
Epoch 9 accuracy: 14.35%
Batch 25, Loss: 2.3669
Batch 50, Loss: 2.3546
Batch 75, Loss: 2.3424
Batch 100, Loss: 2.3300
Batch 125, Loss: 2.3178
Batch 150, Loss: 2.3059
Batch 175, Loss: 2.2946
Noise applied in 915 out of 1920 batches, 47.66
Epoch 10 learning rate: 0.01
Epoch 10 time: 483.15986490249634 seconds
Epoch 10 accuracy: 11.93%
Batch 25, Loss: 2.2763
Batch 50, Loss: 2.2658
Batch 75, Loss: 2.2556
Batch 100, Loss: 2.2457
Batch 125, Loss: 2.2360
Batch 150, Loss: 2.2265
Batch 175, Loss: 2.2172
Noise applied in 1107 out of 2112 batches, 52.41
Epoch 11 learning rate: 0.01
Epoch 11 time: 420.5616943836212 seconds
Epoch 11 accuracy: 11.6%
Batch 25, Loss: 2.2021
Batch 50, Loss: 2.1933
Batch 75, Loss: 2.1847
Batch 100, Loss: 2.1763
Batch 125, Loss: 2.1680
Batch 150, Loss: 2.1599
Batch 175, Loss: 2.1520
Noise applied in 1299 out of 2304 batches, 56.38
Epoch 12 learning rate: 0.01
Epoch 12 time: 383.21138620376587 seconds
Epoch 12 accuracy: 11.06%
Batch 25, Loss: 2.1391
Batch 50, Loss: 2.1317
Batch 75, Loss: 2.1243
Batch 100, Loss: 2.1170
Batch 125, Loss: 2.1099
Batch 150, Loss: 2.1029
Batch 175, Loss: 2.0960
Noise applied in 1491 out of 2496 batches, 59.74
Epoch 13 learning rate: 0.01
Epoch 13 time: 447.95750164985657 seconds
Epoch 13 accuracy: 10.7%
Batch 25, Loss: 2.0846
Batch 50, Loss: 2.0780
Batch 75, Loss: 2.0715
Batch 100, Loss: 2.0652
Batch 125, Loss: 2.0589
Batch 150, Loss: 2.0527
Batch 175, Loss: 2.0466
Noise applied in 1683 out of 2688 batches, 62.61
Epoch 14 learning rate: 0.01
Epoch 14 time: 399.76079416275024 seconds
Epoch 14 accuracy: 10.43%
Batch 25, Loss: 2.0367
Batch 50, Loss: 2.0309
Batch 75, Loss: 2.0252
Batch 100, Loss: 2.0196
Batch 125, Loss: 2.0141
Batch 150, Loss: 2.0087
Batch 175, Loss: 2.0033
Noise applied in 1875 out of 2880 batches, 65.10
Epoch 15 learning rate: 0.01
Epoch 15 time: 365.7319321632385 seconds
Epoch 15 accuracy: 10.13%
Batch 25, Loss: 1.9944
Batch 50, Loss: 1.9892
Batch 75, Loss: 1.9841
Batch 100, Loss: 1.9790
Batch 125, Loss: 1.9740
Batch 150, Loss: 1.9690
Batch 175, Loss: 1.9641
Noise applied in 2067 out of 3072 batches, 67.29
Epoch 16 learning rate: 0.01
Epoch 16 time: 412.12594652175903 seconds
Epoch 16 accuracy: 10.05%
Batch 25, Loss: 1.9559
Batch 50, Loss: 1.9511
Batch 75, Loss: 1.9463
Batch 100, Loss: 1.9415
Batch 125, Loss: 1.9367
Batch 150, Loss: 1.9320
Batch 175, Loss: 1.9274
Noise applied in 2259 out of 3264 batches, 69.21
Epoch 17 learning rate: 0.01
Epoch 17 time: 362.9547185897827 seconds
Epoch 17 accuracy: 10.12%
Batch 25, Loss: 1.9200
Batch 50, Loss: 1.9157
Batch 75, Loss: 1.9114
Batch 100, Loss: 1.9072
Batch 125, Loss: 1.9030
Batch 150, Loss: 1.8989
Batch 175, Loss: 1.8948
Noise applied in 2451 out of 3456 batches, 70.92
Epoch 18 learning rate: 0.01
Epoch 18 time: 354.8133707046509 seconds
Epoch 18 accuracy: 10.46%
Batch 25, Loss: 1.8880
Batch 50, Loss: 1.8841
Batch 75, Loss: 1.8802
Batch 100, Loss: 1.8763
Batch 125, Loss: 1.8726
Batch 150, Loss: 1.8688
Batch 175, Loss: 1.8650
Noise applied in 2643 out of 3648 batches, 72.45
Epoch 19 learning rate: 0.01
Epoch 19 time: 355.94958567619324 seconds
Epoch 19 accuracy: 10.59%
Batch 25, Loss: 1.8587
Batch 50, Loss: 1.8551
Batch 75, Loss: 1.8516
Batch 100, Loss: 1.8481
Batch 125, Loss: 1.8446
Batch 150, Loss: 1.8413
Batch 175, Loss: 1.8380
Noise applied in 2835 out of 3840 batches, 73.83
Epoch 20 learning rate: 0.01
Epoch 20 time: 355.16560769081116 seconds
Epoch 20 accuracy: 10.47%
Batch 25, Loss: 1.8326
Batch 50, Loss: 1.8294
Batch 75, Loss: 1.8262
Batch 100, Loss: 1.8231
Batch 125, Loss: 1.8199
Batch 150, Loss: 1.8165
Batch 175, Loss: 1.8134
Noise applied in 3027 out of 4032 batches, 75.07
Epoch 21 learning rate: 0.01
Epoch 21 time: 355.32732129096985 seconds
Epoch 21 accuracy: 10.62%
Batch 25, Loss: 1.8081
Batch 50, Loss: 1.8051
Batch 75, Loss: 1.8021
Batch 100, Loss: 1.7993
Batch 125, Loss: 1.7967
Batch 150, Loss: 1.7941
Batch 175, Loss: 1.7915
Noise applied in 3219 out of 4224 batches, 76.21
Epoch 22 learning rate: 0.01
Epoch 22 time: 355.33237051963806 seconds
Epoch 22 accuracy: 10.87%
Batch 25, Loss: 1.7873
Batch 50, Loss: 1.7849
Batch 75, Loss: 1.7825
Batch 100, Loss: 1.7802
Batch 125, Loss: 1.7778
Batch 150, Loss: 1.7755
Batch 175, Loss: 1.7733
Noise applied in 3411 out of 4416 batches, 77.24
Epoch 23 learning rate: 0.01
Epoch 23 time: 370.1376647949219 seconds
Epoch 23 accuracy: 11.07%
Batch 25, Loss: 1.7696
Batch 50, Loss: 1.7675
Batch 75, Loss: 1.7653
Batch 100, Loss: 1.7633
Batch 125, Loss: 1.7613
Batch 150, Loss: 1.7592
Batch 175, Loss: 1.7570
Noise applied in 3603 out of 4608 batches, 78.19
Epoch 24 learning rate: 0.01
Epoch 24 time: 355.00925278663635 seconds
Epoch 24 accuracy: 10.92%
Batch 25, Loss: 1.7537
Batch 50, Loss: 1.7520
Batch 75, Loss: 1.7503
Batch 100, Loss: 1.7488
Batch 125, Loss: 1.7473
Batch 150, Loss: 1.7459
Batch 175, Loss: 1.7445
Noise applied in 3795 out of 4800 batches, 79.06
Epoch 25 learning rate: 0.01
Epoch 25 time: 355.76887226104736 seconds
Epoch 25 accuracy: 11.1%
Batch 25, Loss: 1.7424
Batch 50, Loss: 1.7411
Batch 75, Loss: 1.7399
Batch 100, Loss: 1.7388
Batch 125, Loss: 1.7378
Batch 150, Loss: 1.7367
Batch 175, Loss: 1.7357
Noise applied in 3987 out of 4992 batches, 79.87
Epoch 26 learning rate: 0.01
Epoch 26 time: 355.40623569488525 seconds
Epoch 26 accuracy: 11.39%
Batch 25, Loss: 1.7342
Batch 50, Loss: 1.7333
Batch 75, Loss: 1.7325
Batch 100, Loss: 1.7317
Batch 125, Loss: 1.7310
Batch 150, Loss: 1.7303
Batch 175, Loss: 1.7296
Noise applied in 4179 out of 5184 batches, 80.61
Epoch 27 learning rate: 0.01
Epoch 27 time: 355.07528376579285 seconds
Epoch 27 accuracy: 11.84%
Batch 25, Loss: 1.7286
Batch 50, Loss: 1.7280
Batch 75, Loss: 1.7275
Batch 100, Loss: 1.7270
Batch 125, Loss: 1.7265
Batch 150, Loss: 1.7260
Batch 175, Loss: 1.7255
Noise applied in 4371 out of 5376 batches, 81.31
Epoch 28 learning rate: 0.01
Epoch 28 time: 355.26810240745544 seconds
Epoch 28 accuracy: 12.03%
Batch 25, Loss: 1.7247
Batch 50, Loss: 1.7243
Batch 75, Loss: 1.7238
Batch 100, Loss: 1.7233
Batch 125, Loss: 1.7228
Batch 150, Loss: 1.7223
Batch 175, Loss: 1.7217
Noise applied in 4563 out of 5568 batches, 81.95
Epoch 29 learning rate: 0.01
Epoch 29 time: 354.5687692165375 seconds
Epoch 29 accuracy: 12.06%
Batch 25, Loss: 1.7208
Batch 50, Loss: 1.7203
Batch 75, Loss: 1.7197
Batch 100, Loss: 1.7191
Batch 125, Loss: 1.7186
Batch 150, Loss: 1.7180
Batch 175, Loss: 1.7173
Noise applied in 4755 out of 5760 batches, 82.55
Epoch 30 learning rate: 0.01
Epoch 30 time: 355.1131248474121 seconds
Epoch 30 accuracy: 14.68%
Batch 25, Loss: 1.7163
Batch 50, Loss: 1.7158
Batch 75, Loss: 1.7154
Batch 100, Loss: 1.7150
Batch 125, Loss: 1.7147
Batch 150, Loss: 1.7143
Batch 175, Loss: 1.7140
Noise applied in 4947 out of 5952 batches, 83.11
Epoch 31 learning rate: 0.01
Epoch 31 time: 355.07694697380066 seconds
Epoch 31 accuracy: 14.81%
Batch 25, Loss: 1.7134
Batch 50, Loss: 1.7131
Batch 75, Loss: 1.7128
Batch 100, Loss: 1.7125
Batch 125, Loss: 1.7122
Batch 150, Loss: 1.7120
Batch 175, Loss: 1.7117
Noise applied in 5139 out of 6144 batches, 83.64
Epoch 32 learning rate: 0.01
Epoch 32 time: 354.9547734260559 seconds
Epoch 32 accuracy: 14.84%
Batch 25, Loss: 1.7113
Batch 50, Loss: 1.7111
Batch 75, Loss: 1.7109
Batch 100, Loss: 1.7106
Batch 125, Loss: 1.7104
Batch 150, Loss: 1.7102
Batch 175, Loss: 1.7100
Noise applied in 5331 out of 6336 batches, 84.14
Epoch 33 learning rate: 0.01
Epoch 33 time: 354.87208580970764 seconds
Epoch 33 accuracy: 14.88%
Batch 25, Loss: 1.7097
Batch 50, Loss: 1.7096
Batch 75, Loss: 1.7094
Batch 100, Loss: 1.7092
Batch 125, Loss: 1.7091
Batch 150, Loss: 1.7089
Batch 175, Loss: 1.7088
Noise applied in 5523 out of 6528 batches, 84.60
Epoch 34 learning rate: 0.01
Epoch 34 time: 354.792542219162 seconds
Epoch 34 accuracy: 14.91%
Batch 25, Loss: 1.7086
Batch 50, Loss: 1.7084
Batch 75, Loss: 1.7083
Batch 100, Loss: 1.7082
Batch 125, Loss: 1.7081
Batch 150, Loss: 1.7080
Batch 175, Loss: 1.7078
Noise applied in 5715 out of 6720 batches, 85.04
Epoch 35 learning rate: 0.01
Epoch 35 time: 356.0640106201172 seconds
Epoch 35 accuracy: 15.06%
Batch 25, Loss: 1.7077
Batch 50, Loss: 1.7076
Batch 75, Loss: 1.7075
Batch 100, Loss: 1.7074
Batch 125, Loss: 1.7073
Batch 150, Loss: 1.7072
Batch 175, Loss: 1.7071
Noise applied in 5907 out of 6912 batches, 85.46
Epoch 36 learning rate: 0.01
Epoch 36 time: 354.73262882232666 seconds
Epoch 36 accuracy: 15.15%
Batch 25, Loss: 1.7070
Batch 50, Loss: 1.7069
Batch 75, Loss: 1.7068
Batch 100, Loss: 1.7067
Batch 125, Loss: 1.7066
Batch 150, Loss: 1.7066
Batch 175, Loss: 1.7065
Noise applied in 6099 out of 7104 batches, 85.85
Epoch 37 learning rate: 0.01
Epoch 37 time: 354.87114810943604 seconds
Epoch 37 accuracy: 15.22%
Batch 25, Loss: 1.7064
Batch 50, Loss: 1.7063
Batch 75, Loss: 1.7062
Batch 100, Loss: 1.7062
Batch 125, Loss: 1.7061
Batch 150, Loss: 1.7061
Batch 175, Loss: 1.7060
Noise applied in 6291 out of 7296 batches, 86.23
Epoch 38 learning rate: 0.01
Epoch 38 time: 354.57880759239197 seconds
Epoch 38 accuracy: 15.19%
Batch 25, Loss: 1.7059
Batch 50, Loss: 1.7058
Batch 75, Loss: 1.7058
Batch 100, Loss: 1.7057
Batch 125, Loss: 1.7057
Batch 150, Loss: 1.7056
Batch 175, Loss: 1.7056
Noise applied in 6483 out of 7488 batches, 86.58
Epoch 39 learning rate: 0.01
Epoch 39 time: 355.6807425022125 seconds
Epoch 39 accuracy: 15.24%
Batch 25, Loss: 1.7055
Batch 50, Loss: 1.7054
Batch 75, Loss: 1.7054
Batch 100, Loss: 1.7053
Batch 125, Loss: 1.7053
Batch 150, Loss: 1.7052
Batch 175, Loss: 1.7052
Noise applied in 6675 out of 7680 batches, 86.91
Epoch 40 learning rate: 0.01
Epoch 40 time: 355.22205781936646 seconds
Epoch 40 accuracy: 15.17%
Batch 25, Loss: 1.7051
Batch 50, Loss: 1.7051
Batch 75, Loss: 1.7050
Batch 100, Loss: 1.7050
Batch 125, Loss: 1.7049
Batch 150, Loss: 1.7049
Batch 175, Loss: 1.7048
Noise applied in 6867 out of 7872 batches, 87.23
Epoch 41 learning rate: 0.01
Epoch 41 time: 355.3127546310425 seconds
Epoch 41 accuracy: 15.24%
Batch 25, Loss: 1.7048
Batch 50, Loss: 1.7047
Batch 75, Loss: 1.7047
Batch 100, Loss: 1.7047
Batch 125, Loss: 1.7046
Batch 150, Loss: 1.7046
Batch 175, Loss: 1.7045
Noise applied in 7059 out of 8064 batches, 87.54
Epoch 42 learning rate: 0.01
Epoch 42 time: 355.1601278781891 seconds
Epoch 42 accuracy: 15.22%
Batch 25, Loss: 1.7045
Batch 50, Loss: 1.7045
Batch 75, Loss: 1.7044
Batch 100, Loss: 1.7044
Batch 125, Loss: 1.7043
Batch 150, Loss: 1.7043
Batch 175, Loss: 1.7043
Noise applied in 7251 out of 8256 batches, 87.83
Epoch 43 learning rate: 0.01
Epoch 43 time: 355.15162324905396 seconds
Epoch 43 accuracy: 15.21%
Batch 25, Loss: 1.7042
Batch 50, Loss: 1.7042
Batch 75, Loss: 1.7041
Batch 100, Loss: 1.7041
Batch 125, Loss: 1.7041
Batch 150, Loss: 1.7040
Batch 175, Loss: 1.7040
Noise applied in 7443 out of 8448 batches, 88.10
Epoch 44 learning rate: 0.01
Epoch 44 time: 354.6463668346405 seconds
Epoch 44 accuracy: 15.22%
Batch 25, Loss: 1.7040
Batch 50, Loss: 1.7039
Batch 75, Loss: 1.7039
Batch 100, Loss: 1.7039
Batch 125, Loss: 1.7038
Batch 150, Loss: 1.7038
Batch 175, Loss: 1.7038
Noise applied in 7635 out of 8640 batches, 88.37
Epoch 45 learning rate: 0.01
Epoch 45 time: 355.1181116104126 seconds
Epoch 45 accuracy: 15.19%
Batch 25, Loss: 1.7037
Batch 50, Loss: 1.7037
Batch 75, Loss: 1.7037
Batch 100, Loss: 1.7036
Batch 125, Loss: 1.7036
Batch 150, Loss: 1.7036
Batch 175, Loss: 1.7036
Noise applied in 7827 out of 8832 batches, 88.62
Epoch 46 learning rate: 0.01
Epoch 46 time: 356.23456382751465 seconds
Epoch 46 accuracy: 15.21%
Batch 25, Loss: 1.7035
Batch 50, Loss: 1.7035
Batch 75, Loss: 1.7035
Batch 100, Loss: 1.7035
Batch 125, Loss: 1.7035
Batch 150, Loss: 1.7034
Batch 175, Loss: 1.7034
Noise applied in 8019 out of 9024 batches, 88.86
Epoch 47 learning rate: 0.01
Epoch 47 time: 354.75949692726135 seconds
Epoch 47 accuracy: 15.27%
Batch 25, Loss: 1.7034
Batch 50, Loss: 1.7034
Batch 75, Loss: 1.7033
Batch 100, Loss: 1.7033
Batch 125, Loss: 1.7033
Batch 150, Loss: 1.7033
Batch 175, Loss: 1.7033
Noise applied in 8211 out of 9216 batches, 89.10
Epoch 48 learning rate: 0.01
Epoch 48 time: 354.7362675666809 seconds
Epoch 48 accuracy: 15.22%
Batch 25, Loss: 1.7032
Batch 50, Loss: 1.7032
Batch 75, Loss: 1.7032
Batch 100, Loss: 1.7032
Batch 125, Loss: 1.7032
Batch 150, Loss: 1.7031
Batch 175, Loss: 1.7031
Noise applied in 8403 out of 9408 batches, 89.32
Epoch 49 learning rate: 0.01
Epoch 49 time: 355.5152735710144 seconds
Epoch 49 accuracy: 15.23%
Batch 25, Loss: 1.7031
Batch 50, Loss: 1.7031
Batch 75, Loss: 1.7031
Batch 100, Loss: 1.7030
Batch 125, Loss: 1.7030
Batch 150, Loss: 1.7030
Batch 175, Loss: 1.7030
Noise applied in 8595 out of 9600 batches, 89.53
Epoch 50 learning rate: 0.01
Epoch 50 time: 355.8327372074127 seconds
Epoch 50 accuracy: 15.25%
Batch 25, Loss: 1.7030
Batch 50, Loss: 1.7029
Batch 75, Loss: 1.7029
Batch 100, Loss: 1.7029
Batch 125, Loss: 1.7029
Batch 150, Loss: 1.7029
Batch 175, Loss: 1.7029
Noise applied in 8787 out of 9792 batches, 89.74
Epoch 51 learning rate: 0.01
Epoch 51 time: 357.3079357147217 seconds
Epoch 51 accuracy: 15.23%
Batch 25, Loss: 1.7028
Batch 50, Loss: 1.7028
Batch 75, Loss: 1.7028
Batch 100, Loss: 1.7028
Batch 125, Loss: 1.7028
Batch 150, Loss: 1.7027
Batch 175, Loss: 1.7027
Noise applied in 8979 out of 9984 batches, 89.93
Epoch 52 learning rate: 0.01
Epoch 52 time: 355.33115339279175 seconds
Epoch 52 accuracy: 15.25%
Batch 25, Loss: 1.7027
Batch 50, Loss: 1.7027
Batch 75, Loss: 1.7027
Batch 100, Loss: 1.7026
Batch 125, Loss: 1.7026
Batch 150, Loss: 1.7026
Batch 175, Loss: 1.7026
Noise applied in 9171 out of 10176 batches, 90.12
Epoch 53 learning rate: 0.01
Epoch 53 time: 355.42687582969666 seconds
Epoch 53 accuracy: 15.3%
Batch 25, Loss: 1.7026
Batch 50, Loss: 1.7026
Batch 75, Loss: 1.7026
Batch 100, Loss: 1.7026
Batch 125, Loss: 1.7026
Batch 150, Loss: 1.7025
Batch 175, Loss: 1.7025
Noise applied in 9363 out of 10368 batches, 90.31
Epoch 54 learning rate: 0.01
Epoch 54 time: 355.406982421875 seconds
Epoch 54 accuracy: 15.3%
Batch 25, Loss: 1.7025
Batch 50, Loss: 1.7025
Batch 75, Loss: 1.7024
Batch 100, Loss: 1.7024
Batch 125, Loss: 1.7024
Batch 150, Loss: 1.7024
Batch 175, Loss: 1.7024
Noise applied in 9555 out of 10560 batches, 90.48
Epoch 55 learning rate: 0.01
Epoch 55 time: 355.65284299850464 seconds
Epoch 55 accuracy: 15.3%
Batch 25, Loss: 1.7024
Batch 50, Loss: 1.7023
Batch 75, Loss: 1.7023
Batch 100, Loss: 1.7023
Batch 125, Loss: 1.7023
Batch 150, Loss: 1.7023
Batch 175, Loss: 1.7023
Noise applied in 9747 out of 10752 batches, 90.65
Epoch 56 learning rate: 0.01
Epoch 56 time: 355.2750504016876 seconds
Epoch 56 accuracy: 15.39%
Batch 25, Loss: 1.7022
Batch 50, Loss: 1.7022
Batch 75, Loss: 1.7022
Batch 100, Loss: 1.7022
Batch 125, Loss: 1.7021
Batch 150, Loss: 1.7021
Batch 175, Loss: 1.7021
Noise applied in 9939 out of 10944 batches, 90.82
Epoch 57 learning rate: 0.01
Epoch 57 time: 354.869398355484 seconds
Epoch 57 accuracy: 15.39%
Batch 25, Loss: 1.7021
Batch 50, Loss: 1.7021
Batch 75, Loss: 1.7020
Batch 100, Loss: 1.7020
Batch 125, Loss: 1.7020
Batch 150, Loss: 1.7020
Batch 175, Loss: 1.7020
Noise applied in 10131 out of 11136 batches, 90.98
Epoch 58 learning rate: 0.01
Epoch 58 time: 355.778689622879 seconds
Epoch 58 accuracy: 15.36%
Batch 25, Loss: 1.7019
Batch 50, Loss: 1.7019
Batch 75, Loss: 1.7019
Batch 100, Loss: 1.7019
Batch 125, Loss: 1.7019
Batch 150, Loss: 1.7018
Batch 175, Loss: 1.7018
Noise applied in 10323 out of 11328 batches, 91.13
Epoch 59 learning rate: 0.01
Epoch 59 time: 355.1508414745331 seconds
Epoch 59 accuracy: 15.37%
Batch 25, Loss: 1.7018
Batch 50, Loss: 1.7018
Batch 75, Loss: 1.7018
Batch 100, Loss: 1.7017
Batch 125, Loss: 1.7017
Batch 150, Loss: 1.7017
Batch 175, Loss: 1.7017
Noise applied in 10515 out of 11520 batches, 91.28
Epoch 60 learning rate: 0.01
Epoch 60 time: 355.7208285331726 seconds
Epoch 60 accuracy: 15.37%
Batch 25, Loss: 1.7017
Batch 50, Loss: 1.7016
Batch 75, Loss: 1.7016
Batch 100, Loss: 1.7016
Batch 125, Loss: 1.7016
Batch 150, Loss: 1.7016
Batch 175, Loss: 1.7016
Noise applied in 10707 out of 11712 batches, 91.42
Epoch 61 learning rate: 0.01
Epoch 61 time: 354.96479630470276 seconds
Epoch 61 accuracy: 15.35%
Batch 25, Loss: 1.7015
Batch 50, Loss: 1.7015
Batch 75, Loss: 1.7015
Batch 100, Loss: 1.7015
Batch 125, Loss: 1.7015
Batch 150, Loss: 1.7015
Batch 175, Loss: 1.7015
Noise applied in 10899 out of 11904 batches, 91.56
Epoch 62 learning rate: 0.01
Epoch 62 time: 483.99637842178345 seconds
Epoch 62 accuracy: 15.45%
Batch 25, Loss: 1.7015
Batch 50, Loss: 1.7015
Batch 75, Loss: 1.7015
Batch 100, Loss: 1.7015
Batch 125, Loss: 1.7015
Batch 150, Loss: 1.7015
Batch 175, Loss: 1.7015
Noise applied in 11091 out of 12096 batches, 91.69
Epoch 63 learning rate: 0.01
Epoch 63 time: 384.05311465263367 seconds
Epoch 63 accuracy: 15.4%
Batch 25, Loss: 1.7015
Batch 50, Loss: 1.7015
Batch 75, Loss: 1.7014
Batch 100, Loss: 1.7014
Batch 125, Loss: 1.7014
Batch 150, Loss: 1.7014
Batch 175, Loss: 1.7014
Noise applied in 11283 out of 12288 batches, 91.82
Epoch 64 learning rate: 0.01
Epoch 64 time: 364.4347114562988 seconds
Epoch 64 accuracy: 15.41%
Batch 25, Loss: 1.7014
Batch 50, Loss: 1.7014
Batch 75, Loss: 1.7014
Batch 100, Loss: 1.7014
Batch 125, Loss: 1.7014
Batch 150, Loss: 1.7014
Batch 175, Loss: 1.7014
Noise applied in 11475 out of 12480 batches, 91.95
Epoch 65 learning rate: 0.01
Epoch 65 time: 363.73060369491577 seconds
Epoch 65 accuracy: 15.46%
Batch 25, Loss: 1.7014
Batch 50, Loss: 1.7014
Batch 75, Loss: 1.7014
Batch 100, Loss: 1.7014
Batch 125, Loss: 1.7014
Batch 150, Loss: 1.7014
Batch 175, Loss: 1.7014
Noise applied in 11667 out of 12672 batches, 92.07
Epoch 66 learning rate: 0.01
Epoch 66 time: 374.2213008403778 seconds
Epoch 66 accuracy: 15.5%
Batch 25, Loss: 1.7014
Batch 50, Loss: 1.7014
Batch 75, Loss: 1.7014
Batch 100, Loss: 1.7014
Batch 125, Loss: 1.7014
Batch 150, Loss: 1.7014
Batch 175, Loss: 1.7014
Noise applied in 11859 out of 12864 batches, 92.19
Epoch 67 learning rate: 0.01
Epoch 67 time: 378.47155141830444 seconds
Epoch 67 accuracy: 15.57%
Batch 25, Loss: 1.7014
Batch 50, Loss: 1.7014
Batch 75, Loss: 1.7014
Batch 100, Loss: 1.7014
Batch 125, Loss: 1.7014
Batch 150, Loss: 1.7014
Batch 175, Loss: 1.7014
Noise applied in 12051 out of 13056 batches, 92.30
Epoch 68 learning rate: 0.01
Epoch 68 time: 376.45875096321106 seconds
Epoch 68 accuracy: 15.53%
Batch 25, Loss: 1.7014
Batch 50, Loss: 1.7014
Batch 75, Loss: 1.7015
Batch 100, Loss: 1.7015
Batch 125, Loss: 1.7015
Batch 150, Loss: 1.7015
Batch 175, Loss: 1.7015
Noise applied in 12243 out of 13248 batches, 92.41
Epoch 69 learning rate: 0.01
Epoch 69 time: 379.6211049556732 seconds
Epoch 69 accuracy: 15.57%
Batch 25, Loss: 1.7015
Batch 50, Loss: 1.7015
Batch 75, Loss: 1.7015
Batch 100, Loss: 1.7015
Batch 125, Loss: 1.7015
Batch 150, Loss: 1.7015
Batch 175, Loss: 1.7015
Noise applied in 12435 out of 13440 batches, 92.52
Epoch 70 learning rate: 0.01
Epoch 70 time: 422.6500885486603 seconds
Epoch 70 accuracy: 15.56%
Batch 25, Loss: 1.7015
Batch 50, Loss: 1.7016
Batch 75, Loss: 1.7016
Batch 100, Loss: 1.7016
Batch 125, Loss: 1.7016
Batch 150, Loss: 1.7016
Batch 175, Loss: 1.7016
Noise applied in 12627 out of 13632 batches, 92.63
Epoch 71 learning rate: 0.01
Epoch 71 time: 469.84614729881287 seconds
Epoch 71 accuracy: 15.56%
Batch 25, Loss: 1.7016
Batch 50, Loss: 1.7016
Batch 75, Loss: 1.7016
Batch 100, Loss: 1.7017
Batch 125, Loss: 1.7017
Batch 150, Loss: 1.7017
Batch 175, Loss: 1.7017
Noise applied in 12819 out of 13824 batches, 92.73
Epoch 72 learning rate: 0.01
Epoch 72 time: 369.9493865966797 seconds
Epoch 72 accuracy: 15.56%
Batch 25, Loss: 1.7017
Batch 50, Loss: 1.7017
Batch 75, Loss: 1.7017
Batch 100, Loss: 1.7017
Batch 125, Loss: 1.7018
Batch 150, Loss: 1.7018
Batch 175, Loss: 1.7018
Noise applied in 13011 out of 14016 batches, 92.83
Epoch 73 learning rate: 0.01
Epoch 73 time: 429.2298860549927 seconds
Epoch 73 accuracy: 15.52%
Batch 25, Loss: 1.7018
Batch 50, Loss: 1.7018
Batch 75, Loss: 1.7018
Batch 100, Loss: 1.7018
Batch 125, Loss: 1.7019
Batch 150, Loss: 1.7019
Batch 175, Loss: 1.7019
Noise applied in 13203 out of 14208 batches, 92.93
Epoch 74 learning rate: 0.01
Epoch 74 time: 383.3408770561218 seconds
Epoch 74 accuracy: 15.53%
Batch 25, Loss: 1.7019
Batch 50, Loss: 1.7019
Batch 75, Loss: 1.7019
Batch 100, Loss: 1.7019
Batch 125, Loss: 1.7019
Batch 150, Loss: 1.7020
Batch 175, Loss: 1.7020
Noise applied in 13395 out of 14400 batches, 93.02
Epoch 75 learning rate: 0.01
Epoch 75 time: 477.4936270713806 seconds
Epoch 75 accuracy: 15.56%
Batch 25, Loss: 1.7020
Batch 50, Loss: 1.7020
Batch 75, Loss: 1.7020
Batch 100, Loss: 1.7020
Batch 125, Loss: 1.7021
Batch 150, Loss: 1.7021
Batch 175, Loss: 1.7021
Noise applied in 13587 out of 14592 batches, 93.11
Epoch 76 learning rate: 0.01
Epoch 76 time: 378.5462522506714 seconds
Epoch 76 accuracy: 15.57%
Batch 25, Loss: 1.7021
Batch 50, Loss: 1.7021
Batch 75, Loss: 1.7022
Batch 100, Loss: 1.7022
Batch 125, Loss: 1.7022
Batch 150, Loss: 1.7022
Batch 175, Loss: 1.7022
Noise applied in 13779 out of 14784 batches, 93.20
Epoch 77 learning rate: 0.01
Epoch 77 time: 377.6440830230713 seconds
Epoch 77 accuracy: 15.52%
Batch 25, Loss: 1.7023
Batch 50, Loss: 1.7023
Batch 75, Loss: 1.7023
Batch 100, Loss: 1.7023
Batch 125, Loss: 1.7023
Batch 150, Loss: 1.7024
Batch 175, Loss: 1.7024
Noise applied in 13971 out of 14976 batches, 93.29
Epoch 78 learning rate: 0.01
Epoch 78 time: 391.712149143219 seconds
Epoch 78 accuracy: 15.53%
Batch 25, Loss: 1.7024
Batch 50, Loss: 1.7024
Batch 75, Loss: 1.7024
Batch 100, Loss: 1.7025
Batch 125, Loss: 1.7025
Batch 150, Loss: 1.7025
Batch 175, Loss: 1.7025
Noise applied in 14163 out of 15168 batches, 93.37
Epoch 79 learning rate: 0.01
Epoch 79 time: 403.59840750694275 seconds
Epoch 79 accuracy: 15.59%
Batch 25, Loss: 1.7025
Batch 50, Loss: 1.7026
Batch 75, Loss: 1.7026
Batch 100, Loss: 1.7026
Batch 125, Loss: 1.7026
Batch 150, Loss: 1.7026
Batch 175, Loss: 1.7027
Noise applied in 14355 out of 15360 batches, 93.46
Epoch 80 learning rate: 0.01
Epoch 80 time: 380.3017547130585 seconds
Epoch 80 accuracy: 15.61%
Batch 25, Loss: 1.7027
Batch 50, Loss: 1.7027
Batch 75, Loss: 1.7027
Batch 100, Loss: 1.7028
Batch 125, Loss: 1.7028
Batch 150, Loss: 1.7028
Batch 175, Loss: 1.7028
Noise applied in 14547 out of 15552 batches, 93.54
Epoch 81 learning rate: 0.01
Epoch 81 time: 453.80827808380127 seconds
Epoch 81 accuracy: 15.61%
Batch 25, Loss: 1.7029
Batch 50, Loss: 1.7029
Batch 75, Loss: 1.7029
Batch 100, Loss: 1.7029
Batch 125, Loss: 1.7029
Batch 150, Loss: 1.7030
Batch 175, Loss: 1.7030
Noise applied in 14739 out of 15744 batches, 93.62
Epoch 82 learning rate: 0.01
Epoch 82 time: 403.104199886322 seconds
Epoch 82 accuracy: 15.63%
Batch 25, Loss: 1.7030
Batch 50, Loss: 1.7030
Batch 75, Loss: 1.7030
Batch 100, Loss: 1.7031
Batch 125, Loss: 1.7031
Batch 150, Loss: 1.7031
Batch 175, Loss: 1.7031
Noise applied in 14931 out of 15936 batches, 93.69
Epoch 83 learning rate: 0.01
Epoch 83 time: 363.06498169898987 seconds
Epoch 83 accuracy: 15.66%
Batch 25, Loss: 1.7032
Batch 50, Loss: 1.7032
Batch 75, Loss: 1.7032
Batch 100, Loss: 1.7032
Batch 125, Loss: 1.7032
Batch 150, Loss: 1.7033
Batch 175, Loss: 1.7033
Noise applied in 15123 out of 16128 batches, 93.77
Epoch 84 learning rate: 0.01
Epoch 84 time: 387.156396150589 seconds
Epoch 84 accuracy: 15.68%
Batch 25, Loss: 1.7033
Batch 50, Loss: 1.7033
Batch 75, Loss: 1.7034
Batch 100, Loss: 1.7034
Batch 125, Loss: 1.7034
Batch 150, Loss: 1.7034
Batch 175, Loss: 1.7034
Noise applied in 15315 out of 16320 batches, 93.84
Epoch 85 learning rate: 0.01
Epoch 85 time: 375.83666491508484 seconds
Epoch 85 accuracy: 15.69%
Batch 25, Loss: 1.7035
Batch 50, Loss: 1.7035
Batch 75, Loss: 1.7035
Batch 100, Loss: 1.7035
Batch 125, Loss: 1.7036
Batch 150, Loss: 1.7036
Batch 175, Loss: 1.7036
Noise applied in 15507 out of 16512 batches, 93.91
Epoch 86 learning rate: 0.01
Epoch 86 time: 372.64813137054443 seconds
Epoch 86 accuracy: 15.69%
Batch 25, Loss: 1.7036
Batch 50, Loss: 1.7037
Batch 75, Loss: 1.7037
Batch 100, Loss: 1.7037
Batch 125, Loss: 1.7037
Batch 150, Loss: 1.7037
Batch 175, Loss: 1.7038
Noise applied in 15699 out of 16704 batches, 93.98
Epoch 87 learning rate: 0.01
Epoch 87 time: 625.3091986179352 seconds
Epoch 87 accuracy: 15.71%
Batch 25, Loss: 1.7038
Batch 50, Loss: 1.7038
Batch 75, Loss: 1.7038
Batch 100, Loss: 1.7039
Batch 125, Loss: 1.7039
Batch 150, Loss: 1.7039
Batch 175, Loss: 1.7039
Noise applied in 15891 out of 16896 batches, 94.05
Epoch 88 learning rate: 0.01
Epoch 88 time: 368.46231722831726 seconds
Epoch 88 accuracy: 15.7%
Batch 25, Loss: 1.7040
Batch 50, Loss: 1.7040
Batch 75, Loss: 1.7040
Batch 100, Loss: 1.7040
Batch 125, Loss: 1.7041
Batch 150, Loss: 1.7041
Batch 175, Loss: 1.7041
Noise applied in 16083 out of 17088 batches, 94.12
Epoch 89 learning rate: 0.01
Epoch 89 time: 368.1170196533203 seconds
Epoch 89 accuracy: 15.69%
Batch 25, Loss: 1.7041
Batch 50, Loss: 1.7042
Batch 75, Loss: 1.7042
Batch 100, Loss: 1.7042
Batch 125, Loss: 1.7042
Batch 150, Loss: 1.7043
Batch 175, Loss: 1.7043
Noise applied in 16275 out of 17280 batches, 94.18
Epoch 90 learning rate: 0.01
Epoch 90 time: 396.1265916824341 seconds
Epoch 90 accuracy: 15.71%
Batch 25, Loss: 1.7043
Batch 50, Loss: 1.7043
Batch 75, Loss: 1.7044
Batch 100, Loss: 1.7044
Batch 125, Loss: 1.7044
Batch 150, Loss: 1.7044
Batch 175, Loss: 1.7044
Noise applied in 16467 out of 17472 batches, 94.25
Epoch 91 learning rate: 0.01
Epoch 91 time: 371.08051228523254 seconds
Epoch 91 accuracy: 15.71%
Batch 25, Loss: 1.7045
Batch 50, Loss: 1.7045
Batch 75, Loss: 1.7045
Batch 100, Loss: 1.7045
Batch 125, Loss: 1.7046
Batch 150, Loss: 1.7046
Batch 175, Loss: 1.7046
Noise applied in 16659 out of 17664 batches, 94.31
Epoch 92 learning rate: 0.01
Epoch 92 time: 385.5887587070465 seconds
Epoch 92 accuracy: 15.67%
Batch 25, Loss: 1.7047
Batch 50, Loss: 1.7047
Batch 75, Loss: 1.7047
Batch 100, Loss: 1.7047
Batch 125, Loss: 1.7047
Batch 150, Loss: 1.7048
Batch 175, Loss: 1.7048
Noise applied in 16851 out of 17856 batches, 94.37
Epoch 93 learning rate: 0.01
Epoch 93 time: 382.67015528678894 seconds
Epoch 93 accuracy: 15.69%
Batch 25, Loss: 1.7048
Batch 50, Loss: 1.7049
Batch 75, Loss: 1.7049
Batch 100, Loss: 1.7049
Batch 125, Loss: 1.7049
Batch 150, Loss: 1.7050
Batch 175, Loss: 1.7050
Noise applied in 17043 out of 18048 batches, 94.43
Epoch 94 learning rate: 0.01
Epoch 94 time: 394.46174240112305 seconds
Epoch 94 accuracy: 15.68%
Batch 25, Loss: 1.7050
Batch 50, Loss: 1.7050
Batch 75, Loss: 1.7051
Batch 100, Loss: 1.7051
Batch 125, Loss: 1.7051
Batch 150, Loss: 1.7051
Batch 175, Loss: 1.7052
Noise applied in 17235 out of 18240 batches, 94.49
Epoch 95 learning rate: 0.01
Epoch 95 time: 362.81268882751465 seconds
Epoch 95 accuracy: 15.68%
Batch 25, Loss: 1.7052
Batch 50, Loss: 1.7052
Batch 75, Loss: 1.7053
Batch 100, Loss: 1.7053
Batch 125, Loss: 1.7053
Batch 150, Loss: 1.7053
Batch 175, Loss: 1.7053
Noise applied in 17427 out of 18432 batches, 94.55
Epoch 96 learning rate: 0.01
Epoch 96 time: 376.97431659698486 seconds
Epoch 96 accuracy: 15.65%
Batch 25, Loss: 1.7054
Batch 50, Loss: 1.7054
Batch 75, Loss: 1.7054
Batch 100, Loss: 1.7055
Batch 125, Loss: 1.7055
Batch 150, Loss: 1.7055
Batch 175, Loss: 1.7055
Noise applied in 17619 out of 18624 batches, 94.60
Epoch 97 learning rate: 0.01
Epoch 97 time: 368.73801732063293 seconds
Epoch 97 accuracy: 15.63%
Batch 25, Loss: 1.7056
Batch 50, Loss: 1.7056
Batch 75, Loss: 1.7056
Batch 100, Loss: 1.7056
Batch 125, Loss: 1.7056
Batch 150, Loss: 1.7057
Batch 175, Loss: 1.7057
Noise applied in 17811 out of 18816 batches, 94.66
Epoch 98 learning rate: 0.01
Epoch 98 time: 361.9737205505371 seconds
Epoch 98 accuracy: 15.62%
Batch 25, Loss: 1.7057
Batch 50, Loss: 1.7057
Batch 75, Loss: 1.7058
Batch 100, Loss: 1.7058
Batch 125, Loss: 1.7058
Batch 150, Loss: 1.7058
Batch 175, Loss: 1.7059
Noise applied in 18003 out of 19008 batches, 94.71
Epoch 99 learning rate: 0.01
Epoch 99 time: 377.88547372817993 seconds
Epoch 99 accuracy: 15.62%
Batch 25, Loss: 1.7059
Batch 50, Loss: 1.7059
Batch 75, Loss: 1.7059
Batch 100, Loss: 1.7060
Batch 125, Loss: 1.7060
Batch 150, Loss: 1.7060
Batch 175, Loss: 1.7060
Noise applied in 18195 out of 19200 batches, 94.77
Epoch 100 learning rate: 0.01
Epoch 100 time: 366.59803557395935 seconds
Epoch 100 accuracy: 15.6%
Batch 25, Loss: 1.7061
Batch 50, Loss: 1.7061
Batch 75, Loss: 1.7061
Batch 100, Loss: 1.7061
Batch 125, Loss: 1.7062
Batch 150, Loss: 1.7062
Batch 175, Loss: 1.7062
Noise applied in 18387 out of 19392 batches, 94.82
Epoch 101 learning rate: 0.01
Epoch 101 time: 355.12034368515015 seconds
Epoch 101 accuracy: 15.59%
Batch 25, Loss: 1.7063
Batch 50, Loss: 1.7063
Batch 75, Loss: 1.7063
Batch 100, Loss: 1.7063
Batch 125, Loss: 1.7063
Batch 150, Loss: 1.7064
Batch 175, Loss: 1.7064
Noise applied in 18579 out of 19584 batches, 94.87
Epoch 102 learning rate: 0.01
Epoch 102 time: 355.2921051979065 seconds
Epoch 102 accuracy: 15.6%
Batch 25, Loss: 1.7064
Batch 50, Loss: 1.7065
Batch 75, Loss: 1.7065
Batch 100, Loss: 1.7065
Batch 125, Loss: 1.7065
Batch 150, Loss: 1.7066
Batch 175, Loss: 1.7066
Noise applied in 18771 out of 19776 batches, 94.92
Epoch 103 learning rate: 0.01
Epoch 103 time: 356.358580827713 seconds
Epoch 103 accuracy: 15.58%
Batch 25, Loss: 1.7066
Batch 50, Loss: 1.7067
Batch 75, Loss: 1.7067
Batch 100, Loss: 1.7067
Batch 125, Loss: 1.7067
Batch 150, Loss: 1.7067
Batch 175, Loss: 1.7068
Noise applied in 18963 out of 19968 batches, 94.97
Epoch 104 learning rate: 0.01
Epoch 104 time: 355.84755778312683 seconds
Epoch 104 accuracy: 15.56%
Batch 25, Loss: 1.7068
Batch 50, Loss: 1.7068
Batch 75, Loss: 1.7069
Batch 100, Loss: 1.7069
Batch 125, Loss: 1.7069
Batch 150, Loss: 1.7069
Batch 175, Loss: 1.7070
Noise applied in 19155 out of 20160 batches, 95.01
Epoch 105 learning rate: 0.01
Epoch 105 time: 403.1188790798187 seconds
Epoch 105 accuracy: 15.54%
Batch 25, Loss: 1.7070
Batch 50, Loss: 1.7070
Batch 75, Loss: 1.7071
Batch 100, Loss: 1.7071
Batch 125, Loss: 1.7071
Batch 150, Loss: 1.7071
Batch 175, Loss: 1.7072
Noise applied in 19347 out of 20352 batches, 95.06
Epoch 106 learning rate: 0.01
Epoch 106 time: 379.33990478515625 seconds
Epoch 106 accuracy: 15.54%
Batch 25, Loss: 1.7072
Batch 50, Loss: 1.7072
Batch 75, Loss: 1.7073
Batch 100, Loss: 1.7073
Batch 125, Loss: 1.7073
Batch 150, Loss: 1.7074
Batch 175, Loss: 1.7074
Noise applied in 19539 out of 20544 batches, 95.11
Epoch 107 learning rate: 0.01
Epoch 107 time: 355.9233887195587 seconds
Epoch 107 accuracy: 15.54%
Batch 25, Loss: 1.7074
Batch 50, Loss: 1.7075
Batch 75, Loss: 1.7075
Batch 100, Loss: 1.7075
Batch 125, Loss: 1.7075
Batch 150, Loss: 1.7076
Batch 175, Loss: 1.7076
Noise applied in 19731 out of 20736 batches, 95.15
Epoch 108 learning rate: 0.01
Epoch 108 time: 355.3225932121277 seconds
Epoch 108 accuracy: 15.54%
Batch 25, Loss: 1.7076
Batch 50, Loss: 1.7077
Batch 75, Loss: 1.7077
Batch 100, Loss: 1.7077
Batch 125, Loss: 1.7078
Batch 150, Loss: 1.7078
Batch 175, Loss: 1.7078
Noise applied in 19923 out of 20928 batches, 95.20
Epoch 109 learning rate: 0.01
Epoch 109 time: 356.0524868965149 seconds
Epoch 109 accuracy: 15.54%
Batch 25, Loss: 1.7079
Batch 50, Loss: 1.7079
Batch 75, Loss: 1.7079
Batch 100, Loss: 1.7080
Batch 125, Loss: 1.7080
Batch 150, Loss: 1.7080
Batch 175, Loss: 1.7080
Noise applied in 20115 out of 21120 batches, 95.24
Epoch 110 learning rate: 0.01
Epoch 110 time: 355.56304597854614 seconds
Epoch 110 accuracy: 15.54%
Batch 25, Loss: 1.7081
Batch 50, Loss: 1.7081
Batch 75, Loss: 1.7082
Batch 100, Loss: 1.7082
Batch 125, Loss: 1.7082
Batch 150, Loss: 1.7082
Batch 175, Loss: 1.7083
Noise applied in 20307 out of 21312 batches, 95.28
Epoch 111 learning rate: 0.01
Epoch 111 time: 355.80926418304443 seconds
Epoch 111 accuracy: 15.55%
Batch 25, Loss: 1.7083
Batch 50, Loss: 1.7084
Batch 75, Loss: 1.7084
Batch 100, Loss: 1.7084
Batch 125, Loss: 1.7084
Batch 150, Loss: 1.7085
Batch 175, Loss: 1.7085
Noise applied in 20499 out of 21504 batches, 95.33
Epoch 112 learning rate: 0.01
Epoch 112 time: 355.42382550239563 seconds
Epoch 112 accuracy: 15.57%
Batch 25, Loss: 1.7086
Batch 50, Loss: 1.7086
Batch 75, Loss: 1.7086
Batch 100, Loss: 1.7087
Batch 125, Loss: 1.7087
Batch 150, Loss: 1.7087
Batch 175, Loss: 1.7088
Noise applied in 20691 out of 21696 batches, 95.37
Epoch 113 learning rate: 0.01
Epoch 113 time: 355.611154794693 seconds
Epoch 113 accuracy: 15.56%
Batch 25, Loss: 1.7088
Batch 50, Loss: 1.7088
Batch 75, Loss: 1.7089
Batch 100, Loss: 1.7089
Batch 125, Loss: 1.7089
Batch 150, Loss: 1.7090
Batch 175, Loss: 1.7090
Noise applied in 20883 out of 21888 batches, 95.41
Epoch 114 learning rate: 0.01
Epoch 114 time: 355.5143373012543 seconds
Epoch 114 accuracy: 15.56%
Batch 25, Loss: 1.7091
Batch 50, Loss: 1.7091
Batch 75, Loss: 1.7091
Batch 100, Loss: 1.7092
Batch 125, Loss: 1.7092
Batch 150, Loss: 1.7092
Batch 175, Loss: 1.7093
Noise applied in 21075 out of 22080 batches, 95.45
Epoch 115 learning rate: 0.01
Epoch 115 time: 355.6656057834625 seconds
Epoch 115 accuracy: 15.59%
Batch 25, Loss: 1.7093
Batch 50, Loss: 1.7094
Batch 75, Loss: 1.7094
Batch 100, Loss: 1.7094
Batch 125, Loss: 1.7095
Batch 150, Loss: 1.7095
Batch 175, Loss: 1.7096
Noise applied in 21267 out of 22272 batches, 95.49
Epoch 116 learning rate: 0.01
Epoch 116 time: 356.0036780834198 seconds
Epoch 116 accuracy: 15.61%
Batch 25, Loss: 1.7096
Batch 50, Loss: 1.7097
Batch 75, Loss: 1.7097
Batch 100, Loss: 1.7097
Batch 125, Loss: 1.7098
Batch 150, Loss: 1.7098
Batch 175, Loss: 1.7098
Noise applied in 21459 out of 22464 batches, 95.53
Epoch 117 learning rate: 0.01
Epoch 117 time: 354.9744658470154 seconds
Epoch 117 accuracy: 15.58%
Batch 25, Loss: 1.7099
Batch 50, Loss: 1.7099
Batch 75, Loss: 1.7100
Batch 100, Loss: 1.7100
Batch 125, Loss: 1.7100
Batch 150, Loss: 1.7101
Batch 175, Loss: 1.7101
Noise applied in 21651 out of 22656 batches, 95.56
Epoch 118 learning rate: 0.01
Epoch 118 time: 355.4117660522461 seconds
Epoch 118 accuracy: 15.61%
Batch 25, Loss: 1.7102
Batch 50, Loss: 1.7102
Batch 75, Loss: 1.7102
Batch 100, Loss: 1.7103
Batch 125, Loss: 1.7103
Batch 150, Loss: 1.7103
Batch 175, Loss: 1.7104
Noise applied in 21843 out of 22848 batches, 95.60
Epoch 119 learning rate: 0.01
Epoch 119 time: 354.7562162876129 seconds
Epoch 119 accuracy: 15.64%
Batch 25, Loss: 1.7104
Batch 50, Loss: 1.7105
Batch 75, Loss: 1.7105
Batch 100, Loss: 1.7105
Batch 125, Loss: 1.7106
Batch 150, Loss: 1.7106
Batch 175, Loss: 1.7107
Noise applied in 22035 out of 23040 batches, 95.64
Epoch 120 learning rate: 0.01
Epoch 120 time: 355.7593152523041 seconds
Epoch 120 accuracy: 15.65%
Batch 25, Loss: 1.7107
Batch 50, Loss: 1.7107
Batch 75, Loss: 1.7108
Batch 100, Loss: 1.7108
Batch 125, Loss: 1.7109
Batch 150, Loss: 1.7109
Batch 175, Loss: 1.7109
Noise applied in 22227 out of 23232 batches, 95.67
Epoch 121 learning rate: 0.01
Epoch 121 time: 356.1573963165283 seconds
Epoch 121 accuracy: 15.65%
Batch 25, Loss: 1.7110
Batch 50, Loss: 1.7110
Batch 75, Loss: 1.7111
Batch 100, Loss: 1.7111
Batch 125, Loss: 1.7111
Batch 150, Loss: 1.7112
Batch 175, Loss: 1.7112
Noise applied in 22419 out of 23424 batches, 95.71
Epoch 122 learning rate: 0.01
Epoch 122 time: 355.78690576553345 seconds
Epoch 122 accuracy: 15.62%
Batch 25, Loss: 1.7113
Batch 50, Loss: 1.7113
Batch 75, Loss: 1.7113
Batch 100, Loss: 1.7114
Batch 125, Loss: 1.7114
Batch 150, Loss: 1.7114
Batch 175, Loss: 1.7115
Noise applied in 22611 out of 23616 batches, 95.74
Epoch 123 learning rate: 0.01
Epoch 123 time: 355.5793285369873 seconds
Epoch 123 accuracy: 15.64%
Batch 25, Loss: 1.7115
Batch 50, Loss: 1.7116
Batch 75, Loss: 1.7116
Batch 100, Loss: 1.7116
Batch 125, Loss: 1.7117
Batch 150, Loss: 1.7117
Batch 175, Loss: 1.7118
Noise applied in 22803 out of 23808 batches, 95.78
Epoch 124 learning rate: 0.01
Epoch 124 time: 356.5338454246521 seconds
Epoch 124 accuracy: 15.64%
Batch 25, Loss: 1.7118
Batch 50, Loss: 1.7118
Batch 75, Loss: 1.7119
Batch 100, Loss: 1.7119
Batch 125, Loss: 1.7119
Batch 150, Loss: 1.7120
Batch 175, Loss: 1.7120
Noise applied in 22995 out of 24000 batches, 95.81
Epoch 125 learning rate: 0.01
Epoch 125 time: 355.8715491294861 seconds
Epoch 125 accuracy: 15.66%
Batch 25, Loss: 1.7121
Batch 50, Loss: 1.7121
Batch 75, Loss: 1.7121
Batch 100, Loss: 1.7122
Batch 125, Loss: 1.7122
Batch 150, Loss: 1.7122
Batch 175, Loss: 1.7123
Noise applied in 23187 out of 24192 batches, 95.85
Epoch 126 learning rate: 0.01
Epoch 126 time: 355.39581966400146 seconds
Epoch 126 accuracy: 15.66%
Batch 25, Loss: 1.7123
Batch 50, Loss: 1.7124
Batch 75, Loss: 1.7124
Batch 100, Loss: 1.7124
Batch 125, Loss: 1.7125
Batch 150, Loss: 1.7125
Batch 175, Loss: 1.7125
Noise applied in 23379 out of 24384 batches, 95.88
Epoch 127 learning rate: 0.01
Epoch 127 time: 356.6342821121216 seconds
Epoch 127 accuracy: 15.64%
Batch 25, Loss: 1.7126
Batch 50, Loss: 1.7126
Batch 75, Loss: 1.7127
Batch 100, Loss: 1.7127
Batch 125, Loss: 1.7127
Batch 150, Loss: 1.7128
Batch 175, Loss: 1.7128
Noise applied in 23571 out of 24576 batches, 95.91
Epoch 128 learning rate: 0.01
Epoch 128 time: 355.6463944911957 seconds
Epoch 128 accuracy: 15.63%
Batch 25, Loss: 1.7128
Batch 50, Loss: 1.7129
Batch 75, Loss: 1.7129
Batch 100, Loss: 1.7129
Batch 125, Loss: 1.7130
Batch 150, Loss: 1.7130
Batch 175, Loss: 1.7130
Noise applied in 23763 out of 24768 batches, 95.94
Epoch 129 learning rate: 0.01
Epoch 129 time: 356.262544631958 seconds
Epoch 129 accuracy: 15.62%
Batch 25, Loss: 1.7131
Batch 50, Loss: 1.7131
Batch 75, Loss: 1.7132
Batch 100, Loss: 1.7132
Batch 125, Loss: 1.7132
Batch 150, Loss: 1.7133
Batch 175, Loss: 1.7133
Noise applied in 23955 out of 24960 batches, 95.97
Epoch 130 learning rate: 0.01
Epoch 130 time: 355.5866575241089 seconds
Epoch 130 accuracy: 15.6%
Batch 25, Loss: 1.7133
Batch 50, Loss: 1.7134
Batch 75, Loss: 1.7134
Batch 100, Loss: 1.7134
Batch 125, Loss: 1.7135
Batch 150, Loss: 1.7135
Batch 175, Loss: 1.7135
Noise applied in 24147 out of 25152 batches, 96.00
Epoch 131 learning rate: 0.01
Epoch 131 time: 355.4676425457001 seconds
Epoch 131 accuracy: 15.59%
Batch 25, Loss: 1.7136
Batch 50, Loss: 1.7136
Batch 75, Loss: 1.7137
Batch 100, Loss: 1.7137
Batch 125, Loss: 1.7137
Batch 150, Loss: 1.7138
Batch 175, Loss: 1.7138
Noise applied in 24339 out of 25344 batches, 96.03
Epoch 132 learning rate: 0.01
Epoch 132 time: 355.84727573394775 seconds
Epoch 132 accuracy: 15.58%
Batch 25, Loss: 1.7139
Batch 50, Loss: 1.7139
Batch 75, Loss: 1.7139
Batch 100, Loss: 1.7140
Batch 125, Loss: 1.7140
Batch 150, Loss: 1.7140
Batch 175, Loss: 1.7141
Noise applied in 24531 out of 25536 batches, 96.06
Epoch 133 learning rate: 0.01
Epoch 133 time: 365.21195793151855 seconds
Epoch 133 accuracy: 15.57%
Batch 25, Loss: 1.7141
Batch 50, Loss: 1.7142
Batch 75, Loss: 1.7142
Batch 100, Loss: 1.7142
Batch 125, Loss: 1.7143
Batch 150, Loss: 1.7143
Batch 175, Loss: 1.7143
Noise applied in 24723 out of 25728 batches, 96.09
Epoch 134 learning rate: 0.01
Epoch 134 time: 392.2181963920593 seconds
Epoch 134 accuracy: 15.55%
Batch 25, Loss: 1.7144
Batch 50, Loss: 1.7144
Batch 75, Loss: 1.7144
Batch 100, Loss: 1.7145
Batch 125, Loss: 1.7145
Batch 150, Loss: 1.7145
Batch 175, Loss: 1.7146
Noise applied in 24915 out of 25920 batches, 96.12
Epoch 135 learning rate: 0.01
Epoch 135 time: 380.205584526062 seconds
Epoch 135 accuracy: 15.53%
Batch 25, Loss: 1.7146
Batch 50, Loss: 1.7147
Batch 75, Loss: 1.7147
Batch 100, Loss: 1.7147
Batch 125, Loss: 1.7148
Batch 150, Loss: 1.7148
Batch 175, Loss: 1.7148
Noise applied in 25107 out of 26112 batches, 96.15
Epoch 136 learning rate: 0.01
Epoch 136 time: 419.38368487358093 seconds
Epoch 136 accuracy: 15.53%
Batch 25, Loss: 1.7149
Batch 50, Loss: 1.7149
Batch 75, Loss: 1.7149
Batch 100, Loss: 1.7150
Batch 125, Loss: 1.7150
Batch 150, Loss: 1.7150
Batch 175, Loss: 1.7151
Noise applied in 25299 out of 26304 batches, 96.18
Epoch 137 learning rate: 0.01
Epoch 137 time: 413.11796832084656 seconds
Epoch 137 accuracy: 15.53%
Batch 25, Loss: 1.7151
Batch 50, Loss: 1.7152
Batch 75, Loss: 1.7152
Batch 100, Loss: 1.7152
Batch 125, Loss: 1.7153
Batch 150, Loss: 1.7153
Batch 175, Loss: 1.7153
Noise applied in 25491 out of 26496 batches, 96.21
Epoch 138 learning rate: 0.01
Epoch 138 time: 376.0736393928528 seconds
Epoch 138 accuracy: 15.53%
Batch 25, Loss: 1.7154
Batch 50, Loss: 1.7154
Batch 75, Loss: 1.7154
Batch 100, Loss: 1.7155
Batch 125, Loss: 1.7155
Batch 150, Loss: 1.7155
Batch 175, Loss: 1.7156
Noise applied in 25683 out of 26688 batches, 96.23
Epoch 139 learning rate: 0.01
Epoch 139 time: 380.91646933555603 seconds
Epoch 139 accuracy: 15.51%
Batch 25, Loss: 1.7156
Batch 50, Loss: 1.7156
Batch 75, Loss: 1.7157
Batch 100, Loss: 1.7157
Batch 125, Loss: 1.7157
Batch 150, Loss: 1.7158
Batch 175, Loss: 1.7158
Noise applied in 25875 out of 26880 batches, 96.26
Epoch 140 learning rate: 0.01
Epoch 140 time: 376.1204490661621 seconds
Epoch 140 accuracy: 15.52%
Batch 25, Loss: 1.7158
Batch 50, Loss: 1.7159
Batch 75, Loss: 1.7159
Batch 100, Loss: 1.7159
Batch 125, Loss: 1.7160
Batch 150, Loss: 1.7160
Batch 175, Loss: 1.7160
Noise applied in 26067 out of 27072 batches, 96.29
Epoch 141 learning rate: 0.01
Epoch 141 time: 437.55480003356934 seconds
Epoch 141 accuracy: 15.48%
Batch 25, Loss: 1.7161
Batch 50, Loss: 1.7161
Batch 75, Loss: 1.7162
Batch 100, Loss: 1.7162
Batch 125, Loss: 1.7162
Batch 150, Loss: 1.7163
Batch 175, Loss: 1.7163
Noise applied in 26259 out of 27264 batches, 96.31
Epoch 142 learning rate: 0.01
Epoch 142 time: 424.18550062179565 seconds
Epoch 142 accuracy: 15.48%
Batch 25, Loss: 1.7163
Batch 50, Loss: 1.7164
Batch 75, Loss: 1.7164
Batch 100, Loss: 1.7164
Batch 125, Loss: 1.7165
Batch 150, Loss: 1.7165
Batch 175, Loss: 1.7165
Noise applied in 26451 out of 27456 batches, 96.34
Epoch 143 learning rate: 0.01
Epoch 143 time: 384.61756467819214 seconds
Epoch 143 accuracy: 15.47%
Batch 25, Loss: 1.7166
Batch 50, Loss: 1.7166
Batch 75, Loss: 1.7167
Batch 100, Loss: 1.7167
Batch 125, Loss: 1.7167
Batch 150, Loss: 1.7168
Batch 175, Loss: 1.7168
Noise applied in 26643 out of 27648 batches, 96.37
Epoch 144 learning rate: 0.01
Epoch 144 time: 384.94285774230957 seconds
Epoch 144 accuracy: 15.47%
Batch 25, Loss: 1.7168
Batch 50, Loss: 1.7169
Batch 75, Loss: 1.7169
Batch 100, Loss: 1.7169
Batch 125, Loss: 1.7170
Batch 150, Loss: 1.7170
Batch 175, Loss: 1.7170
Noise applied in 26835 out of 27840 batches, 96.39
Epoch 145 learning rate: 0.01
Epoch 145 time: 386.2284815311432 seconds
Epoch 145 accuracy: 15.47%
Batch 25, Loss: 1.7171
Batch 50, Loss: 1.7171
Batch 75, Loss: 1.7172
Batch 100, Loss: 1.7172
Batch 125, Loss: 1.7172
Batch 150, Loss: 1.7173
Batch 175, Loss: 1.7173
Noise applied in 27027 out of 28032 batches, 96.41
Epoch 146 learning rate: 0.01
Epoch 146 time: 385.3520805835724 seconds
Epoch 146 accuracy: 15.48%
Batch 25, Loss: 1.7174
Batch 50, Loss: 1.7174
Batch 75, Loss: 1.7174
Batch 100, Loss: 1.7175
Batch 125, Loss: 1.7175
Batch 150, Loss: 1.7175
Batch 175, Loss: 1.7176
Noise applied in 27219 out of 28224 batches, 96.44
Epoch 147 learning rate: 0.01
Epoch 147 time: 442.5349237918854 seconds
Epoch 147 accuracy: 15.48%
Batch 25, Loss: 1.7176
Batch 50, Loss: 1.7177
Batch 75, Loss: 1.7177
Batch 100, Loss: 1.7177
Batch 125, Loss: 1.7178
Batch 150, Loss: 1.7178
Batch 175, Loss: 1.7178
Noise applied in 27411 out of 28416 batches, 96.46
Epoch 148 learning rate: 0.01
Epoch 148 time: 428.8254773616791 seconds
Epoch 148 accuracy: 15.47%
Batch 25, Loss: 1.7179
Batch 50, Loss: 1.7179
Batch 75, Loss: 1.7180
Batch 100, Loss: 1.7180
Batch 125, Loss: 1.7180
Batch 150, Loss: 1.7181
Batch 175, Loss: 1.7181
Noise applied in 27603 out of 28608 batches, 96.49
Epoch 149 learning rate: 0.01
Epoch 149 time: 426.0394694805145 seconds
Epoch 149 accuracy: 15.47%
Batch 25, Loss: 1.7182
Batch 50, Loss: 1.7182
Batch 75, Loss: 1.7182
Batch 100, Loss: 1.7183
Batch 125, Loss: 1.7183
Batch 150, Loss: 1.7183
Batch 175, Loss: 1.7184
Noise applied in 27795 out of 28800 batches, 96.51
Epoch 150 learning rate: 0.01
Epoch 150 time: 397.3283152580261 seconds
Epoch 150 accuracy: 15.46%
Batch 25, Loss: 1.7184
Batch 50, Loss: 1.7185
Batch 75, Loss: 1.7185
Batch 100, Loss: 1.7185
Batch 125, Loss: 1.7186
Batch 150, Loss: 1.7186
Batch 175, Loss: 1.7186
Noise applied in 27987 out of 28992 batches, 96.53
Epoch 151 learning rate: 0.01
Epoch 151 time: 424.51063442230225 seconds
Epoch 151 accuracy: 15.45%
Batch 25, Loss: 1.7187
Batch 50, Loss: 1.7187
Batch 75, Loss: 1.7188
Batch 100, Loss: 1.7188
Batch 125, Loss: 1.7188
Batch 150, Loss: 1.7189
Batch 175, Loss: 1.7189
Noise applied in 28179 out of 29184 batches, 96.56
Epoch 152 learning rate: 0.01
Epoch 152 time: 391.317419052124 seconds
Epoch 152 accuracy: 15.44%
Batch 25, Loss: 1.7190
Batch 50, Loss: 1.7190
Batch 75, Loss: 1.7190
Batch 100, Loss: 1.7191
Batch 125, Loss: 1.7191
Batch 150, Loss: 1.7191
Batch 175, Loss: 1.7192
Noise applied in 28371 out of 29376 batches, 96.58
Epoch 153 learning rate: 0.01
Epoch 153 time: 377.05945920944214 seconds
Epoch 153 accuracy: 15.45%
Batch 25, Loss: 1.7192
Batch 50, Loss: 1.7193
Batch 75, Loss: 1.7193
Batch 100, Loss: 1.7193
Batch 125, Loss: 1.7194
Batch 150, Loss: 1.7194
Batch 175, Loss: 1.7194
Noise applied in 28563 out of 29568 batches, 96.60
Epoch 154 learning rate: 0.01
Epoch 154 time: 421.18746972084045 seconds
Epoch 154 accuracy: 15.44%
Batch 25, Loss: 1.7195
Batch 50, Loss: 1.7195
Batch 75, Loss: 1.7196
Batch 100, Loss: 1.7196
Batch 125, Loss: 1.7196
Batch 150, Loss: 1.7197
Batch 175, Loss: 1.7197
Noise applied in 28755 out of 29760 batches, 96.62
Epoch 155 learning rate: 0.01
Epoch 155 time: 529.6994137763977 seconds
Epoch 155 accuracy: 15.44%
Batch 25, Loss: 1.7197
Batch 50, Loss: 1.7198
Batch 75, Loss: 1.7198
Batch 100, Loss: 1.7198
Batch 125, Loss: 1.7198
Batch 150, Loss: 1.7199
Batch 175, Loss: 1.7199
Noise applied in 28947 out of 29952 batches, 96.64
Epoch 156 learning rate: 0.01
Epoch 156 time: 397.9781494140625 seconds
Epoch 156 accuracy: 15.41%
Batch 25, Loss: 1.7199
Batch 50, Loss: 1.7200
Batch 75, Loss: 1.7200
Batch 100, Loss: 1.7200
Batch 125, Loss: 1.7200
Batch 150, Loss: 1.7201
Batch 175, Loss: 1.7201
Noise applied in 29139 out of 30144 batches, 96.67
Epoch 157 learning rate: 0.01
Epoch 157 time: 404.95651173591614 seconds
Epoch 157 accuracy: 15.41%
Batch 25, Loss: 1.7201
Batch 50, Loss: 1.7202
Batch 75, Loss: 1.7202
Batch 100, Loss: 1.7202
Batch 125, Loss: 1.7202
Batch 150, Loss: 1.7203
Batch 175, Loss: 1.7203
Noise applied in 29331 out of 30336 batches, 96.69
Epoch 158 learning rate: 0.01
Epoch 158 time: 420.51134753227234 seconds
Epoch 158 accuracy: 15.42%
Batch 25, Loss: 1.7203
Batch 50, Loss: 1.7204
Batch 75, Loss: 1.7204
Batch 100, Loss: 1.7204
Batch 125, Loss: 1.7205
Batch 150, Loss: 1.7205
Batch 175, Loss: 1.7205
Noise applied in 29523 out of 30528 batches, 96.71
Epoch 159 learning rate: 0.01
Epoch 159 time: 445.1396315097809 seconds
Epoch 159 accuracy: 15.44%
Batch 25, Loss: 1.7206
Batch 50, Loss: 1.7206
Batch 75, Loss: 1.7206
Batch 100, Loss: 1.7207
Batch 125, Loss: 1.7207
Batch 150, Loss: 1.7207
Batch 175, Loss: 1.7208
Noise applied in 29715 out of 30720 batches, 96.73
Epoch 160 learning rate: 0.01
Epoch 160 time: 391.9446997642517 seconds
Epoch 160 accuracy: 15.39%
Batch 25, Loss: 1.7208
Batch 50, Loss: 1.7209
Batch 75, Loss: 1.7209
Batch 100, Loss: 1.7209
Batch 125, Loss: 1.7210
Batch 150, Loss: 1.7210
Batch 175, Loss: 1.7210
Noise applied in 29907 out of 30912 batches, 96.75
Epoch 161 learning rate: 0.01
Epoch 161 time: 407.5348422527313 seconds
Epoch 161 accuracy: 15.38%
Batch 25, Loss: 1.7211
Batch 50, Loss: 1.7211
Batch 75, Loss: 1.7211
Batch 100, Loss: 1.7212
Batch 125, Loss: 1.7212
Batch 150, Loss: 1.7212
Batch 175, Loss: 1.7212
Noise applied in 30099 out of 31104 batches, 96.77
Epoch 162 learning rate: 0.01
Epoch 162 time: 505.25272011756897 seconds
Epoch 162 accuracy: 15.37%
Batch 25, Loss: 1.7213
Batch 50, Loss: 1.7213
Batch 75, Loss: 1.7214
Batch 100, Loss: 1.7214
Batch 125, Loss: 1.7214
Batch 150, Loss: 1.7215
Batch 175, Loss: 1.7215
Noise applied in 30291 out of 31296 batches, 96.79
Epoch 163 learning rate: 0.01
Epoch 163 time: 492.92390060424805 seconds
Epoch 163 accuracy: 15.37%
Batch 25, Loss: 1.7215
Batch 50, Loss: 1.7216
Batch 75, Loss: 1.7216
Batch 100, Loss: 1.7216
Batch 125, Loss: 1.7217
Batch 150, Loss: 1.7217
Batch 175, Loss: 1.7217
Noise applied in 30483 out of 31488 batches, 96.81
Epoch 164 learning rate: 0.01
Epoch 164 time: 423.2480642795563 seconds
Epoch 164 accuracy: 15.36%
Batch 25, Loss: 1.7218
Batch 50, Loss: 1.7218
Batch 75, Loss: 1.7218
Batch 100, Loss: 1.7219
Batch 125, Loss: 1.7219
Batch 150, Loss: 1.7219
Batch 175, Loss: 1.7220
Noise applied in 30675 out of 31680 batches, 96.83
Epoch 165 learning rate: 0.01
Epoch 165 time: 402.94372630119324 seconds
Epoch 165 accuracy: 15.34%
Batch 25, Loss: 1.7220
Batch 50, Loss: 1.7220
Batch 75, Loss: 1.7221
Batch 100, Loss: 1.7221
Batch 125, Loss: 1.7221
Batch 150, Loss: 1.7222
Batch 175, Loss: 1.7222
Noise applied in 30867 out of 31872 batches, 96.85
Epoch 166 learning rate: 0.01
Epoch 166 time: 420.9668855667114 seconds
Epoch 166 accuracy: 15.35%
Batch 25, Loss: 1.7222
Batch 50, Loss: 1.7223
Batch 75, Loss: 1.7223
Batch 100, Loss: 1.7223
Batch 125, Loss: 1.7224
Batch 150, Loss: 1.7224
Batch 175, Loss: 1.7224
Noise applied in 31059 out of 32064 batches, 96.87
Epoch 167 learning rate: 0.01
Epoch 167 time: 411.322669506073 seconds
Epoch 167 accuracy: 15.37%
Batch 25, Loss: 1.7225
Batch 50, Loss: 1.7225
Batch 75, Loss: 1.7226
Batch 100, Loss: 1.7226
Batch 125, Loss: 1.7226
Batch 150, Loss: 1.7227
Batch 175, Loss: 1.7227
Noise applied in 31251 out of 32256 batches, 96.88
Epoch 168 learning rate: 0.01
Epoch 168 time: 504.4654927253723 seconds
Epoch 168 accuracy: 15.36%
Batch 25, Loss: 1.7227
Batch 50, Loss: 1.7228
Batch 75, Loss: 1.7228
Batch 100, Loss: 1.7228
Batch 125, Loss: 1.7229
Batch 150, Loss: 1.7229
Batch 175, Loss: 1.7229
Noise applied in 31443 out of 32448 batches, 96.90
Epoch 169 learning rate: 0.01
Epoch 169 time: 470.3088147640228 seconds
Epoch 169 accuracy: 15.36%
Batch 25, Loss: 1.7230
Batch 50, Loss: 1.7230
Batch 75, Loss: 1.7230
Batch 100, Loss: 1.7231
Batch 125, Loss: 1.7231
Batch 150, Loss: 1.7231
Batch 175, Loss: 1.7232
Noise applied in 31635 out of 32640 batches, 96.92
Epoch 170 learning rate: 0.01
Epoch 170 time: 614.6130774021149 seconds
Epoch 170 accuracy: 15.35%
Batch 25, Loss: 1.7232
Batch 50, Loss: 1.7232
Batch 75, Loss: 1.7233
Batch 100, Loss: 1.7233
Batch 125, Loss: 1.7233
Batch 150, Loss: 1.7234
Batch 175, Loss: 1.7234
Noise applied in 31827 out of 32832 batches, 96.94
Epoch 171 learning rate: 0.01
Epoch 171 time: 395.3003091812134 seconds
Epoch 171 accuracy: 15.33%
Batch 25, Loss: 1.7234
Batch 50, Loss: 1.7235
Batch 75, Loss: 1.7235
Batch 100, Loss: 1.7235
Batch 125, Loss: 1.7236
Batch 150, Loss: 1.7236
Batch 175, Loss: 1.7236
Noise applied in 32019 out of 33024 batches, 96.96
Epoch 172 learning rate: 0.01
Epoch 172 time: 494.413516998291 seconds
Epoch 172 accuracy: 15.33%
Batch 25, Loss: 1.7237
Batch 50, Loss: 1.7237
Batch 75, Loss: 1.7238
Batch 100, Loss: 1.7238
Batch 125, Loss: 1.7238
Batch 150, Loss: 1.7239
Batch 175, Loss: 1.7239
Noise applied in 32211 out of 33216 batches, 96.97
Epoch 173 learning rate: 0.01
Epoch 173 time: 444.5204699039459 seconds
Epoch 173 accuracy: 15.33%
Batch 25, Loss: 1.7239
Batch 50, Loss: 1.7240
Batch 75, Loss: 1.7240
Batch 100, Loss: 1.7240
Batch 125, Loss: 1.7241
Batch 150, Loss: 1.7241
Batch 175, Loss: 1.7241
Noise applied in 32403 out of 33408 batches, 96.99
Epoch 174 learning rate: 0.01
Epoch 174 time: 433.94289469718933 seconds
Epoch 174 accuracy: 15.33%
Batch 25, Loss: 1.7242
Batch 50, Loss: 1.7242
Batch 75, Loss: 1.7243
Batch 100, Loss: 1.7243
Batch 125, Loss: 1.7243
Batch 150, Loss: 1.7244
Batch 175, Loss: 1.7244
Noise applied in 32595 out of 33600 batches, 97.01
Epoch 175 learning rate: 0.01
Epoch 175 time: 437.52507638931274 seconds
Epoch 175 accuracy: 15.34%
Batch 25, Loss: 1.7245
Batch 50, Loss: 1.7245
Batch 75, Loss: 1.7245
Batch 100, Loss: 1.7246
Batch 125, Loss: 1.7246
Batch 150, Loss: 1.7246
Batch 175, Loss: 1.7247
Noise applied in 32787 out of 33792 batches, 97.03
Epoch 176 learning rate: 0.01
Epoch 176 time: 550.4078631401062 seconds
Epoch 176 accuracy: 15.33%
Batch 25, Loss: 1.7247
Batch 50, Loss: 1.7248
Batch 75, Loss: 1.7248
Batch 100, Loss: 1.7248
Batch 125, Loss: 1.7249
Batch 150, Loss: 1.7249
Batch 175, Loss: 1.7250
Noise applied in 32979 out of 33984 batches, 97.04
Epoch 177 learning rate: 0.01
Epoch 177 time: 433.71095299720764 seconds
Epoch 177 accuracy: 15.31%
Batch 25, Loss: 1.7250
Batch 50, Loss: 1.7250
Batch 75, Loss: 1.7251
Batch 100, Loss: 1.7251
Batch 125, Loss: 1.7252
Batch 150, Loss: 1.7252
Batch 175, Loss: 1.7252
Noise applied in 33171 out of 34176 batches, 97.06
Epoch 178 learning rate: 0.01
Epoch 178 time: 452.34539341926575 seconds
Epoch 178 accuracy: 15.29%
Batch 25, Loss: 1.7253
Batch 50, Loss: 1.7253
Batch 75, Loss: 1.7254
Batch 100, Loss: 1.7254
Batch 125, Loss: 1.7254
Batch 150, Loss: 1.7255
Batch 175, Loss: 1.7255
Noise applied in 33363 out of 34368 batches, 97.08
Epoch 179 learning rate: 0.01
Epoch 179 time: 445.59765338897705 seconds
Epoch 179 accuracy: 15.25%
Batch 25, Loss: 1.7256
Batch 50, Loss: 1.7256
Batch 75, Loss: 1.7257
Batch 100, Loss: 1.7257
Batch 125, Loss: 1.7257
Batch 150, Loss: 1.7258
Batch 175, Loss: 1.7258
Noise applied in 33555 out of 34560 batches, 97.09
Epoch 180 learning rate: 0.01
Epoch 180 time: 434.6057348251343 seconds
Epoch 180 accuracy: 15.17%
Batch 25, Loss: 1.7259
Batch 50, Loss: 1.7259
Batch 75, Loss: 1.7260
Batch 100, Loss: 1.7260
Batch 125, Loss: 1.7260
Batch 150, Loss: 1.7261
Batch 175, Loss: 1.7261
Noise applied in 33747 out of 34752 batches, 97.11
Epoch 181 learning rate: 0.01
Epoch 181 time: 392.48961997032166 seconds
Epoch 181 accuracy: 15.2%
Batch 25, Loss: 1.7262
Batch 50, Loss: 1.7262
Batch 75, Loss: 1.7263
Batch 100, Loss: 1.7263
Batch 125, Loss: 1.7263
Batch 150, Loss: 1.7264
Batch 175, Loss: 1.7264
Noise applied in 33939 out of 34944 batches, 97.12
Epoch 182 learning rate: 0.01
Epoch 182 time: 409.3450801372528 seconds
Epoch 182 accuracy: 15.17%
Batch 25, Loss: 1.7265
Batch 50, Loss: 1.7265
Batch 75, Loss: 1.7266
Batch 100, Loss: 1.7266
Batch 125, Loss: 1.7266
Batch 150, Loss: 1.7267
Batch 175, Loss: 1.7267
Noise applied in 34131 out of 35136 batches, 97.14
Epoch 183 learning rate: 0.01
Epoch 183 time: 535.004506111145 seconds
Epoch 183 accuracy: 15.16%
Batch 25, Loss: 1.7268
Batch 50, Loss: 1.7268
Batch 75, Loss: 1.7269
Batch 100, Loss: 1.7269
Batch 125, Loss: 1.7270
Batch 150, Loss: 1.7270
Batch 175, Loss: 1.7270
Noise applied in 34323 out of 35328 batches, 97.16
Epoch 184 learning rate: 0.01
Epoch 184 time: 528.439966917038 seconds
Epoch 184 accuracy: 15.15%
Batch 25, Loss: 1.7271
Batch 50, Loss: 1.7271
Batch 75, Loss: 1.7272
Batch 100, Loss: 1.7272
Batch 125, Loss: 1.7273
Batch 150, Loss: 1.7273
Batch 175, Loss: 1.7273
Noise applied in 34515 out of 35520 batches, 97.17
Epoch 185 learning rate: 0.01
Epoch 185 time: 662.3121991157532 seconds
Epoch 185 accuracy: 15.11%
Batch 25, Loss: 1.7274
Batch 50, Loss: 1.7274
Batch 75, Loss: 1.7275
Batch 100, Loss: 1.7275
Batch 125, Loss: 1.7276
Batch 150, Loss: 1.7276
Batch 175, Loss: 1.7276
Noise applied in 34707 out of 35712 batches, 97.19
Epoch 186 learning rate: 0.01
Epoch 186 time: 517.5092282295227 seconds
Epoch 186 accuracy: 15.09%
Batch 25, Loss: 1.7277
Batch 50, Loss: 1.7278
Batch 75, Loss: 1.7278
Batch 100, Loss: 1.7278
Batch 125, Loss: 1.7279
Batch 150, Loss: 1.7279
Batch 175, Loss: 1.7280
Noise applied in 34899 out of 35904 batches, 97.20
Epoch 187 learning rate: 0.01
Epoch 187 time: 473.79612922668457 seconds
Epoch 187 accuracy: 15.04%
Batch 25, Loss: 1.7280
Batch 50, Loss: 1.7281
Batch 75, Loss: 1.7281
Batch 100, Loss: 1.7281
Batch 125, Loss: 1.7282
Batch 150, Loss: 1.7282
Batch 175, Loss: 1.7283
Noise applied in 35091 out of 36096 batches, 97.22
Epoch 188 learning rate: 0.01
Epoch 188 time: 398.13401770591736 seconds
Epoch 188 accuracy: 15.07%
Batch 25, Loss: 1.7283
Batch 50, Loss: 1.7284
Batch 75, Loss: 1.7284
Batch 100, Loss: 1.7284
Batch 125, Loss: 1.7285
Batch 150, Loss: 1.7285
Batch 175, Loss: 1.7286
Noise applied in 35283 out of 36288 batches, 97.23
Epoch 189 learning rate: 0.01
Epoch 189 time: 389.21598744392395 seconds
Epoch 189 accuracy: 15.0%
Batch 25, Loss: 1.7286
Batch 50, Loss: 1.7287
Batch 75, Loss: 1.7287
Batch 100, Loss: 1.7287
Batch 125, Loss: 1.7288
Batch 150, Loss: 1.7288
Batch 175, Loss: 1.7289
Noise applied in 35475 out of 36480 batches, 97.25
Epoch 190 learning rate: 0.01
Epoch 190 time: 408.23973751068115 seconds
Epoch 190 accuracy: 14.97%
Batch 25, Loss: 1.7289
Batch 50, Loss: 1.7290
Batch 75, Loss: 1.7290
Batch 100, Loss: 1.7290
Batch 125, Loss: 1.7291
Batch 150, Loss: 1.7291
Batch 175, Loss: 1.7292
Noise applied in 35667 out of 36672 batches, 97.26
Epoch 191 learning rate: 0.01
Epoch 191 time: 464.6393795013428 seconds
Epoch 191 accuracy: 14.96%
Batch 25, Loss: 1.7292
Batch 50, Loss: 1.7293
Batch 75, Loss: 1.7293
Batch 100, Loss: 1.7293
Batch 125, Loss: 1.7294
Batch 150, Loss: 1.7294
Batch 175, Loss: 1.7295
Noise applied in 35859 out of 36864 batches, 97.27
Epoch 192 learning rate: 0.01
Epoch 192 time: 392.0226709842682 seconds
Epoch 192 accuracy: 14.93%
Batch 25, Loss: 1.7295
Batch 50, Loss: 1.7296
Batch 75, Loss: 1.7296
Batch 100, Loss: 1.7296
Batch 125, Loss: 1.7297
Batch 150, Loss: 1.7297
Batch 175, Loss: 1.7298
Noise applied in 36051 out of 37056 batches, 97.29
Epoch 193 learning rate: 0.01
Epoch 193 time: 357.70037508010864 seconds
Epoch 193 accuracy: 14.88%
Batch 25, Loss: 1.7298
Batch 50, Loss: 1.7299
Batch 75, Loss: 1.7299
Batch 100, Loss: 1.7299
Batch 125, Loss: 1.7300
Batch 150, Loss: 1.7300
Batch 175, Loss: 1.7301
Noise applied in 36243 out of 37248 batches, 97.30
Epoch 194 learning rate: 0.01
Epoch 194 time: 356.2561893463135 seconds
Epoch 194 accuracy: 14.86%
Batch 25, Loss: 1.7301
Batch 50, Loss: 1.7302
Batch 75, Loss: 1.7302
Batch 100, Loss: 1.7302
Batch 125, Loss: 1.7303
Batch 150, Loss: 1.7303
Batch 175, Loss: 1.7304
Noise applied in 36435 out of 37440 batches, 97.32
Epoch 195 learning rate: 0.01
Epoch 195 time: 357.11260986328125 seconds
Epoch 195 accuracy: 14.81%
Batch 25, Loss: 1.7304
Batch 50, Loss: 1.7305
Batch 75, Loss: 1.7305
Batch 100, Loss: 1.7305
Batch 125, Loss: 1.7306
Batch 150, Loss: 1.7306
Batch 175, Loss: 1.7307
Noise applied in 36627 out of 37632 batches, 97.33
Epoch 196 learning rate: 0.01
Epoch 196 time: 356.7548840045929 seconds
Epoch 196 accuracy: 14.79%
Batch 25, Loss: 1.7307
Batch 50, Loss: 1.7308
Batch 75, Loss: 1.7308
Batch 100, Loss: 1.7308
Batch 125, Loss: 1.7309
Batch 150, Loss: 1.7309
Batch 175, Loss: 1.7310
Noise applied in 36819 out of 37824 batches, 97.34
Epoch 197 learning rate: 0.01
Epoch 197 time: 356.59200072288513 seconds
Epoch 197 accuracy: 14.82%
Batch 25, Loss: 1.7310
Batch 50, Loss: 1.7311
Batch 75, Loss: 1.7311
Batch 100, Loss: 1.7311
Batch 125, Loss: 1.7312
Batch 150, Loss: 1.7312
Batch 175, Loss: 1.7313
Noise applied in 37011 out of 38016 batches, 97.36
Epoch 198 learning rate: 0.01
Epoch 198 time: 356.75199270248413 seconds
Epoch 198 accuracy: 14.79%
Batch 25, Loss: 1.7313
Batch 50, Loss: 1.7314
Batch 75, Loss: 1.7314
Batch 100, Loss: 1.7314
Batch 125, Loss: 1.7315
Batch 150, Loss: 1.7315
Batch 175, Loss: 1.7315
Noise applied in 37203 out of 38208 batches, 97.37
Epoch 199 learning rate: 0.01
Epoch 199 time: 357.05904483795166 seconds
Epoch 199 accuracy: 14.81%
Batch 25, Loss: 1.7316
Batch 50, Loss: 1.7316
Batch 75, Loss: 1.7317
Batch 100, Loss: 1.7317
Batch 125, Loss: 1.7318
Batch 150, Loss: 1.7318
Batch 175, Loss: 1.7318
Noise applied in 37395 out of 38400 batches, 97.38
Epoch 200 learning rate: 0.01
Epoch 200 time: 355.9546949863434 seconds
Epoch 200 accuracy: 14.75%
rho:  0.04 , alpha:  0.3
Total training time: 78273.64017486572 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.3064
Norm of the Gradient: 2.0948927104e-01
Smallest Hessian Eigenvalue: -0.2923
Noise Threshold: 0.6
Noise Radius: 0.01
