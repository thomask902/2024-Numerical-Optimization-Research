The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:232: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/newtonMR/basicaug/lr-0.1/batchsize-256/2024-09-10-16:30:19
Using Newton's Minimum Residual Algorithm (newtonMR)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py", line 543, in <module>
    main()
  File "/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py", line 245, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py", line 457, in main_worker
    train_epoch_gam(model, train_loader, optimizer, gpu, args)
  File "/project/6070520/tkleinkn/Vanilla-GAM/utils/train_utils_gam.py", line 26, in train_epoch_gam
    predictions, loss = optimizer.step()
                        ^^^^^^^^^^^^^^^^
  File "/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/project/6070520/tkleinkn/Vanilla-GAM/utils/newton_mr.py", line 127, in step
    H = self.hessian(g)
        ^^^^^^^^^^^^^^^
  File "/project/6070520/tkleinkn/Vanilla-GAM/utils/newton_mr.py", line 54, in hessian
    grad2_ij = torch.autograd.grad(grad_i, param, retain_graph=True, create_graph=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/__init__.py", line 412, in grad
    result = _engine_run_backward(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 
