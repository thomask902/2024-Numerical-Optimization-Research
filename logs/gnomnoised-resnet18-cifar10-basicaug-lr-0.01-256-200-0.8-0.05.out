The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-06-11:25:34
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 138.5829
Batch 50, Loss: 64.8203
Batch 75, Loss: 30.0224
Batch 100, Loss: 20.4872
Batch 125, Loss: 15.9109
Batch 150, Loss: 12.9107
Batch 175, Loss: 10.7494
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 300.04352164268494 seconds
Epoch 1 accuracy: 11.6%
Batch 25, Loss: 8.4002
Batch 50, Loss: 7.4306
Batch 75, Loss: 6.6379
Batch 100, Loss: 5.9787
Batch 125, Loss: 5.4246
Batch 150, Loss: 4.9530
Batch 175, Loss: 4.5493
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 334.85020542144775 seconds
Epoch 2 accuracy: 11.2%
Batch 25, Loss: 4.0151
Batch 50, Loss: 3.7803
Batch 75, Loss: 3.5865
Batch 100, Loss: 3.4204
Batch 125, Loss: 3.2767
Batch 150, Loss: 3.1516
Batch 175, Loss: 3.0419
Noise applied in 136 out of 576 batches, 23.61
Epoch 3 learning rate: 0.01
Epoch 3 time: 492.67773628234863 seconds
Epoch 3 accuracy: 9.78%
Batch 25, Loss: 2.8856
Batch 50, Loss: 2.8062
Batch 75, Loss: 2.7353
Batch 100, Loss: 2.6718
Batch 125, Loss: 2.6147
Batch 150, Loss: 2.5638
Batch 175, Loss: 2.5186
Noise applied in 328 out of 768 batches, 42.71
Epoch 4 learning rate: 0.01
Epoch 4 time: 392.60898542404175 seconds
Epoch 4 accuracy: 10.39%
Batch 25, Loss: 2.4533
Batch 50, Loss: 2.4196
Batch 75, Loss: 2.3892
Batch 100, Loss: 2.3615
Batch 125, Loss: 2.3360
Batch 150, Loss: 2.3123
Batch 175, Loss: 2.2903
Noise applied in 520 out of 960 batches, 54.17
Epoch 5 learning rate: 0.01
Epoch 5 time: 379.2696101665497 seconds
Epoch 5 accuracy: 10.62%
Batch 25, Loss: 2.2567
Batch 50, Loss: 2.2384
Batch 75, Loss: 2.2210
Batch 100, Loss: 2.2046
Batch 125, Loss: 2.1894
Batch 150, Loss: 2.1752
Batch 175, Loss: 2.1618
Noise applied in 712 out of 1152 batches, 61.81
Epoch 6 learning rate: 0.01
Epoch 6 time: 594.8208215236664 seconds
Epoch 6 accuracy: 10.93%
Batch 25, Loss: 2.1410
Batch 50, Loss: 2.1295
Batch 75, Loss: 2.1186
Batch 100, Loss: 2.1083
Batch 125, Loss: 2.0984
Batch 150, Loss: 2.0889
Batch 175, Loss: 2.0798
Noise applied in 904 out of 1344 batches, 67.26
Epoch 7 learning rate: 0.01
Epoch 7 time: 495.90230321884155 seconds
Epoch 7 accuracy: 10.79%
Batch 25, Loss: 2.0655
Batch 50, Loss: 2.0574
Batch 75, Loss: 2.0497
Batch 100, Loss: 2.0423
Batch 125, Loss: 2.0351
Batch 150, Loss: 2.0282
Batch 175, Loss: 2.0217
Noise applied in 1096 out of 1536 batches, 71.35
Epoch 8 learning rate: 0.01
Epoch 8 time: 432.8265221118927 seconds
Epoch 8 accuracy: 10.84%
Batch 25, Loss: 2.0111
Batch 50, Loss: 2.0052
Batch 75, Loss: 1.9995
Batch 100, Loss: 1.9940
Batch 125, Loss: 1.9887
Batch 150, Loss: 1.9836
Batch 175, Loss: 1.9786
Noise applied in 1288 out of 1728 batches, 74.54
Epoch 9 learning rate: 0.01
Epoch 9 time: 380.52850103378296 seconds
Epoch 9 accuracy: 12.23%
Batch 25, Loss: 1.9706
Batch 50, Loss: 1.9661
Batch 75, Loss: 1.9618
Batch 100, Loss: 1.9575
Batch 125, Loss: 1.9534
Batch 150, Loss: 1.9494
Batch 175, Loss: 1.9455
Noise applied in 1480 out of 1920 batches, 77.08
Epoch 10 learning rate: 0.01
Epoch 10 time: 372.04354453086853 seconds
Epoch 10 accuracy: 13.52%
Batch 25, Loss: 1.9392
Batch 50, Loss: 1.9356
Batch 75, Loss: 1.9320
Batch 100, Loss: 1.9285
Batch 125, Loss: 1.9250
Batch 150, Loss: 1.9216
Batch 175, Loss: 1.9182
Noise applied in 1672 out of 2112 batches, 79.17
Epoch 11 learning rate: 0.01
Epoch 11 time: 446.0510742664337 seconds
Epoch 11 accuracy: 13.54%
Batch 25, Loss: 1.9126
Batch 50, Loss: 1.9094
Batch 75, Loss: 1.9062
Batch 100, Loss: 1.9030
Batch 125, Loss: 1.8998
Batch 150, Loss: 1.8967
Batch 175, Loss: 1.8936
Noise applied in 1864 out of 2304 batches, 80.90
Epoch 12 learning rate: 0.01
Epoch 12 time: 382.7780864238739 seconds
Epoch 12 accuracy: 13.5%
Batch 25, Loss: 1.8884
Batch 50, Loss: 1.8853
Batch 75, Loss: 1.8822
Batch 100, Loss: 1.8791
Batch 125, Loss: 1.8760
Batch 150, Loss: 1.8730
Batch 175, Loss: 1.8699
Noise applied in 2056 out of 2496 batches, 82.37
Epoch 13 learning rate: 0.01
Epoch 13 time: 408.3520927429199 seconds
Epoch 13 accuracy: 13.32%
Batch 25, Loss: 1.8648
Batch 50, Loss: 1.8617
Batch 75, Loss: 1.8586
Batch 100, Loss: 1.8555
Batch 125, Loss: 1.8524
Batch 150, Loss: 1.8494
Batch 175, Loss: 1.8463
Noise applied in 2248 out of 2688 batches, 83.63
Epoch 14 learning rate: 0.01
Epoch 14 time: 361.8493354320526 seconds
Epoch 14 accuracy: 13.3%
Batch 25, Loss: 1.8413
Batch 50, Loss: 1.8382
Batch 75, Loss: 1.8352
Batch 100, Loss: 1.8322
Batch 125, Loss: 1.8292
Batch 150, Loss: 1.8262
Batch 175, Loss: 1.8233
Noise applied in 2440 out of 2880 batches, 84.72
Epoch 15 learning rate: 0.01
Epoch 15 time: 353.281875371933 seconds
Epoch 15 accuracy: 13.21%
Batch 25, Loss: 1.8184
Batch 50, Loss: 1.8156
Batch 75, Loss: 1.8127
Batch 100, Loss: 1.8099
Batch 125, Loss: 1.8072
Batch 150, Loss: 1.8044
Batch 175, Loss: 1.8017
Noise applied in 2632 out of 3072 batches, 85.68
Epoch 16 learning rate: 0.01
Epoch 16 time: 353.24006056785583 seconds
Epoch 16 accuracy: 13.05%
Batch 25, Loss: 1.7971
Batch 50, Loss: 1.7944
Batch 75, Loss: 1.7918
Batch 100, Loss: 1.7891
Batch 125, Loss: 1.7865
Batch 150, Loss: 1.7840
Batch 175, Loss: 1.7815
Noise applied in 2824 out of 3264 batches, 86.52
Epoch 17 learning rate: 0.01
Epoch 17 time: 353.0004200935364 seconds
Epoch 17 accuracy: 13.01%
Batch 25, Loss: 1.7773
Batch 50, Loss: 1.7749
Batch 75, Loss: 1.7726
Batch 100, Loss: 1.7703
Batch 125, Loss: 1.7680
Batch 150, Loss: 1.7658
Batch 175, Loss: 1.7637
Noise applied in 3016 out of 3456 batches, 87.27
Epoch 18 learning rate: 0.01
Epoch 18 time: 353.35536766052246 seconds
Epoch 18 accuracy: 12.87%
Batch 25, Loss: 1.7601
Batch 50, Loss: 1.7581
Batch 75, Loss: 1.7561
Batch 100, Loss: 1.7542
Batch 125, Loss: 1.7523
Batch 150, Loss: 1.7505
Batch 175, Loss: 1.7487
Noise applied in 3208 out of 3648 batches, 87.94
Epoch 19 learning rate: 0.01
Epoch 19 time: 352.91615533828735 seconds
Epoch 19 accuracy: 12.69%
Batch 25, Loss: 1.7459
Batch 50, Loss: 1.7442
Batch 75, Loss: 1.7426
Batch 100, Loss: 1.7411
Batch 125, Loss: 1.7396
Batch 150, Loss: 1.7381
Batch 175, Loss: 1.7367
Noise applied in 3400 out of 3840 batches, 88.54
Epoch 20 learning rate: 0.01
Epoch 20 time: 368.88268971443176 seconds
Epoch 20 accuracy: 12.78%
Batch 25, Loss: 1.7344
Batch 50, Loss: 1.7331
Batch 75, Loss: 1.7318
Batch 100, Loss: 1.7306
Batch 125, Loss: 1.7294
Batch 150, Loss: 1.7283
Batch 175, Loss: 1.7272
Noise applied in 3592 out of 4032 batches, 89.09
Epoch 21 learning rate: 0.01
Epoch 21 time: 353.4985489845276 seconds
Epoch 21 accuracy: 12.63%
Batch 25, Loss: 1.7254
Batch 50, Loss: 1.7244
Batch 75, Loss: 1.7235
Batch 100, Loss: 1.7225
Batch 125, Loss: 1.7216
Batch 150, Loss: 1.7208
Batch 175, Loss: 1.7199
Noise applied in 3784 out of 4224 batches, 89.58
Epoch 22 learning rate: 0.01
Epoch 22 time: 352.71886134147644 seconds
Epoch 22 accuracy: 12.54%
Batch 25, Loss: 1.7186
Batch 50, Loss: 1.7178
Batch 75, Loss: 1.7171
Batch 100, Loss: 1.7163
Batch 125, Loss: 1.7156
Batch 150, Loss: 1.7150
Batch 175, Loss: 1.7143
Noise applied in 3976 out of 4416 batches, 90.04
Epoch 23 learning rate: 0.01
Epoch 23 time: 353.3979105949402 seconds
Epoch 23 accuracy: 12.79%
Batch 25, Loss: 1.7133
Batch 50, Loss: 1.7127
Batch 75, Loss: 1.7121
Batch 100, Loss: 1.7116
Batch 125, Loss: 1.7111
Batch 150, Loss: 1.7106
Batch 175, Loss: 1.7101
Noise applied in 4168 out of 4608 batches, 90.45
Epoch 24 learning rate: 0.01
Epoch 24 time: 353.385701417923 seconds
Epoch 24 accuracy: 12.92%
Batch 25, Loss: 1.7093
Batch 50, Loss: 1.7089
Batch 75, Loss: 1.7085
Batch 100, Loss: 1.7081
Batch 125, Loss: 1.7077
Batch 150, Loss: 1.7073
Batch 175, Loss: 1.7069
Noise applied in 4360 out of 4800 batches, 90.83
Epoch 25 learning rate: 0.01
Epoch 25 time: 353.16043519973755 seconds
Epoch 25 accuracy: 12.93%
Batch 25, Loss: 1.7062
Batch 50, Loss: 1.7059
Batch 75, Loss: 1.7055
Batch 100, Loss: 1.7052
Batch 125, Loss: 1.7049
Batch 150, Loss: 1.7045
Batch 175, Loss: 1.7042
Noise applied in 4552 out of 4992 batches, 91.19
Epoch 26 learning rate: 0.01
Epoch 26 time: 352.4595160484314 seconds
Epoch 26 accuracy: 12.97%
Batch 25, Loss: 1.7036
Batch 50, Loss: 1.7033
Batch 75, Loss: 1.7030
Batch 100, Loss: 1.7027
Batch 125, Loss: 1.7025
Batch 150, Loss: 1.7022
Batch 175, Loss: 1.7019
Noise applied in 4744 out of 5184 batches, 91.51
Epoch 27 learning rate: 0.01
Epoch 27 time: 353.4991955757141 seconds
Epoch 27 accuracy: 13.25%
Batch 25, Loss: 1.7015
Batch 50, Loss: 1.7013
Batch 75, Loss: 1.7011
Batch 100, Loss: 1.7009
Batch 125, Loss: 1.7006
Batch 150, Loss: 1.7004
Batch 175, Loss: 1.7002
Noise applied in 4936 out of 5376 batches, 91.82
Epoch 28 learning rate: 0.01
Epoch 28 time: 352.8826780319214 seconds
Epoch 28 accuracy: 13.26%
Batch 25, Loss: 1.6999
Batch 50, Loss: 1.6997
Batch 75, Loss: 1.6995
Batch 100, Loss: 1.6993
Batch 125, Loss: 1.6991
Batch 150, Loss: 1.6989
Batch 175, Loss: 1.6988
Noise applied in 5128 out of 5568 batches, 92.10
Epoch 29 learning rate: 0.01
Epoch 29 time: 352.61541175842285 seconds
Epoch 29 accuracy: 13.38%
Batch 25, Loss: 1.6985
Batch 50, Loss: 1.6984
Batch 75, Loss: 1.6982
Batch 100, Loss: 1.6981
Batch 125, Loss: 1.6980
Batch 150, Loss: 1.6979
Batch 175, Loss: 1.6978
Noise applied in 5320 out of 5760 batches, 92.36
Epoch 30 learning rate: 0.01
Epoch 30 time: 352.9953541755676 seconds
Epoch 30 accuracy: 13.33%
Batch 25, Loss: 1.6976
Batch 50, Loss: 1.6975
Batch 75, Loss: 1.6974
Batch 100, Loss: 1.6973
Batch 125, Loss: 1.6972
Batch 150, Loss: 1.6971
Batch 175, Loss: 1.6970
Noise applied in 5512 out of 5952 batches, 92.61
Epoch 31 learning rate: 0.01
Epoch 31 time: 353.51131868362427 seconds
Epoch 31 accuracy: 13.26%
Batch 25, Loss: 1.6969
Batch 50, Loss: 1.6968
Batch 75, Loss: 1.6967
Batch 100, Loss: 1.6967
Batch 125, Loss: 1.6966
Batch 150, Loss: 1.6965
Batch 175, Loss: 1.6965
Noise applied in 5704 out of 6144 batches, 92.84
Epoch 32 learning rate: 0.01
Epoch 32 time: 353.4647274017334 seconds
Epoch 32 accuracy: 13.29%
Batch 25, Loss: 1.6964
Batch 50, Loss: 1.6963
Batch 75, Loss: 1.6962
Batch 100, Loss: 1.6962
Batch 125, Loss: 1.6961
Batch 150, Loss: 1.6961
Batch 175, Loss: 1.6960
Noise applied in 5896 out of 6336 batches, 93.06
Epoch 33 learning rate: 0.01
Epoch 33 time: 354.5245895385742 seconds
Epoch 33 accuracy: 13.26%
Batch 25, Loss: 1.6959
Batch 50, Loss: 1.6958
Batch 75, Loss: 1.6958
Batch 100, Loss: 1.6957
Batch 125, Loss: 1.6956
Batch 150, Loss: 1.6956
Batch 175, Loss: 1.6955
Noise applied in 6088 out of 6528 batches, 93.26
Epoch 34 learning rate: 0.01
Epoch 34 time: 354.0358130931854 seconds
Epoch 34 accuracy: 13.29%
Batch 25, Loss: 1.6954
Batch 50, Loss: 1.6954
Batch 75, Loss: 1.6953
Batch 100, Loss: 1.6953
Batch 125, Loss: 1.6953
Batch 150, Loss: 1.6952
Batch 175, Loss: 1.6952
Noise applied in 6280 out of 6720 batches, 93.45
Epoch 35 learning rate: 0.01
Epoch 35 time: 353.57694268226624 seconds
Epoch 35 accuracy: 13.27%
Batch 25, Loss: 1.6951
Batch 50, Loss: 1.6951
Batch 75, Loss: 1.6950
Batch 100, Loss: 1.6950
Batch 125, Loss: 1.6949
Batch 150, Loss: 1.6949
Batch 175, Loss: 1.6949
Noise applied in 6472 out of 6912 batches, 93.63
Epoch 36 learning rate: 0.01
Epoch 36 time: 353.11050033569336 seconds
Epoch 36 accuracy: 13.28%
Batch 25, Loss: 1.6948
Batch 50, Loss: 1.6948
Batch 75, Loss: 1.6947
Batch 100, Loss: 1.6947
Batch 125, Loss: 1.6947
Batch 150, Loss: 1.6946
Batch 175, Loss: 1.6946
Noise applied in 6664 out of 7104 batches, 93.81
Epoch 37 learning rate: 0.01
Epoch 37 time: 353.3945426940918 seconds
Epoch 37 accuracy: 13.27%
Batch 25, Loss: 1.6945
Batch 50, Loss: 1.6945
Batch 75, Loss: 1.6945
Batch 100, Loss: 1.6944
Batch 125, Loss: 1.6944
Batch 150, Loss: 1.6944
Batch 175, Loss: 1.6944
Noise applied in 6856 out of 7296 batches, 93.97
Epoch 38 learning rate: 0.01
Epoch 38 time: 353.02256536483765 seconds
Epoch 38 accuracy: 13.35%
Batch 25, Loss: 1.6943
Batch 50, Loss: 1.6943
Batch 75, Loss: 1.6943
Batch 100, Loss: 1.6942
Batch 125, Loss: 1.6942
Batch 150, Loss: 1.6942
Batch 175, Loss: 1.6941
Noise applied in 7048 out of 7488 batches, 94.12
Epoch 39 learning rate: 0.01
Epoch 39 time: 353.3751471042633 seconds
Epoch 39 accuracy: 13.35%
Batch 25, Loss: 1.6941
Batch 50, Loss: 1.6941
Batch 75, Loss: 1.6940
Batch 100, Loss: 1.6940
Batch 125, Loss: 1.6940
Batch 150, Loss: 1.6940
Batch 175, Loss: 1.6940
Noise applied in 7240 out of 7680 batches, 94.27
Epoch 40 learning rate: 0.01
Epoch 40 time: 353.3907699584961 seconds
Epoch 40 accuracy: 13.42%
Batch 25, Loss: 1.6939
Batch 50, Loss: 1.6939
Batch 75, Loss: 1.6939
Batch 100, Loss: 1.6939
Batch 125, Loss: 1.6938
Batch 150, Loss: 1.6938
Batch 175, Loss: 1.6938
Noise applied in 7432 out of 7872 batches, 94.41
Epoch 41 learning rate: 0.01
Epoch 41 time: 353.6707673072815 seconds
Epoch 41 accuracy: 13.48%
Batch 25, Loss: 1.6937
Batch 50, Loss: 1.6937
Batch 75, Loss: 1.6937
Batch 100, Loss: 1.6937
Batch 125, Loss: 1.6937
Batch 150, Loss: 1.6937
Batch 175, Loss: 1.6936
Noise applied in 7624 out of 8064 batches, 94.54
Epoch 42 learning rate: 0.01
Epoch 42 time: 353.6756706237793 seconds
Epoch 42 accuracy: 13.55%
Batch 25, Loss: 1.6936
Batch 50, Loss: 1.6936
Batch 75, Loss: 1.6936
Batch 100, Loss: 1.6936
Batch 125, Loss: 1.6936
Batch 150, Loss: 1.6935
Batch 175, Loss: 1.6935
Noise applied in 7816 out of 8256 batches, 94.67
Epoch 43 learning rate: 0.01
Epoch 43 time: 352.7706596851349 seconds
Epoch 43 accuracy: 13.6%
Batch 25, Loss: 1.6935
Batch 50, Loss: 1.6935
Batch 75, Loss: 1.6935
Batch 100, Loss: 1.6935
Batch 125, Loss: 1.6935
Batch 150, Loss: 1.6935
Batch 175, Loss: 1.6934
Noise applied in 8008 out of 8448 batches, 94.79
Epoch 44 learning rate: 0.01
Epoch 44 time: 353.09996247291565 seconds
Epoch 44 accuracy: 13.66%
Batch 25, Loss: 1.6934
Batch 50, Loss: 1.6934
Batch 75, Loss: 1.6934
Batch 100, Loss: 1.6934
Batch 125, Loss: 1.6934
Batch 150, Loss: 1.6934
Batch 175, Loss: 1.6934
Noise applied in 8200 out of 8640 batches, 94.91
Epoch 45 learning rate: 0.01
Epoch 45 time: 352.356662273407 seconds
Epoch 45 accuracy: 13.74%
Batch 25, Loss: 1.6934
Batch 50, Loss: 1.6934
Batch 75, Loss: 1.6934
Batch 100, Loss: 1.6934
Batch 125, Loss: 1.6934
Batch 150, Loss: 1.6934
Batch 175, Loss: 1.6934
Noise applied in 8392 out of 8832 batches, 95.02
Epoch 46 learning rate: 0.01
Epoch 46 time: 353.50907039642334 seconds
Epoch 46 accuracy: 13.79%
Batch 25, Loss: 1.6934
Batch 50, Loss: 1.6934
Batch 75, Loss: 1.6934
Batch 100, Loss: 1.6934
Batch 125, Loss: 1.6934
Batch 150, Loss: 1.6934
Batch 175, Loss: 1.6934
Noise applied in 8584 out of 9024 batches, 95.12
Epoch 47 learning rate: 0.01
Epoch 47 time: 353.13243794441223 seconds
Epoch 47 accuracy: 13.86%
Batch 25, Loss: 1.6935
Batch 50, Loss: 1.6934
Batch 75, Loss: 1.6934
Batch 100, Loss: 1.6934
Batch 125, Loss: 1.6934
Batch 150, Loss: 1.6935
Batch 175, Loss: 1.6935
Noise applied in 8776 out of 9216 batches, 95.23
Epoch 48 learning rate: 0.01
Epoch 48 time: 355.07061767578125 seconds
Epoch 48 accuracy: 13.84%
Batch 25, Loss: 1.6935
Batch 50, Loss: 1.6935
Batch 75, Loss: 1.6935
Batch 100, Loss: 1.6935
Batch 125, Loss: 1.6935
Batch 150, Loss: 1.6935
Batch 175, Loss: 1.6935
Noise applied in 8968 out of 9408 batches, 95.32
Epoch 49 learning rate: 0.01
Epoch 49 time: 353.30405020713806 seconds
Epoch 49 accuracy: 13.83%
Batch 25, Loss: 1.6935
Batch 50, Loss: 1.6935
Batch 75, Loss: 1.6935
Batch 100, Loss: 1.6936
Batch 125, Loss: 1.6936
Batch 150, Loss: 1.6936
Batch 175, Loss: 1.6936
Noise applied in 9160 out of 9600 batches, 95.42
Epoch 50 learning rate: 0.01
Epoch 50 time: 354.21812105178833 seconds
Epoch 50 accuracy: 13.82%
Batch 25, Loss: 1.6936
Batch 50, Loss: 1.6936
Batch 75, Loss: 1.6936
Batch 100, Loss: 1.6936
Batch 125, Loss: 1.6937
Batch 150, Loss: 1.6937
Batch 175, Loss: 1.6937
Noise applied in 9352 out of 9792 batches, 95.51
Epoch 51 learning rate: 0.01
Epoch 51 time: 353.9027011394501 seconds
Epoch 51 accuracy: 13.87%
Batch 25, Loss: 1.6937
Batch 50, Loss: 1.6937
Batch 75, Loss: 1.6937
Batch 100, Loss: 1.6938
Batch 125, Loss: 1.6938
Batch 150, Loss: 1.6938
Batch 175, Loss: 1.6938
Noise applied in 9544 out of 9984 batches, 95.59
Epoch 52 learning rate: 0.01
Epoch 52 time: 352.68607568740845 seconds
Epoch 52 accuracy: 13.87%
Batch 25, Loss: 1.6939
Batch 50, Loss: 1.6939
Batch 75, Loss: 1.6939
Batch 100, Loss: 1.6939
Batch 125, Loss: 1.6939
Batch 150, Loss: 1.6940
Batch 175, Loss: 1.6940
Noise applied in 9736 out of 10176 batches, 95.68
Epoch 53 learning rate: 0.01
Epoch 53 time: 353.6483669281006 seconds
Epoch 53 accuracy: 13.92%
Batch 25, Loss: 1.6940
Batch 50, Loss: 1.6940
Batch 75, Loss: 1.6940
Batch 100, Loss: 1.6941
Batch 125, Loss: 1.6941
Batch 150, Loss: 1.6941
Batch 175, Loss: 1.6941
Noise applied in 9928 out of 10368 batches, 95.76
Epoch 54 learning rate: 0.01
Epoch 54 time: 353.35146498680115 seconds
Epoch 54 accuracy: 13.9%
Batch 25, Loss: 1.6942
Batch 50, Loss: 1.6942
Batch 75, Loss: 1.6942
Batch 100, Loss: 1.6942
Batch 125, Loss: 1.6942
Batch 150, Loss: 1.6942
Batch 175, Loss: 1.6943
Noise applied in 10120 out of 10560 batches, 95.83
Epoch 55 learning rate: 0.01
Epoch 55 time: 353.64261531829834 seconds
Epoch 55 accuracy: 13.91%
Batch 25, Loss: 1.6943
Batch 50, Loss: 1.6943
Batch 75, Loss: 1.6943
Batch 100, Loss: 1.6943
Batch 125, Loss: 1.6944
Batch 150, Loss: 1.6944
Batch 175, Loss: 1.6944
Noise applied in 10312 out of 10752 batches, 95.91
Epoch 56 learning rate: 0.01
Epoch 56 time: 353.78556418418884 seconds
Epoch 56 accuracy: 13.97%
Batch 25, Loss: 1.6944
Batch 50, Loss: 1.6944
Batch 75, Loss: 1.6945
Batch 100, Loss: 1.6945
Batch 125, Loss: 1.6945
Batch 150, Loss: 1.6945
Batch 175, Loss: 1.6946
Noise applied in 10504 out of 10944 batches, 95.98
Epoch 57 learning rate: 0.01
Epoch 57 time: 353.71956276893616 seconds
Epoch 57 accuracy: 13.9%
Batch 25, Loss: 1.6946
Batch 50, Loss: 1.6946
Batch 75, Loss: 1.6947
Batch 100, Loss: 1.6947
Batch 125, Loss: 1.6947
Batch 150, Loss: 1.6947
Batch 175, Loss: 1.6947
Noise applied in 10696 out of 11136 batches, 96.05
Epoch 58 learning rate: 0.01
Epoch 58 time: 353.5862820148468 seconds
Epoch 58 accuracy: 13.92%
Batch 25, Loss: 1.6948
Batch 50, Loss: 1.6948
Batch 75, Loss: 1.6948
Batch 100, Loss: 1.6949
Batch 125, Loss: 1.6949
Batch 150, Loss: 1.6949
Batch 175, Loss: 1.6949
Noise applied in 10888 out of 11328 batches, 96.12
Epoch 59 learning rate: 0.01
Epoch 59 time: 353.12137150764465 seconds
Epoch 59 accuracy: 13.86%
Batch 25, Loss: 1.6950
Batch 50, Loss: 1.6950
Batch 75, Loss: 1.6950
Batch 100, Loss: 1.6950
Batch 125, Loss: 1.6951
Batch 150, Loss: 1.6951
Batch 175, Loss: 1.6951
Noise applied in 11080 out of 11520 batches, 96.18
Epoch 60 learning rate: 0.01
Epoch 60 time: 491.4814827442169 seconds
Epoch 60 accuracy: 13.8%
Batch 25, Loss: 1.6952
Batch 50, Loss: 1.6952
Batch 75, Loss: 1.6952
Batch 100, Loss: 1.6952
Batch 125, Loss: 1.6953
Batch 150, Loss: 1.6953
Batch 175, Loss: 1.6953
Noise applied in 11272 out of 11712 batches, 96.24
Epoch 61 learning rate: 0.01
Epoch 61 time: 378.79989886283875 seconds
Epoch 61 accuracy: 13.8%
Batch 25, Loss: 1.6954
Batch 50, Loss: 1.6954
Batch 75, Loss: 1.6954
Batch 100, Loss: 1.6955
Batch 125, Loss: 1.6955
Batch 150, Loss: 1.6955
Batch 175, Loss: 1.6956
Noise applied in 11464 out of 11904 batches, 96.30
Epoch 62 learning rate: 0.01
Epoch 62 time: 368.76822686195374 seconds
Epoch 62 accuracy: 13.8%
Batch 25, Loss: 1.6956
Batch 50, Loss: 1.6956
Batch 75, Loss: 1.6957
Batch 100, Loss: 1.6957
Batch 125, Loss: 1.6957
Batch 150, Loss: 1.6958
Batch 175, Loss: 1.6958
Noise applied in 11656 out of 12096 batches, 96.36
Epoch 63 learning rate: 0.01
Epoch 63 time: 359.47702074050903 seconds
Epoch 63 accuracy: 13.86%
Batch 25, Loss: 1.6959
Batch 50, Loss: 1.6959
Batch 75, Loss: 1.6959
Batch 100, Loss: 1.6959
Batch 125, Loss: 1.6960
Batch 150, Loss: 1.6960
Batch 175, Loss: 1.6960
Noise applied in 11848 out of 12288 batches, 96.42
Epoch 64 learning rate: 0.01
Epoch 64 time: 380.0635199546814 seconds
Epoch 64 accuracy: 13.89%
Batch 25, Loss: 1.6961
Batch 50, Loss: 1.6961
Batch 75, Loss: 1.6962
Batch 100, Loss: 1.6962
Batch 125, Loss: 1.6962
Batch 150, Loss: 1.6963
Batch 175, Loss: 1.6963
Noise applied in 12040 out of 12480 batches, 96.47
Epoch 65 learning rate: 0.01
Epoch 65 time: 370.86322689056396 seconds
Epoch 65 accuracy: 13.93%
Batch 25, Loss: 1.6964
Batch 50, Loss: 1.6964
Batch 75, Loss: 1.6964
Batch 100, Loss: 1.6965
Batch 125, Loss: 1.6965
Batch 150, Loss: 1.6965
Batch 175, Loss: 1.6966
Noise applied in 12232 out of 12672 batches, 96.53
Epoch 66 learning rate: 0.01
Epoch 66 time: 371.02243661880493 seconds
Epoch 66 accuracy: 13.95%
Batch 25, Loss: 1.6966
Batch 50, Loss: 1.6967
Batch 75, Loss: 1.6967
Batch 100, Loss: 1.6967
Batch 125, Loss: 1.6968
Batch 150, Loss: 1.6968
Batch 175, Loss: 1.6968
Noise applied in 12424 out of 12864 batches, 96.58
Epoch 67 learning rate: 0.01
Epoch 67 time: 387.33359384536743 seconds
Epoch 67 accuracy: 13.99%
Batch 25, Loss: 1.6969
Batch 50, Loss: 1.6969
Batch 75, Loss: 1.6970
Batch 100, Loss: 1.6970
Batch 125, Loss: 1.6970
Batch 150, Loss: 1.6971
Batch 175, Loss: 1.6971
Noise applied in 12616 out of 13056 batches, 96.63
Epoch 68 learning rate: 0.01
Epoch 68 time: 448.2640459537506 seconds
Epoch 68 accuracy: 14.0%
Batch 25, Loss: 1.6972
Batch 50, Loss: 1.6972
Batch 75, Loss: 1.6972
Batch 100, Loss: 1.6973
Batch 125, Loss: 1.6973
Batch 150, Loss: 1.6973
Batch 175, Loss: 1.6974
Noise applied in 12808 out of 13248 batches, 96.68
Epoch 69 learning rate: 0.01
Epoch 69 time: 428.60886216163635 seconds
Epoch 69 accuracy: 14.03%
Batch 25, Loss: 1.6974
Batch 50, Loss: 1.6975
Batch 75, Loss: 1.6975
Batch 100, Loss: 1.6975
Batch 125, Loss: 1.6976
Batch 150, Loss: 1.6976
Batch 175, Loss: 1.6976
Noise applied in 13000 out of 13440 batches, 96.73
Epoch 70 learning rate: 0.01
Epoch 70 time: 389.0327615737915 seconds
Epoch 70 accuracy: 14.0%
Batch 25, Loss: 1.6977
Batch 50, Loss: 1.6977
Batch 75, Loss: 1.6978
Batch 100, Loss: 1.6978
Batch 125, Loss: 1.6979
Batch 150, Loss: 1.6979
Batch 175, Loss: 1.6979
Noise applied in 13192 out of 13632 batches, 96.77
Epoch 71 learning rate: 0.01
Epoch 71 time: 420.41686391830444 seconds
Epoch 71 accuracy: 14.05%
Batch 25, Loss: 1.6980
Batch 50, Loss: 1.6980
Batch 75, Loss: 1.6981
Batch 100, Loss: 1.6981
Batch 125, Loss: 1.6982
Batch 150, Loss: 1.6982
Batch 175, Loss: 1.6982
Noise applied in 13384 out of 13824 batches, 96.82
Epoch 72 learning rate: 0.01
Epoch 72 time: 396.2729332447052 seconds
Epoch 72 accuracy: 14.06%
Batch 25, Loss: 1.6983
Batch 50, Loss: 1.6983
Batch 75, Loss: 1.6984
Batch 100, Loss: 1.6984
Batch 125, Loss: 1.6985
Batch 150, Loss: 1.6985
Batch 175, Loss: 1.6985
Noise applied in 13576 out of 14016 batches, 96.86
Epoch 73 learning rate: 0.01
Epoch 73 time: 462.5939071178436 seconds
Epoch 73 accuracy: 13.98%
Batch 25, Loss: 1.6986
Batch 50, Loss: 1.6986
Batch 75, Loss: 1.6987
Batch 100, Loss: 1.6987
Batch 125, Loss: 1.6988
Batch 150, Loss: 1.6988
Batch 175, Loss: 1.6988
Noise applied in 13768 out of 14208 batches, 96.90
Epoch 74 learning rate: 0.01
Epoch 74 time: 376.34998846054077 seconds
Epoch 74 accuracy: 13.98%
Batch 25, Loss: 1.6989
Batch 50, Loss: 1.6989
Batch 75, Loss: 1.6990
Batch 100, Loss: 1.6990
Batch 125, Loss: 1.6990
Batch 150, Loss: 1.6991
Batch 175, Loss: 1.6991
Noise applied in 13960 out of 14400 batches, 96.94
Epoch 75 learning rate: 0.01
Epoch 75 time: 380.3311686515808 seconds
Epoch 75 accuracy: 13.92%
Batch 25, Loss: 1.6992
Batch 50, Loss: 1.6992
Batch 75, Loss: 1.6993
Batch 100, Loss: 1.6993
Batch 125, Loss: 1.6993
Batch 150, Loss: 1.6994
Batch 175, Loss: 1.6994
Noise applied in 14152 out of 14592 batches, 96.98
Epoch 76 learning rate: 0.01
Epoch 76 time: 389.47309851646423 seconds
Epoch 76 accuracy: 13.89%
Batch 25, Loss: 1.6995
Batch 50, Loss: 1.6995
Batch 75, Loss: 1.6996
Batch 100, Loss: 1.6996
Batch 125, Loss: 1.6997
Batch 150, Loss: 1.6997
Batch 175, Loss: 1.6997
Noise applied in 14344 out of 14784 batches, 97.02
Epoch 77 learning rate: 0.01
Epoch 77 time: 410.3287456035614 seconds
Epoch 77 accuracy: 13.93%
Batch 25, Loss: 1.6998
Batch 50, Loss: 1.6998
Batch 75, Loss: 1.6999
Batch 100, Loss: 1.6999
Batch 125, Loss: 1.7000
Batch 150, Loss: 1.7000
Batch 175, Loss: 1.7001
Noise applied in 14536 out of 14976 batches, 97.06
Epoch 78 learning rate: 0.01
Epoch 78 time: 367.4459626674652 seconds
Epoch 78 accuracy: 13.87%
Batch 25, Loss: 1.7001
Batch 50, Loss: 1.7002
Batch 75, Loss: 1.7002
Batch 100, Loss: 1.7003
Batch 125, Loss: 1.7003
Batch 150, Loss: 1.7003
Batch 175, Loss: 1.7004
Noise applied in 14728 out of 15168 batches, 97.10
Epoch 79 learning rate: 0.01
Epoch 79 time: 458.2650148868561 seconds
Epoch 79 accuracy: 13.87%
Batch 25, Loss: 1.7004
Batch 50, Loss: 1.7005
Batch 75, Loss: 1.7005
Batch 100, Loss: 1.7006
Batch 125, Loss: 1.7006
Batch 150, Loss: 1.7006
Batch 175, Loss: 1.7007
Noise applied in 14920 out of 15360 batches, 97.14
Epoch 80 learning rate: 0.01
Epoch 80 time: 389.02543902397156 seconds
Epoch 80 accuracy: 13.86%
Batch 25, Loss: 1.7008
Batch 50, Loss: 1.7008
Batch 75, Loss: 1.7009
Batch 100, Loss: 1.7009
Batch 125, Loss: 1.7009
Batch 150, Loss: 1.7010
Batch 175, Loss: 1.7010
Noise applied in 15112 out of 15552 batches, 97.17
Epoch 81 learning rate: 0.01
Epoch 81 time: 366.09190034866333 seconds
Epoch 81 accuracy: 13.85%
Batch 25, Loss: 1.7011
Batch 50, Loss: 1.7011
Batch 75, Loss: 1.7012
Batch 100, Loss: 1.7012
Batch 125, Loss: 1.7013
Batch 150, Loss: 1.7013
Batch 175, Loss: 1.7014
Noise applied in 15304 out of 15744 batches, 97.21
Epoch 82 learning rate: 0.01
Epoch 82 time: 389.55428743362427 seconds
Epoch 82 accuracy: 13.88%
Batch 25, Loss: 1.7014
Batch 50, Loss: 1.7015
Batch 75, Loss: 1.7015
Batch 100, Loss: 1.7015
Batch 125, Loss: 1.7016
Batch 150, Loss: 1.7016
Batch 175, Loss: 1.7017
Noise applied in 15496 out of 15936 batches, 97.24
Epoch 83 learning rate: 0.01
Epoch 83 time: 370.9979774951935 seconds
Epoch 83 accuracy: 13.92%
Batch 25, Loss: 1.7018
Batch 50, Loss: 1.7018
Batch 75, Loss: 1.7018
Batch 100, Loss: 1.7019
Batch 125, Loss: 1.7019
Batch 150, Loss: 1.7020
Batch 175, Loss: 1.7020
Noise applied in 15688 out of 16128 batches, 97.27
Epoch 84 learning rate: 0.01
Epoch 84 time: 624.0412583351135 seconds
Epoch 84 accuracy: 13.91%
Batch 25, Loss: 1.7021
Batch 50, Loss: 1.7021
Batch 75, Loss: 1.7022
Batch 100, Loss: 1.7022
Batch 125, Loss: 1.7023
Batch 150, Loss: 1.7023
Batch 175, Loss: 1.7023
Noise applied in 15880 out of 16320 batches, 97.30
Epoch 85 learning rate: 0.01
Epoch 85 time: 372.4529149532318 seconds
Epoch 85 accuracy: 13.96%
Batch 25, Loss: 1.7024
Batch 50, Loss: 1.7025
Batch 75, Loss: 1.7025
Batch 100, Loss: 1.7025
Batch 125, Loss: 1.7026
Batch 150, Loss: 1.7026
Batch 175, Loss: 1.7027
Noise applied in 16072 out of 16512 batches, 97.34
Epoch 86 learning rate: 0.01
Epoch 86 time: 361.66190791130066 seconds
Epoch 86 accuracy: 13.96%
Batch 25, Loss: 1.7027
Batch 50, Loss: 1.7028
Batch 75, Loss: 1.7028
Batch 100, Loss: 1.7029
Batch 125, Loss: 1.7029
Batch 150, Loss: 1.7029
Batch 175, Loss: 1.7030
Noise applied in 16264 out of 16704 batches, 97.37
Epoch 87 learning rate: 0.01
Epoch 87 time: 392.847284078598 seconds
Epoch 87 accuracy: 13.99%
Batch 25, Loss: 1.7031
Batch 50, Loss: 1.7031
Batch 75, Loss: 1.7031
Batch 100, Loss: 1.7032
Batch 125, Loss: 1.7032
Batch 150, Loss: 1.7033
Batch 175, Loss: 1.7033
Noise applied in 16456 out of 16896 batches, 97.40
Epoch 88 learning rate: 0.01
Epoch 88 time: 371.34491896629333 seconds
Epoch 88 accuracy: 14.01%
Batch 25, Loss: 1.7034
Batch 50, Loss: 1.7034
Batch 75, Loss: 1.7035
Batch 100, Loss: 1.7035
Batch 125, Loss: 1.7036
Batch 150, Loss: 1.7036
Batch 175, Loss: 1.7036
Noise applied in 16648 out of 17088 batches, 97.43
Epoch 89 learning rate: 0.01
Epoch 89 time: 372.1313056945801 seconds
Epoch 89 accuracy: 14.02%
Batch 25, Loss: 1.7037
Batch 50, Loss: 1.7038
Batch 75, Loss: 1.7038
Batch 100, Loss: 1.7038
Batch 125, Loss: 1.7039
Batch 150, Loss: 1.7039
Batch 175, Loss: 1.7040
Noise applied in 16840 out of 17280 batches, 97.45
Epoch 90 learning rate: 0.01
Epoch 90 time: 391.8641176223755 seconds
Epoch 90 accuracy: 14.0%
Batch 25, Loss: 1.7040
Batch 50, Loss: 1.7041
Batch 75, Loss: 1.7041
Batch 100, Loss: 1.7041
Batch 125, Loss: 1.7042
Batch 150, Loss: 1.7042
Batch 175, Loss: 1.7043
Noise applied in 17032 out of 17472 batches, 97.48
Epoch 91 learning rate: 0.01
Epoch 91 time: 391.88480257987976 seconds
Epoch 91 accuracy: 14.0%
Batch 25, Loss: 1.7043
Batch 50, Loss: 1.7044
Batch 75, Loss: 1.7044
Batch 100, Loss: 1.7045
Batch 125, Loss: 1.7045
Batch 150, Loss: 1.7045
Batch 175, Loss: 1.7046
Noise applied in 17224 out of 17664 batches, 97.51
Epoch 92 learning rate: 0.01
Epoch 92 time: 368.94789457321167 seconds
Epoch 92 accuracy: 14.05%
Batch 25, Loss: 1.7047
Batch 50, Loss: 1.7047
Batch 75, Loss: 1.7047
Batch 100, Loss: 1.7048
Batch 125, Loss: 1.7048
Batch 150, Loss: 1.7049
Batch 175, Loss: 1.7049
Noise applied in 17416 out of 17856 batches, 97.54
Epoch 93 learning rate: 0.01
Epoch 93 time: 364.68210196495056 seconds
Epoch 93 accuracy: 14.03%
Batch 25, Loss: 1.7050
Batch 50, Loss: 1.7050
Batch 75, Loss: 1.7051
Batch 100, Loss: 1.7051
Batch 125, Loss: 1.7052
Batch 150, Loss: 1.7052
Batch 175, Loss: 1.7052
Noise applied in 17608 out of 18048 batches, 97.56
Epoch 94 learning rate: 0.01
Epoch 94 time: 376.79649543762207 seconds
Epoch 94 accuracy: 14.03%
Batch 25, Loss: 1.7053
Batch 50, Loss: 1.7054
Batch 75, Loss: 1.7054
Batch 100, Loss: 1.7054
Batch 125, Loss: 1.7055
Batch 150, Loss: 1.7055
Batch 175, Loss: 1.7056
Noise applied in 17800 out of 18240 batches, 97.59
Epoch 95 learning rate: 0.01
Epoch 95 time: 365.6721992492676 seconds
Epoch 95 accuracy: 14.06%
Batch 25, Loss: 1.7056
Batch 50, Loss: 1.7057
Batch 75, Loss: 1.7057
Batch 100, Loss: 1.7058
Batch 125, Loss: 1.7058
Batch 150, Loss: 1.7059
Batch 175, Loss: 1.7059
Noise applied in 17992 out of 18432 batches, 97.61
Epoch 96 learning rate: 0.01
Epoch 96 time: 359.7336690425873 seconds
Epoch 96 accuracy: 14.04%
Batch 25, Loss: 1.7060
Batch 50, Loss: 1.7060
Batch 75, Loss: 1.7061
Batch 100, Loss: 1.7061
Batch 125, Loss: 1.7061
Batch 150, Loss: 1.7062
Batch 175, Loss: 1.7062
Noise applied in 18184 out of 18624 batches, 97.64
Epoch 97 learning rate: 0.01
Epoch 97 time: 374.1875751018524 seconds
Epoch 97 accuracy: 14.04%
Batch 25, Loss: 1.7063
Batch 50, Loss: 1.7063
Batch 75, Loss: 1.7064
Batch 100, Loss: 1.7064
Batch 125, Loss: 1.7065
Batch 150, Loss: 1.7065
Batch 175, Loss: 1.7066
Noise applied in 18376 out of 18816 batches, 97.66
Epoch 98 learning rate: 0.01
Epoch 98 time: 362.3863217830658 seconds
Epoch 98 accuracy: 13.93%
Batch 25, Loss: 1.7066
Batch 50, Loss: 1.7067
Batch 75, Loss: 1.7067
Batch 100, Loss: 1.7068
Batch 125, Loss: 1.7068
Batch 150, Loss: 1.7069
Batch 175, Loss: 1.7069
Noise applied in 18568 out of 19008 batches, 97.69
Epoch 99 learning rate: 0.01
Epoch 99 time: 354.0032787322998 seconds
Epoch 99 accuracy: 13.93%
Batch 25, Loss: 1.7070
Batch 50, Loss: 1.7070
Batch 75, Loss: 1.7071
Batch 100, Loss: 1.7071
Batch 125, Loss: 1.7071
Batch 150, Loss: 1.7072
Batch 175, Loss: 1.7072
Noise applied in 18760 out of 19200 batches, 97.71
Epoch 100 learning rate: 0.01
Epoch 100 time: 353.54083132743835 seconds
Epoch 100 accuracy: 13.97%
Batch 25, Loss: 1.7073
Batch 50, Loss: 1.7074
Batch 75, Loss: 1.7074
Batch 100, Loss: 1.7074
Batch 125, Loss: 1.7075
Batch 150, Loss: 1.7075
Batch 175, Loss: 1.7076
Noise applied in 18952 out of 19392 batches, 97.73
Epoch 101 learning rate: 0.01
Epoch 101 time: 353.7600169181824 seconds
Epoch 101 accuracy: 14.01%
Batch 25, Loss: 1.7076
Batch 50, Loss: 1.7077
Batch 75, Loss: 1.7077
Batch 100, Loss: 1.7078
Batch 125, Loss: 1.7078
Batch 150, Loss: 1.7079
Batch 175, Loss: 1.7079
Noise applied in 19144 out of 19584 batches, 97.75
Epoch 102 learning rate: 0.01
Epoch 102 time: 353.22295331954956 seconds
Epoch 102 accuracy: 13.96%
Batch 25, Loss: 1.7080
Batch 50, Loss: 1.7080
Batch 75, Loss: 1.7081
Batch 100, Loss: 1.7081
Batch 125, Loss: 1.7081
Batch 150, Loss: 1.7082
Batch 175, Loss: 1.7082
Noise applied in 19336 out of 19776 batches, 97.78
Epoch 103 learning rate: 0.01
Epoch 103 time: 426.44900727272034 seconds
Epoch 103 accuracy: 13.97%
Batch 25, Loss: 1.7083
Batch 50, Loss: 1.7083
Batch 75, Loss: 1.7084
Batch 100, Loss: 1.7084
Batch 125, Loss: 1.7085
Batch 150, Loss: 1.7085
Batch 175, Loss: 1.7085
Noise applied in 19528 out of 19968 batches, 97.80
Epoch 104 learning rate: 0.01
Epoch 104 time: 354.1701078414917 seconds
Epoch 104 accuracy: 13.98%
Batch 25, Loss: 1.7086
Batch 50, Loss: 1.7087
Batch 75, Loss: 1.7087
Batch 100, Loss: 1.7087
Batch 125, Loss: 1.7088
Batch 150, Loss: 1.7088
Batch 175, Loss: 1.7089
Noise applied in 19720 out of 20160 batches, 97.82
Epoch 105 learning rate: 0.01
Epoch 105 time: 353.4465138912201 seconds
Epoch 105 accuracy: 13.93%
Batch 25, Loss: 1.7089
Batch 50, Loss: 1.7090
Batch 75, Loss: 1.7090
Batch 100, Loss: 1.7091
Batch 125, Loss: 1.7091
Batch 150, Loss: 1.7092
Batch 175, Loss: 1.7092
Noise applied in 19912 out of 20352 batches, 97.84
Epoch 106 learning rate: 0.01
Epoch 106 time: 353.01670575141907 seconds
Epoch 106 accuracy: 13.9%
Batch 25, Loss: 1.7093
Batch 50, Loss: 1.7093
Batch 75, Loss: 1.7094
Batch 100, Loss: 1.7094
Batch 125, Loss: 1.7094
Batch 150, Loss: 1.7095
Batch 175, Loss: 1.7095
Noise applied in 20104 out of 20544 batches, 97.86
Epoch 107 learning rate: 0.01
Epoch 107 time: 353.848016500473 seconds
Epoch 107 accuracy: 13.9%
Batch 25, Loss: 1.7096
Batch 50, Loss: 1.7096
Batch 75, Loss: 1.7097
Batch 100, Loss: 1.7097
Batch 125, Loss: 1.7098
Batch 150, Loss: 1.7098
Batch 175, Loss: 1.7099
Noise applied in 20296 out of 20736 batches, 97.88
Epoch 108 learning rate: 0.01
Epoch 108 time: 353.78929328918457 seconds
Epoch 108 accuracy: 13.91%
Batch 25, Loss: 1.7099
Batch 50, Loss: 1.7100
Batch 75, Loss: 1.7100
Batch 100, Loss: 1.7101
Batch 125, Loss: 1.7101
Batch 150, Loss: 1.7101
Batch 175, Loss: 1.7102
Noise applied in 20488 out of 20928 batches, 97.90
Epoch 109 learning rate: 0.01
Epoch 109 time: 352.8738887310028 seconds
Epoch 109 accuracy: 13.97%
Batch 25, Loss: 1.7103
Batch 50, Loss: 1.7103
Batch 75, Loss: 1.7103
Batch 100, Loss: 1.7104
Batch 125, Loss: 1.7104
Batch 150, Loss: 1.7105
Batch 175, Loss: 1.7105
Noise applied in 20680 out of 21120 batches, 97.92
Epoch 110 learning rate: 0.01
Epoch 110 time: 353.7766206264496 seconds
Epoch 110 accuracy: 13.95%
Batch 25, Loss: 1.7106
Batch 50, Loss: 1.7106
Batch 75, Loss: 1.7107
Batch 100, Loss: 1.7107
Batch 125, Loss: 1.7107
Batch 150, Loss: 1.7108
Batch 175, Loss: 1.7108
Noise applied in 20872 out of 21312 batches, 97.94
Epoch 111 learning rate: 0.01
Epoch 111 time: 353.31495428085327 seconds
Epoch 111 accuracy: 13.97%
Batch 25, Loss: 1.7109
Batch 50, Loss: 1.7109
Batch 75, Loss: 1.7110
Batch 100, Loss: 1.7110
Batch 125, Loss: 1.7111
Batch 150, Loss: 1.7111
Batch 175, Loss: 1.7112
Noise applied in 21064 out of 21504 batches, 97.95
Epoch 112 learning rate: 0.01
Epoch 112 time: 353.02607440948486 seconds
Epoch 112 accuracy: 14.02%
Batch 25, Loss: 1.7112
Batch 50, Loss: 1.7113
Batch 75, Loss: 1.7113
Batch 100, Loss: 1.7114
Batch 125, Loss: 1.7114
Batch 150, Loss: 1.7114
Batch 175, Loss: 1.7115
Noise applied in 21256 out of 21696 batches, 97.97
Epoch 113 learning rate: 0.01
Epoch 113 time: 353.93965697288513 seconds
Epoch 113 accuracy: 14.03%
Batch 25, Loss: 1.7116
Batch 50, Loss: 1.7116
Batch 75, Loss: 1.7116
Batch 100, Loss: 1.7117
Batch 125, Loss: 1.7117
Batch 150, Loss: 1.7118
Batch 175, Loss: 1.7118
Noise applied in 21448 out of 21888 batches, 97.99
Epoch 114 learning rate: 0.01
Epoch 114 time: 354.1194531917572 seconds
Epoch 114 accuracy: 14.03%
Batch 25, Loss: 1.7119
Batch 50, Loss: 1.7119
Batch 75, Loss: 1.7120
Batch 100, Loss: 1.7120
Batch 125, Loss: 1.7121
Batch 150, Loss: 1.7121
Batch 175, Loss: 1.7122
Noise applied in 21640 out of 22080 batches, 98.01
Epoch 115 learning rate: 0.01
Epoch 115 time: 353.3570353984833 seconds
Epoch 115 accuracy: 14.04%
Batch 25, Loss: 1.7122
Batch 50, Loss: 1.7123
Batch 75, Loss: 1.7123
Batch 100, Loss: 1.7123
Batch 125, Loss: 1.7124
Batch 150, Loss: 1.7124
Batch 175, Loss: 1.7125
Noise applied in 21832 out of 22272 batches, 98.02
Epoch 116 learning rate: 0.01
Epoch 116 time: 353.067848443985 seconds
Epoch 116 accuracy: 14.04%
Batch 25, Loss: 1.7125
Batch 50, Loss: 1.7126
Batch 75, Loss: 1.7126
Batch 100, Loss: 1.7127
Batch 125, Loss: 1.7127
Batch 150, Loss: 1.7128
Batch 175, Loss: 1.7128
Noise applied in 22024 out of 22464 batches, 98.04
Epoch 117 learning rate: 0.01
Epoch 117 time: 354.26484847068787 seconds
Epoch 117 accuracy: 14.05%
Batch 25, Loss: 1.7129
Batch 50, Loss: 1.7129
Batch 75, Loss: 1.7130
Batch 100, Loss: 1.7130
Batch 125, Loss: 1.7130
Batch 150, Loss: 1.7131
Batch 175, Loss: 1.7131
Noise applied in 22216 out of 22656 batches, 98.06
Epoch 118 learning rate: 0.01
Epoch 118 time: 354.4784445762634 seconds
Epoch 118 accuracy: 14.08%
Batch 25, Loss: 1.7132
Batch 50, Loss: 1.7132
Batch 75, Loss: 1.7133
Batch 100, Loss: 1.7133
Batch 125, Loss: 1.7134
Batch 150, Loss: 1.7134
Batch 175, Loss: 1.7135
Noise applied in 22408 out of 22848 batches, 98.07
Epoch 119 learning rate: 0.01
Epoch 119 time: 353.03892707824707 seconds
Epoch 119 accuracy: 14.07%
Batch 25, Loss: 1.7135
Batch 50, Loss: 1.7136
Batch 75, Loss: 1.7136
Batch 100, Loss: 1.7137
Batch 125, Loss: 1.7137
Batch 150, Loss: 1.7138
Batch 175, Loss: 1.7138
Noise applied in 22600 out of 23040 batches, 98.09
Epoch 120 learning rate: 0.01
Epoch 120 time: 352.88308024406433 seconds
Epoch 120 accuracy: 14.03%
Batch 25, Loss: 1.7139
Batch 50, Loss: 1.7139
Batch 75, Loss: 1.7140
Batch 100, Loss: 1.7140
Batch 125, Loss: 1.7141
Batch 150, Loss: 1.7141
Batch 175, Loss: 1.7142
Noise applied in 22792 out of 23232 batches, 98.11
Epoch 121 learning rate: 0.01
Epoch 121 time: 353.960471868515 seconds
Epoch 121 accuracy: 14.02%
Batch 25, Loss: 1.7142
Batch 50, Loss: 1.7143
Batch 75, Loss: 1.7143
Batch 100, Loss: 1.7144
Batch 125, Loss: 1.7144
Batch 150, Loss: 1.7145
Batch 175, Loss: 1.7145
Noise applied in 22984 out of 23424 batches, 98.12
Epoch 122 learning rate: 0.01
Epoch 122 time: 354.57182097435 seconds
Epoch 122 accuracy: 13.98%
Batch 25, Loss: 1.7146
Batch 50, Loss: 1.7146
Batch 75, Loss: 1.7147
Batch 100, Loss: 1.7147
Batch 125, Loss: 1.7148
Batch 150, Loss: 1.7148
Batch 175, Loss: 1.7148
Noise applied in 23176 out of 23616 batches, 98.14
Epoch 123 learning rate: 0.01
Epoch 123 time: 354.4571373462677 seconds
Epoch 123 accuracy: 13.97%
Batch 25, Loss: 1.7149
Batch 50, Loss: 1.7150
Batch 75, Loss: 1.7150
Batch 100, Loss: 1.7151
Batch 125, Loss: 1.7151
Batch 150, Loss: 1.7152
Batch 175, Loss: 1.7152
Noise applied in 23368 out of 23808 batches, 98.15
Epoch 124 learning rate: 0.01
Epoch 124 time: 353.75750732421875 seconds
Epoch 124 accuracy: 13.97%
Batch 25, Loss: 1.7153
Batch 50, Loss: 1.7153
Batch 75, Loss: 1.7154
Batch 100, Loss: 1.7154
Batch 125, Loss: 1.7155
Batch 150, Loss: 1.7155
Batch 175, Loss: 1.7156
Noise applied in 23560 out of 24000 batches, 98.17
Epoch 125 learning rate: 0.01
Epoch 125 time: 353.398234128952 seconds
Epoch 125 accuracy: 13.86%
Batch 25, Loss: 1.7156
Batch 50, Loss: 1.7157
Batch 75, Loss: 1.7157
Batch 100, Loss: 1.7158
Batch 125, Loss: 1.7158
Batch 150, Loss: 1.7159
Batch 175, Loss: 1.7159
Noise applied in 23752 out of 24192 batches, 98.18
Epoch 126 learning rate: 0.01
Epoch 126 time: 354.05693459510803 seconds
Epoch 126 accuracy: 13.83%
Batch 25, Loss: 1.7160
Batch 50, Loss: 1.7160
Batch 75, Loss: 1.7161
Batch 100, Loss: 1.7161
Batch 125, Loss: 1.7162
Batch 150, Loss: 1.7162
Batch 175, Loss: 1.7162
Noise applied in 23944 out of 24384 batches, 98.20
Epoch 127 learning rate: 0.01
Epoch 127 time: 354.792138338089 seconds
Epoch 127 accuracy: 13.79%
Batch 25, Loss: 1.7163
Batch 50, Loss: 1.7164
Batch 75, Loss: 1.7164
Batch 100, Loss: 1.7165
Batch 125, Loss: 1.7165
Batch 150, Loss: 1.7166
Batch 175, Loss: 1.7166
Noise applied in 24136 out of 24576 batches, 98.21
Epoch 128 learning rate: 0.01
Epoch 128 time: 353.5328645706177 seconds
Epoch 128 accuracy: 13.77%
Batch 25, Loss: 1.7167
Batch 50, Loss: 1.7167
Batch 75, Loss: 1.7168
Batch 100, Loss: 1.7168
Batch 125, Loss: 1.7169
Batch 150, Loss: 1.7169
Batch 175, Loss: 1.7170
Noise applied in 24328 out of 24768 batches, 98.22
Epoch 129 learning rate: 0.01
Epoch 129 time: 353.8986876010895 seconds
Epoch 129 accuracy: 13.75%
Batch 25, Loss: 1.7170
Batch 50, Loss: 1.7171
Batch 75, Loss: 1.7171
Batch 100, Loss: 1.7172
Batch 125, Loss: 1.7172
Batch 150, Loss: 1.7173
Batch 175, Loss: 1.7173
Noise applied in 24520 out of 24960 batches, 98.24
Epoch 130 learning rate: 0.01
Epoch 130 time: 354.2136113643646 seconds
Epoch 130 accuracy: 13.72%
Batch 25, Loss: 1.7174
Batch 50, Loss: 1.7174
Batch 75, Loss: 1.7175
Batch 100, Loss: 1.7175
Batch 125, Loss: 1.7176
Batch 150, Loss: 1.7176
Batch 175, Loss: 1.7176
Noise applied in 24712 out of 25152 batches, 98.25
Epoch 131 learning rate: 0.01
Epoch 131 time: 364.1304979324341 seconds
Epoch 131 accuracy: 13.71%
Batch 25, Loss: 1.7177
Batch 50, Loss: 1.7178
Batch 75, Loss: 1.7178
Batch 100, Loss: 1.7179
Batch 125, Loss: 1.7179
Batch 150, Loss: 1.7180
Batch 175, Loss: 1.7180
Noise applied in 24904 out of 25344 batches, 98.26
Epoch 132 learning rate: 0.01
Epoch 132 time: 392.43032693862915 seconds
Epoch 132 accuracy: 13.59%
Batch 25, Loss: 1.7181
Batch 50, Loss: 1.7181
Batch 75, Loss: 1.7182
Batch 100, Loss: 1.7182
Batch 125, Loss: 1.7183
Batch 150, Loss: 1.7183
Batch 175, Loss: 1.7184
Noise applied in 25096 out of 25536 batches, 98.28
Epoch 133 learning rate: 0.01
Epoch 133 time: 401.66936016082764 seconds
Epoch 133 accuracy: 13.63%
Batch 25, Loss: 1.7184
Batch 50, Loss: 1.7185
Batch 75, Loss: 1.7185
Batch 100, Loss: 1.7186
Batch 125, Loss: 1.7186
Batch 150, Loss: 1.7187
Batch 175, Loss: 1.7187
Noise applied in 25288 out of 25728 batches, 98.29
Epoch 134 learning rate: 0.01
Epoch 134 time: 397.5756537914276 seconds
Epoch 134 accuracy: 13.63%
Batch 25, Loss: 1.7188
Batch 50, Loss: 1.7188
Batch 75, Loss: 1.7189
Batch 100, Loss: 1.7189
Batch 125, Loss: 1.7190
Batch 150, Loss: 1.7190
Batch 175, Loss: 1.7190
Noise applied in 25480 out of 25920 batches, 98.30
Epoch 135 learning rate: 0.01
Epoch 135 time: 409.23010754585266 seconds
Epoch 135 accuracy: 13.6%
Batch 25, Loss: 1.7191
Batch 50, Loss: 1.7192
Batch 75, Loss: 1.7192
Batch 100, Loss: 1.7192
Batch 125, Loss: 1.7193
Batch 150, Loss: 1.7193
Batch 175, Loss: 1.7194
Noise applied in 25672 out of 26112 batches, 98.31
Epoch 136 learning rate: 0.01
Epoch 136 time: 373.1890456676483 seconds
Epoch 136 accuracy: 13.5%
Batch 25, Loss: 1.7195
Batch 50, Loss: 1.7195
Batch 75, Loss: 1.7195
Batch 100, Loss: 1.7196
Batch 125, Loss: 1.7196
Batch 150, Loss: 1.7197
Batch 175, Loss: 1.7197
Noise applied in 25864 out of 26304 batches, 98.33
Epoch 137 learning rate: 0.01
Epoch 137 time: 382.4883234500885 seconds
Epoch 137 accuracy: 13.47%
Batch 25, Loss: 1.7198
Batch 50, Loss: 1.7198
Batch 75, Loss: 1.7199
Batch 100, Loss: 1.7199
Batch 125, Loss: 1.7200
Batch 150, Loss: 1.7200
Batch 175, Loss: 1.7201
Noise applied in 26056 out of 26496 batches, 98.34
Epoch 138 learning rate: 0.01
Epoch 138 time: 372.6208939552307 seconds
Epoch 138 accuracy: 13.32%
Batch 25, Loss: 1.7201
Batch 50, Loss: 1.7202
Batch 75, Loss: 1.7202
Batch 100, Loss: 1.7203
Batch 125, Loss: 1.7203
Batch 150, Loss: 1.7204
Batch 175, Loss: 1.7204
Noise applied in 26248 out of 26688 batches, 98.35
Epoch 139 learning rate: 0.01
Epoch 139 time: 435.4755733013153 seconds
Epoch 139 accuracy: 13.28%
Batch 25, Loss: 1.7205
Batch 50, Loss: 1.7205
Batch 75, Loss: 1.7206
Batch 100, Loss: 1.7206
Batch 125, Loss: 1.7206
Batch 150, Loss: 1.7207
Batch 175, Loss: 1.7207
Noise applied in 26440 out of 26880 batches, 98.36
Epoch 140 learning rate: 0.01
Epoch 140 time: 428.6082212924957 seconds
Epoch 140 accuracy: 13.18%
Batch 25, Loss: 1.7208
Batch 50, Loss: 1.7209
Batch 75, Loss: 1.7209
Batch 100, Loss: 1.7209
Batch 125, Loss: 1.7210
Batch 150, Loss: 1.7210
Batch 175, Loss: 1.7211
Noise applied in 26632 out of 27072 batches, 98.37
Epoch 141 learning rate: 0.01
Epoch 141 time: 389.9377202987671 seconds
Epoch 141 accuracy: 13.16%
Batch 25, Loss: 1.7211
Batch 50, Loss: 1.7212
Batch 75, Loss: 1.7212
Batch 100, Loss: 1.7213
Batch 125, Loss: 1.7213
Batch 150, Loss: 1.7214
Batch 175, Loss: 1.7214
Noise applied in 26824 out of 27264 batches, 98.39
Epoch 142 learning rate: 0.01
Epoch 142 time: 382.34963870048523 seconds
Epoch 142 accuracy: 13.12%
Batch 25, Loss: 1.7215
Batch 50, Loss: 1.7215
Batch 75, Loss: 1.7215
Batch 100, Loss: 1.7216
Batch 125, Loss: 1.7216
Batch 150, Loss: 1.7217
Batch 175, Loss: 1.7217
Noise applied in 27016 out of 27456 batches, 98.40
Epoch 143 learning rate: 0.01
Epoch 143 time: 384.73096776008606 seconds
Epoch 143 accuracy: 13.01%
Batch 25, Loss: 1.7218
Batch 50, Loss: 1.7218
Batch 75, Loss: 1.7219
Batch 100, Loss: 1.7219
Batch 125, Loss: 1.7219
Batch 150, Loss: 1.7220
Batch 175, Loss: 1.7220
Noise applied in 27208 out of 27648 batches, 98.41
Epoch 144 learning rate: 0.01
Epoch 144 time: 386.8379747867584 seconds
Epoch 144 accuracy: 12.96%
Batch 25, Loss: 1.7221
Batch 50, Loss: 1.7221
Batch 75, Loss: 1.7222
Batch 100, Loss: 1.7222
Batch 125, Loss: 1.7223
Batch 150, Loss: 1.7223
Batch 175, Loss: 1.7224
Noise applied in 27400 out of 27840 batches, 98.42
Epoch 145 learning rate: 0.01
Epoch 145 time: 448.7048671245575 seconds
Epoch 145 accuracy: 12.9%
Batch 25, Loss: 1.7224
Batch 50, Loss: 1.7225
Batch 75, Loss: 1.7225
Batch 100, Loss: 1.7225
Batch 125, Loss: 1.7226
Batch 150, Loss: 1.7226
Batch 175, Loss: 1.7227
Noise applied in 27592 out of 28032 batches, 98.43
Epoch 146 learning rate: 0.01
Epoch 146 time: 434.6428017616272 seconds
Epoch 146 accuracy: 12.85%
Batch 25, Loss: 1.7227
Batch 50, Loss: 1.7228
Batch 75, Loss: 1.7228
Batch 100, Loss: 1.7229
Batch 125, Loss: 1.7229
Batch 150, Loss: 1.7229
Batch 175, Loss: 1.7230
Noise applied in 27784 out of 28224 batches, 98.44
Epoch 147 learning rate: 0.01
Epoch 147 time: 428.1695234775543 seconds
Epoch 147 accuracy: 12.76%
Batch 25, Loss: 1.7230
Batch 50, Loss: 1.7231
Batch 75, Loss: 1.7231
Batch 100, Loss: 1.7232
Batch 125, Loss: 1.7232
Batch 150, Loss: 1.7232
Batch 175, Loss: 1.7233
Noise applied in 27976 out of 28416 batches, 98.45
Epoch 148 learning rate: 0.01
Epoch 148 time: 406.48858666419983 seconds
Epoch 148 accuracy: 12.7%
Batch 25, Loss: 1.7234
Batch 50, Loss: 1.7234
Batch 75, Loss: 1.7234
Batch 100, Loss: 1.7235
Batch 125, Loss: 1.7235
Batch 150, Loss: 1.7236
Batch 175, Loss: 1.7236
Noise applied in 28168 out of 28608 batches, 98.46
Epoch 149 learning rate: 0.01
Epoch 149 time: 449.976598739624 seconds
Epoch 149 accuracy: 12.6%
Batch 25, Loss: 1.7237
Batch 50, Loss: 1.7237
Batch 75, Loss: 1.7237
Batch 100, Loss: 1.7238
Batch 125, Loss: 1.7238
Batch 150, Loss: 1.7239
Batch 175, Loss: 1.7239
Noise applied in 28360 out of 28800 batches, 98.47
Epoch 150 learning rate: 0.01
Epoch 150 time: 385.9144489765167 seconds
Epoch 150 accuracy: 12.43%
Batch 25, Loss: 1.7240
Batch 50, Loss: 1.7240
Batch 75, Loss: 1.7240
Batch 100, Loss: 1.7241
Batch 125, Loss: 1.7241
Batch 150, Loss: 1.7242
Batch 175, Loss: 1.7242
Noise applied in 28552 out of 28992 batches, 98.48
Epoch 151 learning rate: 0.01
Epoch 151 time: 384.71449041366577 seconds
Epoch 151 accuracy: 12.41%
Batch 25, Loss: 1.7243
Batch 50, Loss: 1.7243
Batch 75, Loss: 1.7243
Batch 100, Loss: 1.7244
Batch 125, Loss: 1.7244
Batch 150, Loss: 1.7244
Batch 175, Loss: 1.7245
Noise applied in 28744 out of 29184 batches, 98.49
Epoch 152 learning rate: 0.01
Epoch 152 time: 433.6099410057068 seconds
Epoch 152 accuracy: 12.33%
Batch 25, Loss: 1.7245
Batch 50, Loss: 1.7246
Batch 75, Loss: 1.7246
Batch 100, Loss: 1.7246
Batch 125, Loss: 1.7247
Batch 150, Loss: 1.7247
Batch 175, Loss: 1.7248
Noise applied in 28936 out of 29376 batches, 98.50
Epoch 153 learning rate: 0.01
Epoch 153 time: 446.56256675720215 seconds
Epoch 153 accuracy: 12.3%
Batch 25, Loss: 1.7248
Batch 50, Loss: 1.7249
Batch 75, Loss: 1.7249
Batch 100, Loss: 1.7249
Batch 125, Loss: 1.7250
Batch 150, Loss: 1.7250
Batch 175, Loss: 1.7250
Noise applied in 29128 out of 29568 batches, 98.51
Epoch 154 learning rate: 0.01
Epoch 154 time: 401.042028427124 seconds
Epoch 154 accuracy: 12.17%
Batch 25, Loss: 1.7251
Batch 50, Loss: 1.7251
Batch 75, Loss: 1.7252
Batch 100, Loss: 1.7252
Batch 125, Loss: 1.7252
Batch 150, Loss: 1.7253
Batch 175, Loss: 1.7253
Noise applied in 29320 out of 29760 batches, 98.52
Epoch 155 learning rate: 0.01
Epoch 155 time: 406.19248628616333 seconds
Epoch 155 accuracy: 11.99%
Batch 25, Loss: 1.7254
Batch 50, Loss: 1.7254
Batch 75, Loss: 1.7254
Batch 100, Loss: 1.7255
Batch 125, Loss: 1.7255
Batch 150, Loss: 1.7255
Batch 175, Loss: 1.7256
Noise applied in 29512 out of 29952 batches, 98.53
Epoch 156 learning rate: 0.01
Epoch 156 time: 423.1436336040497 seconds
Epoch 156 accuracy: 11.87%
Batch 25, Loss: 1.7256
Batch 50, Loss: 1.7257
Batch 75, Loss: 1.7257
Batch 100, Loss: 1.7257
Batch 125, Loss: 1.7258
Batch 150, Loss: 1.7258
Batch 175, Loss: 1.7258
Noise applied in 29704 out of 30144 batches, 98.54
Epoch 157 learning rate: 0.01
Epoch 157 time: 444.9663896560669 seconds
Epoch 157 accuracy: 11.86%
Batch 25, Loss: 1.7259
Batch 50, Loss: 1.7259
Batch 75, Loss: 1.7260
Batch 100, Loss: 1.7260
Batch 125, Loss: 1.7260
Batch 150, Loss: 1.7261
Batch 175, Loss: 1.7261
Noise applied in 29896 out of 30336 batches, 98.55
Epoch 158 learning rate: 0.01
Epoch 158 time: 393.82746291160583 seconds
Epoch 158 accuracy: 11.8%
Batch 25, Loss: 1.7261
Batch 50, Loss: 1.7262
Batch 75, Loss: 1.7262
Batch 100, Loss: 1.7262
Batch 125, Loss: 1.7263
Batch 150, Loss: 1.7263
Batch 175, Loss: 1.7263
Noise applied in 30088 out of 30528 batches, 98.56
Epoch 159 learning rate: 0.01
Epoch 159 time: 401.89892387390137 seconds
Epoch 159 accuracy: 11.71%
Batch 25, Loss: 1.7264
Batch 50, Loss: 1.7264
Batch 75, Loss: 1.7265
Batch 100, Loss: 1.7265
Batch 125, Loss: 1.7265
Batch 150, Loss: 1.7266
Batch 175, Loss: 1.7266
Noise applied in 30280 out of 30720 batches, 98.57
Epoch 160 learning rate: 0.01
Epoch 160 time: 465.5802855491638 seconds
Epoch 160 accuracy: 11.62%
Batch 25, Loss: 1.7267
Batch 50, Loss: 1.7267
Batch 75, Loss: 1.7267
Batch 100, Loss: 1.7267
Batch 125, Loss: 1.7268
Batch 150, Loss: 1.7268
Batch 175, Loss: 1.7268
Noise applied in 30472 out of 30912 batches, 98.58
Epoch 161 learning rate: 0.01
Epoch 161 time: 438.39071011543274 seconds
Epoch 161 accuracy: 11.6%
Batch 25, Loss: 1.7269
Batch 50, Loss: 1.7269
Batch 75, Loss: 1.7269
Batch 100, Loss: 1.7270
Batch 125, Loss: 1.7270
Batch 150, Loss: 1.7270
Batch 175, Loss: 1.7271
Noise applied in 30664 out of 31104 batches, 98.59
Epoch 162 learning rate: 0.01
Epoch 162 time: 461.1613163948059 seconds
Epoch 162 accuracy: 11.42%
Batch 25, Loss: 1.7271
Batch 50, Loss: 1.7271
Batch 75, Loss: 1.7272
Batch 100, Loss: 1.7272
Batch 125, Loss: 1.7272
Batch 150, Loss: 1.7273
Batch 175, Loss: 1.7273
Noise applied in 30856 out of 31296 batches, 98.59
Epoch 163 learning rate: 0.01
Epoch 163 time: 403.4026951789856 seconds
Epoch 163 accuracy: 11.36%
Batch 25, Loss: 1.7273
Batch 50, Loss: 1.7274
Batch 75, Loss: 1.7274
Batch 100, Loss: 1.7274
Batch 125, Loss: 1.7275
Batch 150, Loss: 1.7275
Batch 175, Loss: 1.7275
Noise applied in 31048 out of 31488 batches, 98.60
Epoch 164 learning rate: 0.01
Epoch 164 time: 432.89645743370056 seconds
Epoch 164 accuracy: 11.23%
Batch 25, Loss: 1.7276
Batch 50, Loss: 1.7276
Batch 75, Loss: 1.7276
Batch 100, Loss: 1.7277
Batch 125, Loss: 1.7277
Batch 150, Loss: 1.7277
Batch 175, Loss: 1.7277
Noise applied in 31240 out of 31680 batches, 98.61
Epoch 165 learning rate: 0.01
Epoch 165 time: 408.59608793258667 seconds
Epoch 165 accuracy: 11.09%
Batch 25, Loss: 1.7278
Batch 50, Loss: 1.7278
Batch 75, Loss: 1.7278
Batch 100, Loss: 1.7279
Batch 125, Loss: 1.7279
Batch 150, Loss: 1.7279
Batch 175, Loss: 1.7280
Noise applied in 31432 out of 31872 batches, 98.62
Epoch 166 learning rate: 0.01
Epoch 166 time: 502.7415316104889 seconds
Epoch 166 accuracy: 11.01%
Batch 25, Loss: 1.7280
Batch 50, Loss: 1.7280
Batch 75, Loss: 1.7281
Batch 100, Loss: 1.7281
Batch 125, Loss: 1.7281
Batch 150, Loss: 1.7281
Batch 175, Loss: 1.7282
Noise applied in 31624 out of 32064 batches, 98.63
Epoch 167 learning rate: 0.01
Epoch 167 time: 458.0901257991791 seconds
Epoch 167 accuracy: 10.96%
Batch 25, Loss: 1.7282
Batch 50, Loss: 1.7282
Batch 75, Loss: 1.7283
Batch 100, Loss: 1.7283
Batch 125, Loss: 1.7283
Batch 150, Loss: 1.7284
Batch 175, Loss: 1.7284
Noise applied in 31816 out of 32256 batches, 98.64
Epoch 168 learning rate: 0.01
Epoch 168 time: 653.1773722171783 seconds
Epoch 168 accuracy: 10.9%
Batch 25, Loss: 1.7284
Batch 50, Loss: 1.7285
Batch 75, Loss: 1.7285
Batch 100, Loss: 1.7285
Batch 125, Loss: 1.7285
Batch 150, Loss: 1.7286
Batch 175, Loss: 1.7286
Noise applied in 32008 out of 32448 batches, 98.64
Epoch 169 learning rate: 0.01
Epoch 169 time: 386.04667568206787 seconds
Epoch 169 accuracy: 10.83%
Batch 25, Loss: 1.7286
Batch 50, Loss: 1.7287
Batch 75, Loss: 1.7287
Batch 100, Loss: 1.7287
Batch 125, Loss: 1.7287
Batch 150, Loss: 1.7288
Batch 175, Loss: 1.7288
Noise applied in 32200 out of 32640 batches, 98.65
Epoch 170 learning rate: 0.01
Epoch 170 time: 490.8326060771942 seconds
Epoch 170 accuracy: 10.82%
Batch 25, Loss: 1.7288
Batch 50, Loss: 1.7289
Batch 75, Loss: 1.7289
Batch 100, Loss: 1.7289
Batch 125, Loss: 1.7289
Batch 150, Loss: 1.7290
Batch 175, Loss: 1.7290
Noise applied in 32392 out of 32832 batches, 98.66
Epoch 171 learning rate: 0.01
Epoch 171 time: 444.48875164985657 seconds
Epoch 171 accuracy: 10.8%
Batch 25, Loss: 1.7290
Batch 50, Loss: 1.7290
Batch 75, Loss: 1.7291
Batch 100, Loss: 1.7291
Batch 125, Loss: 1.7291
Batch 150, Loss: 1.7291
Batch 175, Loss: 1.7292
Noise applied in 32584 out of 33024 batches, 98.67
Epoch 172 learning rate: 0.01
Epoch 172 time: 440.3543105125427 seconds
Epoch 172 accuracy: 10.77%
Batch 25, Loss: 1.7292
Batch 50, Loss: 1.7292
Batch 75, Loss: 1.7293
Batch 100, Loss: 1.7293
Batch 125, Loss: 1.7293
Batch 150, Loss: 1.7293
Batch 175, Loss: 1.7294
Noise applied in 32776 out of 33216 batches, 98.68
Epoch 173 learning rate: 0.01
Epoch 173 time: 429.2100901603699 seconds
Epoch 173 accuracy: 10.74%
Batch 25, Loss: 1.7294
Batch 50, Loss: 1.7294
Batch 75, Loss: 1.7295
Batch 100, Loss: 1.7295
Batch 125, Loss: 1.7295
Batch 150, Loss: 1.7295
Batch 175, Loss: 1.7295
Noise applied in 32968 out of 33408 batches, 98.68
Epoch 174 learning rate: 0.01
Epoch 174 time: 552.6528694629669 seconds
Epoch 174 accuracy: 10.72%
Batch 25, Loss: 1.7296
Batch 50, Loss: 1.7296
Batch 75, Loss: 1.7296
Batch 100, Loss: 1.7297
Batch 125, Loss: 1.7297
Batch 150, Loss: 1.7297
Batch 175, Loss: 1.7297
Noise applied in 33160 out of 33600 batches, 98.69
Epoch 175 learning rate: 0.01
Epoch 175 time: 447.6132056713104 seconds
Epoch 175 accuracy: 10.71%
Batch 25, Loss: 1.7298
Batch 50, Loss: 1.7298
Batch 75, Loss: 1.7298
Batch 100, Loss: 1.7298
Batch 125, Loss: 1.7298
Batch 150, Loss: 1.7299
Batch 175, Loss: 1.7299
Noise applied in 33352 out of 33792 batches, 98.70
Epoch 176 learning rate: 0.01
Epoch 176 time: 441.4576733112335 seconds
Epoch 176 accuracy: 10.66%
Batch 25, Loss: 1.7299
Batch 50, Loss: 1.7300
Batch 75, Loss: 1.7300
Batch 100, Loss: 1.7300
Batch 125, Loss: 1.7300
Batch 150, Loss: 1.7300
Batch 175, Loss: 1.7301
Noise applied in 33544 out of 33984 batches, 98.71
Epoch 177 learning rate: 0.01
Epoch 177 time: 433.2211103439331 seconds
Epoch 177 accuracy: 10.63%
Batch 25, Loss: 1.7301
Batch 50, Loss: 1.7301
Batch 75, Loss: 1.7301
Batch 100, Loss: 1.7302
Batch 125, Loss: 1.7302
Batch 150, Loss: 1.7302
Batch 175, Loss: 1.7302
Noise applied in 33736 out of 34176 batches, 98.71
Epoch 178 learning rate: 0.01
Epoch 178 time: 453.2935366630554 seconds
Epoch 178 accuracy: 10.6%
Batch 25, Loss: 1.7303
Batch 50, Loss: 1.7303
Batch 75, Loss: 1.7303
Batch 100, Loss: 1.7303
Batch 125, Loss: 1.7303
Batch 150, Loss: 1.7304
Batch 175, Loss: 1.7304
Noise applied in 33928 out of 34368 batches, 98.72
Epoch 179 learning rate: 0.01
Epoch 179 time: 392.46983075141907 seconds
Epoch 179 accuracy: 10.57%
Batch 25, Loss: 1.7304
Batch 50, Loss: 1.7304
Batch 75, Loss: 1.7305
Batch 100, Loss: 1.7305
Batch 125, Loss: 1.7305
Batch 150, Loss: 1.7305
Batch 175, Loss: 1.7305
Noise applied in 34120 out of 34560 batches, 98.73
Epoch 180 learning rate: 0.01
Epoch 180 time: 411.0469422340393 seconds
Epoch 180 accuracy: 10.53%
Batch 25, Loss: 1.7306
Batch 50, Loss: 1.7306
Batch 75, Loss: 1.7306
Batch 100, Loss: 1.7306
Batch 125, Loss: 1.7306
Batch 150, Loss: 1.7307
Batch 175, Loss: 1.7307
Noise applied in 34312 out of 34752 batches, 98.73
Epoch 181 learning rate: 0.01
Epoch 181 time: 448.74730110168457 seconds
Epoch 181 accuracy: 10.53%
Batch 25, Loss: 1.7307
Batch 50, Loss: 1.7307
Batch 75, Loss: 1.7308
Batch 100, Loss: 1.7308
Batch 125, Loss: 1.7308
Batch 150, Loss: 1.7308
Batch 175, Loss: 1.7308
Noise applied in 34504 out of 34944 batches, 98.74
Epoch 182 learning rate: 0.01
Epoch 182 time: 546.2430152893066 seconds
Epoch 182 accuracy: 10.5%
Batch 25, Loss: 1.7309
Batch 50, Loss: 1.7309
Batch 75, Loss: 1.7309
Batch 100, Loss: 1.7309
Batch 125, Loss: 1.7309
Batch 150, Loss: 1.7309
Batch 175, Loss: 1.7310
Noise applied in 34696 out of 35136 batches, 98.75
Epoch 183 learning rate: 0.01
Epoch 183 time: 601.1638278961182 seconds
Epoch 183 accuracy: 10.46%
Batch 25, Loss: 1.7310
Batch 50, Loss: 1.7310
Batch 75, Loss: 1.7310
Batch 100, Loss: 1.7311
Batch 125, Loss: 1.7311
Batch 150, Loss: 1.7311
Batch 175, Loss: 1.7311
Noise applied in 34888 out of 35328 batches, 98.75
Epoch 184 learning rate: 0.01
Epoch 184 time: 545.9252412319183 seconds
Epoch 184 accuracy: 10.38%
Batch 25, Loss: 1.7311
Batch 50, Loss: 1.7312
Batch 75, Loss: 1.7312
Batch 100, Loss: 1.7312
Batch 125, Loss: 1.7312
Batch 150, Loss: 1.7312
Batch 175, Loss: 1.7312
Noise applied in 35080 out of 35520 batches, 98.76
Epoch 185 learning rate: 0.01
Epoch 185 time: 472.0635542869568 seconds
Epoch 185 accuracy: 10.33%
Batch 25, Loss: 1.7313
Batch 50, Loss: 1.7313
Batch 75, Loss: 1.7313
Batch 100, Loss: 1.7313
Batch 125, Loss: 1.7313
Batch 150, Loss: 1.7314
Batch 175, Loss: 1.7314
Noise applied in 35272 out of 35712 batches, 98.77
Epoch 186 learning rate: 0.01
Epoch 186 time: 431.8709728717804 seconds
Epoch 186 accuracy: 10.35%
Batch 25, Loss: 1.7314
Batch 50, Loss: 1.7314
Batch 75, Loss: 1.7314
Batch 100, Loss: 1.7314
Batch 125, Loss: 1.7315
Batch 150, Loss: 1.7315
Batch 175, Loss: 1.7315
Noise applied in 35464 out of 35904 batches, 98.77
Epoch 187 learning rate: 0.01
Epoch 187 time: 380.31978034973145 seconds
Epoch 187 accuracy: 10.35%
Batch 25, Loss: 1.7315
Batch 50, Loss: 1.7315
Batch 75, Loss: 1.7316
Batch 100, Loss: 1.7316
Batch 125, Loss: 1.7316
Batch 150, Loss: 1.7316
Batch 175, Loss: 1.7316
Noise applied in 35656 out of 36096 batches, 98.78
Epoch 188 learning rate: 0.01
Epoch 188 time: 393.95288372039795 seconds
Epoch 188 accuracy: 10.34%
Batch 25, Loss: 1.7316
Batch 50, Loss: 1.7317
Batch 75, Loss: 1.7317
Batch 100, Loss: 1.7317
Batch 125, Loss: 1.7317
Batch 150, Loss: 1.7317
Batch 175, Loss: 1.7317
Noise applied in 35848 out of 36288 batches, 98.79
Epoch 189 learning rate: 0.01
Epoch 189 time: 476.0453679561615 seconds
Epoch 189 accuracy: 10.31%
Batch 25, Loss: 1.7318
Batch 50, Loss: 1.7318
Batch 75, Loss: 1.7318
Batch 100, Loss: 1.7318
Batch 125, Loss: 1.7318
Batch 150, Loss: 1.7318
Batch 175, Loss: 1.7318
Noise applied in 36040 out of 36480 batches, 98.79
Epoch 190 learning rate: 0.01
Epoch 190 time: 396.6879711151123 seconds
Epoch 190 accuracy: 10.32%
Batch 25, Loss: 1.7319
Batch 50, Loss: 1.7319
Batch 75, Loss: 1.7319
Batch 100, Loss: 1.7319
Batch 125, Loss: 1.7319
Batch 150, Loss: 1.7319
Batch 175, Loss: 1.7320
Noise applied in 36232 out of 36672 batches, 98.80
Epoch 191 learning rate: 0.01
Epoch 191 time: 353.91408944129944 seconds
Epoch 191 accuracy: 10.32%
Batch 25, Loss: 1.7320
Batch 50, Loss: 1.7320
Batch 75, Loss: 1.7320
Batch 100, Loss: 1.7320
Batch 125, Loss: 1.7320
Batch 150, Loss: 1.7321
Batch 175, Loss: 1.7321
Noise applied in 36424 out of 36864 batches, 98.81
Epoch 192 learning rate: 0.01
Epoch 192 time: 355.16733741760254 seconds
Epoch 192 accuracy: 10.32%
Batch 25, Loss: 1.7321
Batch 50, Loss: 1.7321
Batch 75, Loss: 1.7321
Batch 100, Loss: 1.7321
Batch 125, Loss: 1.7321
Batch 150, Loss: 1.7322
Batch 175, Loss: 1.7322
Noise applied in 36616 out of 37056 batches, 98.81
Epoch 193 learning rate: 0.01
Epoch 193 time: 353.8302011489868 seconds
Epoch 193 accuracy: 10.31%
Batch 25, Loss: 1.7322
Batch 50, Loss: 1.7322
Batch 75, Loss: 1.7322
Batch 100, Loss: 1.7322
Batch 125, Loss: 1.7322
Batch 150, Loss: 1.7323
Batch 175, Loss: 1.7323
Noise applied in 36808 out of 37248 batches, 98.82
Epoch 194 learning rate: 0.01
Epoch 194 time: 355.4628658294678 seconds
Epoch 194 accuracy: 10.26%
Batch 25, Loss: 1.7323
Batch 50, Loss: 1.7323
Batch 75, Loss: 1.7323
Batch 100, Loss: 1.7323
Batch 125, Loss: 1.7323
Batch 150, Loss: 1.7324
Batch 175, Loss: 1.7324
Noise applied in 37000 out of 37440 batches, 98.82
Epoch 195 learning rate: 0.01
Epoch 195 time: 353.8993909358978 seconds
Epoch 195 accuracy: 10.19%
Batch 25, Loss: 1.7324
Batch 50, Loss: 1.7324
Batch 75, Loss: 1.7324
Batch 100, Loss: 1.7324
Batch 125, Loss: 1.7324
Batch 150, Loss: 1.7324
Batch 175, Loss: 1.7325
Noise applied in 37192 out of 37632 batches, 98.83
Epoch 196 learning rate: 0.01
Epoch 196 time: 353.9377603530884 seconds
Epoch 196 accuracy: 10.13%
Batch 25, Loss: 1.7325
Batch 50, Loss: 1.7325
Batch 75, Loss: 1.7325
Batch 100, Loss: 1.7325
Batch 125, Loss: 1.7325
Batch 150, Loss: 1.7325
Batch 175, Loss: 1.7326
Noise applied in 37384 out of 37824 batches, 98.84
Epoch 197 learning rate: 0.01
Epoch 197 time: 354.3529222011566 seconds
Epoch 197 accuracy: 10.05%
Batch 25, Loss: 1.7326
Batch 50, Loss: 1.7326
Batch 75, Loss: 1.7326
Batch 100, Loss: 1.7326
Batch 125, Loss: 1.7326
Batch 150, Loss: 1.7326
Batch 175, Loss: 1.7326
Noise applied in 37576 out of 38016 batches, 98.84
Epoch 198 learning rate: 0.01
Epoch 198 time: 354.371750831604 seconds
Epoch 198 accuracy: 10.03%
Batch 25, Loss: 1.7327
Batch 50, Loss: 1.7327
Batch 75, Loss: 1.7327
Batch 100, Loss: 1.7327
Batch 125, Loss: 1.7327
Batch 150, Loss: 1.7327
Batch 175, Loss: 1.7327
Noise applied in 37768 out of 38208 batches, 98.85
Epoch 199 learning rate: 0.01
Epoch 199 time: 354.5565629005432 seconds
Epoch 199 accuracy: 9.99%
Batch 25, Loss: 1.7328
Batch 50, Loss: 1.7328
Batch 75, Loss: 1.7328
Batch 100, Loss: 1.7328
Batch 125, Loss: 1.7328
Batch 150, Loss: 1.7328
Batch 175, Loss: 1.7328
Noise applied in 37960 out of 38400 batches, 98.85
Epoch 200 learning rate: 0.01
Epoch 200 time: 353.37533044815063 seconds
Epoch 200 accuracy: 9.95%
rho:  0.04 , alpha:  0.3
Total training time: 78058.97713518143 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.2201
Norm of the Gradient: 9.7507074475e-02
Smallest Hessian Eigenvalue: -0.1129
Noise Threshold: 0.8
Noise Radius: 0.05
