The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-18:41:47
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 149.1084
Batch 50, Loss: 73.9832
Batch 75, Loss: 42.1973
Batch 100, Loss: 29.2149
Batch 125, Loss: 21.7479
Batch 150, Loss: 17.4354
Batch 175, Loss: 14.5271
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 285.31011295318604 seconds
Epoch 1 accuracy: 11.01%
Batch 25, Loss: 11.4737
Batch 50, Loss: 10.3690
Batch 75, Loss: 9.5799
Batch 100, Loss: 8.9874
Batch 125, Loss: 8.5150
Batch 150, Loss: 8.1192
Batch 175, Loss: 7.7799
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 273.1777436733246 seconds
Epoch 2 accuracy: 11.26%
Batch 25, Loss: 7.3010
Batch 50, Loss: 7.0550
Batch 75, Loss: 6.8294
Batch 100, Loss: 6.6195
Batch 125, Loss: 6.4193
Batch 150, Loss: 6.2239
Batch 175, Loss: 6.0312
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 275.4831702709198 seconds
Epoch 3 accuracy: 11.42%
Batch 25, Loss: 5.7084
Batch 50, Loss: 5.5136
Batch 75, Loss: 5.3172
Batch 100, Loss: 5.1228
Batch 125, Loss: 4.9381
Batch 150, Loss: 4.7746
Batch 175, Loss: 4.6417
Noise applied in 9 out of 192 batches, 4.69
Epoch 4 learning rate: 0.01
Epoch 4 time: 284.443163394928 seconds
Epoch 4 accuracy: 10.03%
Batch 25, Loss: 4.4768
Batch 50, Loss: 4.3976
Batch 75, Loss: 4.3260
Batch 100, Loss: 4.2601
Batch 125, Loss: 4.1988
Batch 150, Loss: 4.1417
Batch 175, Loss: 4.0886
Noise applied in 192 out of 192 batches, 100.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 382.471795797348 seconds
Epoch 5 accuracy: 9.58%
Batch 25, Loss: 4.0064
Batch 50, Loss: 3.9606
Batch 75, Loss: 3.9170
Batch 100, Loss: 3.8753
Batch 125, Loss: 3.8352
Batch 150, Loss: 3.7968
Batch 175, Loss: 3.7596
Noise applied in 192 out of 192 batches, 100.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 433.00673842430115 seconds
Epoch 6 accuracy: 9.32%
Batch 25, Loss: 3.6997
Batch 50, Loss: 3.6655
Batch 75, Loss: 3.6327
Batch 100, Loss: 3.6010
Batch 125, Loss: 3.5703
Batch 150, Loss: 3.5407
Batch 175, Loss: 3.5117
Noise applied in 192 out of 192 batches, 100.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 383.7494685649872 seconds
Epoch 7 accuracy: 8.98%
Batch 25, Loss: 3.4652
Batch 50, Loss: 3.4386
Batch 75, Loss: 3.4126
Batch 100, Loss: 3.3871
Batch 125, Loss: 3.3625
Batch 150, Loss: 3.3386
Batch 175, Loss: 3.3155
Noise applied in 192 out of 192 batches, 100.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 381.4883801937103 seconds
Epoch 8 accuracy: 8.86%
Batch 25, Loss: 3.2779
Batch 50, Loss: 3.2563
Batch 75, Loss: 3.2351
Batch 100, Loss: 3.2146
Batch 125, Loss: 3.1945
Batch 150, Loss: 3.1750
Batch 175, Loss: 3.1560
Noise applied in 192 out of 192 batches, 100.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 384.9054901599884 seconds
Epoch 9 accuracy: 8.83%
Batch 25, Loss: 3.1250
Batch 50, Loss: 3.1072
Batch 75, Loss: 3.0898
Batch 100, Loss: 3.0729
Batch 125, Loss: 3.0564
Batch 150, Loss: 3.0404
Batch 175, Loss: 3.0247
Noise applied in 258 out of 192 batches, 134.38
Epoch 10 learning rate: 0.01
Epoch 10 time: 423.0383162498474 seconds
Epoch 10 accuracy: 8.72%
Batch 25, Loss: 2.9991
Batch 50, Loss: 2.9843
Batch 75, Loss: 2.9697
Batch 100, Loss: 2.9555
Batch 125, Loss: 2.9415
Batch 150, Loss: 2.9276
Batch 175, Loss: 2.9141
Noise applied in 384 out of 192 batches, 200.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 474.48764085769653 seconds
Epoch 11 accuracy: 8.76%
Batch 25, Loss: 2.8920
Batch 50, Loss: 2.8791
Batch 75, Loss: 2.8665
Batch 100, Loss: 2.8541
Batch 125, Loss: 2.8420
Batch 150, Loss: 2.8302
Batch 175, Loss: 2.8185
Noise applied in 384 out of 192 batches, 200.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 477.2681972980499 seconds
Epoch 12 accuracy: 8.69%
Batch 25, Loss: 2.7996
Batch 50, Loss: 2.7886
Batch 75, Loss: 2.7777
Batch 100, Loss: 2.7671
Batch 125, Loss: 2.7567
Batch 150, Loss: 2.7464
Batch 175, Loss: 2.7362
Noise applied in 384 out of 192 batches, 200.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 483.38561248779297 seconds
Epoch 13 accuracy: 8.73%
Batch 25, Loss: 2.7193
Batch 50, Loss: 2.7095
Batch 75, Loss: 2.6999
Batch 100, Loss: 2.6906
Batch 125, Loss: 2.6813
Batch 150, Loss: 2.6722
Batch 175, Loss: 2.6632
Noise applied in 384 out of 192 batches, 200.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 532.2666232585907 seconds
Epoch 14 accuracy: 8.77%
Batch 25, Loss: 2.6486
Batch 50, Loss: 2.6401
Batch 75, Loss: 2.6316
Batch 100, Loss: 2.6234
Batch 125, Loss: 2.6152
Batch 150, Loss: 2.6071
Batch 175, Loss: 2.5993
Noise applied in 384 out of 192 batches, 200.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 489.5031690597534 seconds
Epoch 15 accuracy: 8.88%
Batch 25, Loss: 2.5865
Batch 50, Loss: 2.5792
Batch 75, Loss: 2.5719
Batch 100, Loss: 2.5646
Batch 125, Loss: 2.5576
Batch 150, Loss: 2.5507
Batch 175, Loss: 2.5438
Noise applied in 384 out of 192 batches, 200.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 529.0672585964203 seconds
Epoch 16 accuracy: 9.0%
Batch 25, Loss: 2.5324
Batch 50, Loss: 2.5257
Batch 75, Loss: 2.5192
Batch 100, Loss: 2.5127
Batch 125, Loss: 2.5065
Batch 150, Loss: 2.5002
Batch 175, Loss: 2.4939
Noise applied in 384 out of 192 batches, 200.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 485.82600116729736 seconds
Epoch 17 accuracy: 9.08%
Batch 25, Loss: 2.4836
Batch 50, Loss: 2.4776
Batch 75, Loss: 2.4716
Batch 100, Loss: 2.4657
Batch 125, Loss: 2.4598
Batch 150, Loss: 2.4540
Batch 175, Loss: 2.4484
Noise applied in 384 out of 192 batches, 200.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 491.08843517303467 seconds
Epoch 18 accuracy: 9.3%
Batch 25, Loss: 2.4389
Batch 50, Loss: 2.4334
Batch 75, Loss: 2.4280
Batch 100, Loss: 2.4226
Batch 125, Loss: 2.4173
Batch 150, Loss: 2.4121
Batch 175, Loss: 2.4069
Noise applied in 384 out of 192 batches, 200.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 477.42219519615173 seconds
Epoch 19 accuracy: 9.4%
Batch 25, Loss: 2.3985
Batch 50, Loss: 2.3936
Batch 75, Loss: 2.3887
Batch 100, Loss: 2.3839
Batch 125, Loss: 2.3790
Batch 150, Loss: 2.3742
Batch 175, Loss: 2.3694
Noise applied in 384 out of 192 batches, 200.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 493.8010222911835 seconds
Epoch 20 accuracy: 9.53%
rho:  0.04 , alpha:  0.3
Total training time: 8441.205951452255 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.9211
Norm of the Gradient: 3.0513438582e-01
Smallest Hessian Eigenvalue: -0.1094
Noise Threshold: 0.8
Noise Radius: 0.1
