The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.05/batchsize-256/2024-08-18-16:59:12
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 23.2805
Noise applied in 0 out of 196 batches, 0.00
Epoch 1 learning rate: 0.05
Epoch 1 time: 123.93508410453796 seconds
Epoch 1 accuracy: 11.46%
Batch 100, Loss: 5.6026
Noise applied in 0 out of 392 batches, 0.00
Epoch 2 learning rate: 0.05
Epoch 2 time: 111.62960529327393 seconds
Epoch 2 accuracy: 10.88%
Batch 100, Loss: 4.1083
Noise applied in 0 out of 588 batches, 0.00
Epoch 3 learning rate: 0.05
Epoch 3 time: 111.63351392745972 seconds
Epoch 3 accuracy: 10.93%
Batch 100, Loss: 3.2138
Noise applied in 62 out of 784 batches, 7.91
Epoch 4 learning rate: 0.05
Epoch 4 time: 148.64807534217834 seconds
Epoch 4 accuracy: 10.56%
Batch 100, Loss: 2.5645
Noise applied in 257 out of 980 batches, 26.22
Epoch 5 learning rate: 0.05
Epoch 5 time: 220.8429160118103 seconds
Epoch 5 accuracy: 10.67%
Batch 100, Loss: 2.2129
Noise applied in 453 out of 1176 batches, 38.52
Epoch 6 learning rate: 0.05
Epoch 6 time: 221.27628660202026 seconds
Epoch 6 accuracy: 11.09%
Batch 100, Loss: 2.0993
Noise applied in 649 out of 1372 batches, 47.30
Epoch 7 learning rate: 0.05
Epoch 7 time: 221.4308841228485 seconds
Epoch 7 accuracy: 13.35%
Batch 100, Loss: 2.0277
Noise applied in 845 out of 1568 batches, 53.89
Epoch 8 learning rate: 0.05
Epoch 8 time: 221.33177042007446 seconds
Epoch 8 accuracy: 13.32%
Batch 100, Loss: 1.9611
Noise applied in 1041 out of 1764 batches, 59.01
Epoch 9 learning rate: 0.05
Epoch 9 time: 221.90000700950623 seconds
Epoch 9 accuracy: 16.66%
Batch 100, Loss: 1.9054
Noise applied in 1237 out of 1960 batches, 63.11
Epoch 10 learning rate: 0.05
Epoch 10 time: 221.3631670475006 seconds
Epoch 10 accuracy: 16.8%
Batch 100, Loss: 1.8491
Noise applied in 1433 out of 2156 batches, 66.47
Epoch 11 learning rate: 0.05
Epoch 11 time: 221.24911975860596 seconds
Epoch 11 accuracy: 16.91%
Batch 100, Loss: 1.7918
Noise applied in 1629 out of 2352 batches, 69.26
Epoch 12 learning rate: 0.05
Epoch 12 time: 221.3446991443634 seconds
Epoch 12 accuracy: 17.31%
Batch 100, Loss: 1.7575
Noise applied in 1825 out of 2548 batches, 71.62
Epoch 13 learning rate: 0.05
Epoch 13 time: 222.08523774147034 seconds
Epoch 13 accuracy: 17.12%
Batch 100, Loss: 1.7421
Noise applied in 2021 out of 2744 batches, 73.65
Epoch 14 learning rate: 0.05
Epoch 14 time: 221.891925573349 seconds
Epoch 14 accuracy: 14.07%
Batch 100, Loss: 1.7378
Noise applied in 2217 out of 2940 batches, 75.41
Epoch 15 learning rate: 0.05
Epoch 15 time: 221.7816460132599 seconds
Epoch 15 accuracy: 15.73%
Batch 100, Loss: 1.7367
Noise applied in 2413 out of 3136 batches, 76.95
Epoch 16 learning rate: 0.05
Epoch 16 time: 221.51707863807678 seconds
Epoch 16 accuracy: 15.9%
Batch 100, Loss: 1.7336
Noise applied in 2609 out of 3332 batches, 78.30
Epoch 17 learning rate: 0.05
Epoch 17 time: 221.13718485832214 seconds
Epoch 17 accuracy: 16.01%
Batch 100, Loss: 1.7335
Noise applied in 2805 out of 3528 batches, 79.51
Epoch 18 learning rate: 0.05
Epoch 18 time: 221.89523887634277 seconds
Epoch 18 accuracy: 15.33%
Batch 100, Loss: 1.7335
Noise applied in 3001 out of 3724 batches, 80.59
Epoch 19 learning rate: 0.05
Epoch 19 time: 221.39398169517517 seconds
Epoch 19 accuracy: 14.9%
Batch 100, Loss: 1.7349
Noise applied in 3197 out of 3920 batches, 81.56
Epoch 20 learning rate: 0.05
Epoch 20 time: 221.96971821784973 seconds
Epoch 20 accuracy: 14.49%
rho:  0.04 , alpha:  0.3
Total training time: 4040.272609949112 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.1369
Norm of the Gradient: 2.9503735900e-01
Smallest Hessian Eigenvalue: -0.3101
Noise Threshold: 0.5
Noise Radius: 0.1
