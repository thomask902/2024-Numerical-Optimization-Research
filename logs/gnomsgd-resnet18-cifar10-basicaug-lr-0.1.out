The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 431.2085
Batch 20, Loss: 783.5614
Batch 30, Loss: 756.6834
Batch 40, Loss: 951.9225
Batch 50, Loss: 631.2611
Batch 60, Loss: 1177.8652
Batch 70, Loss: 545.9284
Batch 80, Loss: 188.0966
Batch 90, Loss: 162.0498
Batch 100, Loss: 158.8118
Batch 110, Loss: 183.3313
Batch 120, Loss: 186.0234
Batch 130, Loss: 170.3504
Batch 140, Loss: 155.5338
Batch 150, Loss: 150.6691
Batch 160, Loss: 107.5072
Batch 170, Loss: 70.6036
Batch 180, Loss: 60.4417
Batch 190, Loss: 53.6771
Batch 200, Loss: 46.5139
Batch 210, Loss: 37.3475
Batch 220, Loss: 34.1053
Batch 230, Loss: 27.9112
Batch 240, Loss: 28.1490
Batch 250, Loss: 26.1868
Batch 260, Loss: 24.9700
Batch 270, Loss: 22.0828
Batch 280, Loss: 22.0197
Batch 290, Loss: 19.5906
Batch 300, Loss: 18.7309
Batch 310, Loss: 18.1594
Batch 320, Loss: 17.1462
Batch 330, Loss: 17.5116
Batch 340, Loss: 16.5440
Batch 350, Loss: 16.4715
Batch 360, Loss: 15.9805
Batch 370, Loss: 15.1660
Batch 380, Loss: 15.5128
Batch 390, Loss: 14.4632
Epoch 1 learning rate: 0.1
Epoch 1 time: 132.8909502029419 seconds
Epoch 1 accuracy: 8.02%
Batch 10, Loss: 14.1063
Batch 20, Loss: 13.4910
Batch 30, Loss: 14.0150
Batch 40, Loss: 12.9816
Batch 50, Loss: 13.1876
Batch 60, Loss: 13.5591
Batch 70, Loss: 13.6616
Batch 80, Loss: 13.0174
Batch 90, Loss: 13.1734
Batch 100, Loss: 12.2175
Batch 110, Loss: 11.6585
Batch 120, Loss: 11.2714
Batch 130, Loss: 12.9707
Batch 140, Loss: 11.8803
Batch 150, Loss: 12.1016
Batch 160, Loss: 11.9432
Batch 170, Loss: 10.9509
Batch 180, Loss: 11.2674
Batch 190, Loss: 10.9611
Batch 200, Loss: 10.0869
Batch 210, Loss: 10.1691
Batch 220, Loss: 9.5965
Batch 230, Loss: 10.0334
Batch 240, Loss: 9.5057
Batch 250, Loss: 9.7789
Batch 260, Loss: 9.3527
Batch 270, Loss: 9.1949
Batch 280, Loss: 8.6917
Batch 290, Loss: 8.8433
Batch 300, Loss: 9.0133
Batch 310, Loss: 8.4968
Batch 320, Loss: 7.7454
Batch 330, Loss: 7.9303
Batch 340, Loss: 7.8695
Batch 350, Loss: 8.2135
Batch 360, Loss: 7.8396
Batch 370, Loss: 7.5582
Batch 380, Loss: 7.9015
Batch 390, Loss: 7.2199
Epoch 2 learning rate: 0.1
Epoch 2 time: 119.20035409927368 seconds
Epoch 2 accuracy: 7.78%
Batch 10, Loss: 7.1088
Batch 20, Loss: 7.1933
Batch 30, Loss: 6.8247
Batch 40, Loss: 7.0846
Batch 50, Loss: 7.3850
Batch 60, Loss: 6.5600
Batch 70, Loss: 6.7372
Batch 80, Loss: 6.1914
Batch 90, Loss: 6.1859
Batch 100, Loss: 6.2861
Batch 110, Loss: 6.2014
Batch 120, Loss: 6.2089
Batch 130, Loss: 6.1876
Batch 140, Loss: 6.2117
Batch 150, Loss: 6.1819
Batch 160, Loss: 5.8670
Batch 170, Loss: 5.7477
Batch 180, Loss: 5.5396
Batch 190, Loss: 5.5022
Batch 200, Loss: 5.4489
Batch 210, Loss: 5.5202
Batch 220, Loss: 5.2954
Batch 230, Loss: 5.3761
Batch 240, Loss: 5.7695
Batch 250, Loss: 5.2642
Batch 260, Loss: 5.2209
Batch 270, Loss: 5.0059
Batch 280, Loss: 5.0561
Batch 290, Loss: 5.0644
Batch 300, Loss: 5.0080
Batch 310, Loss: 5.0409
Batch 320, Loss: 4.9855
Batch 330, Loss: 4.9643
Batch 340, Loss: 4.9670
Batch 350, Loss: 4.8007
Batch 360, Loss: 4.5989
Batch 370, Loss: 4.9171
Batch 380, Loss: 4.7063
Batch 390, Loss: 4.6229
Epoch 3 learning rate: 0.1
Epoch 3 time: 119.1576247215271 seconds
Epoch 3 accuracy: 8.09%
Batch 10, Loss: 4.5713
Batch 20, Loss: 4.6088
Batch 30, Loss: 4.6180
Batch 40, Loss: 4.5152
Batch 50, Loss: 4.1890
Batch 60, Loss: 4.6074
Batch 70, Loss: 4.3251
Batch 80, Loss: 4.2613
Batch 90, Loss: 4.3260
Batch 100, Loss: 4.3250
Batch 110, Loss: 4.0233
Batch 120, Loss: 4.3700
Batch 130, Loss: 4.1286
Batch 140, Loss: 4.1145
Batch 150, Loss: 3.9703
Batch 160, Loss: 3.9877
Batch 170, Loss: 3.9528
Batch 180, Loss: 3.8858
Batch 190, Loss: 3.6523
Batch 200, Loss: 3.6998
Batch 210, Loss: 3.8777
Batch 220, Loss: 3.7395
Batch 230, Loss: 3.6163
Batch 240, Loss: 3.6014
Batch 250, Loss: 3.6806
Batch 260, Loss: 3.7305
Batch 270, Loss: 3.4929
Batch 280, Loss: 3.5360
Batch 290, Loss: 3.4999
Batch 300, Loss: 3.3088
Batch 310, Loss: 3.5090
Batch 320, Loss: 3.2332
Batch 330, Loss: 3.3313
Batch 340, Loss: 3.3648
Batch 350, Loss: 3.3465
Batch 360, Loss: 3.1910
Batch 370, Loss: 3.0285
Batch 380, Loss: 3.2124
Batch 390, Loss: 3.0326
Epoch 4 learning rate: 0.1
Epoch 4 time: 119.01455402374268 seconds
Epoch 4 accuracy: 6.92%
Batch 10, Loss: 3.1396
Batch 20, Loss: 3.2967
Batch 30, Loss: 3.1341
Batch 40, Loss: 2.9086
Batch 50, Loss: 3.0280
Batch 60, Loss: 3.0693
Batch 70, Loss: 2.9655
Batch 80, Loss: 2.9042
Batch 90, Loss: 2.8889
Batch 100, Loss: 2.8177
Batch 110, Loss: 2.9170
Batch 120, Loss: 2.9084
Batch 130, Loss: 2.9843
Batch 140, Loss: 2.8766
Batch 150, Loss: 2.8696
Batch 160, Loss: 2.9149
Batch 170, Loss: 2.8088
Batch 180, Loss: 2.6792
Batch 190, Loss: 2.7161
Batch 200, Loss: 2.7392
Batch 210, Loss: 2.6447
Batch 220, Loss: 2.6868
Batch 230, Loss: 2.6845
Batch 240, Loss: 2.7295
Batch 250, Loss: 2.7164
Batch 260, Loss: 2.6483
Batch 270, Loss: 2.5814
Batch 280, Loss: 2.6397
Batch 290, Loss: 2.7497
Batch 300, Loss: 2.6113
Batch 310, Loss: 2.5542
Batch 320, Loss: 2.6280
Batch 330, Loss: 2.5662
Batch 340, Loss: 2.5534
Batch 350, Loss: 2.4620
Batch 360, Loss: 2.4590
Batch 370, Loss: 2.4704
Batch 380, Loss: 2.4906
Batch 390, Loss: 2.5101
Epoch 5 learning rate: 0.1
Epoch 5 time: 119.10555696487427 seconds
Epoch 5 accuracy: 10.63%
Batch 10, Loss: 2.4924
Batch 20, Loss: 2.4583
Batch 30, Loss: 2.4441
Batch 40, Loss: 2.4330
Batch 50, Loss: 2.3470
Batch 60, Loss: 2.4621
Batch 70, Loss: 2.5621
Batch 80, Loss: 2.3507
Batch 90, Loss: 2.3540
Batch 100, Loss: 2.4405
Batch 110, Loss: 2.3669
Batch 120, Loss: 2.3949
Batch 130, Loss: 2.3924
Batch 140, Loss: 2.3746
Batch 150, Loss: 2.3874
Batch 160, Loss: 2.4027
Batch 170, Loss: 2.3458
Batch 180, Loss: 2.2948
Batch 190, Loss: 2.3422
Batch 200, Loss: 2.3904
Batch 210, Loss: 2.2842
Batch 220, Loss: 2.2644
Batch 230, Loss: 2.3990
Batch 240, Loss: 2.3655
Batch 250, Loss: 2.2903
Batch 260, Loss: 2.2915
Batch 270, Loss: 2.3114
Batch 280, Loss: 2.3317
Batch 290, Loss: 2.2954
Batch 300, Loss: 2.3237
Batch 310, Loss: 2.3778
Batch 320, Loss: 2.2520
Batch 330, Loss: 2.2993
Batch 340, Loss: 2.3154
Batch 350, Loss: 2.2219
Batch 360, Loss: 2.2174
Batch 370, Loss: 2.2432
Batch 380, Loss: 2.2331
Batch 390, Loss: 2.2252
Epoch 6 learning rate: 0.1
Epoch 6 time: 119.19351410865784 seconds
Epoch 6 accuracy: 9.57%
Batch 10, Loss: 2.1434
Batch 20, Loss: 2.1854
Batch 30, Loss: 2.1659
Batch 40, Loss: 2.1405
Batch 50, Loss: 2.2391
Batch 60, Loss: 2.2548
Batch 70, Loss: 2.1648
Batch 80, Loss: 2.2354
Batch 90, Loss: 2.1810
Batch 100, Loss: 2.1710
Batch 110, Loss: 2.1953
Batch 120, Loss: 2.1863
Batch 130, Loss: 2.2559
Batch 140, Loss: 2.2521
Batch 150, Loss: 2.1479
Batch 160, Loss: 2.2004
Batch 170, Loss: 2.2153
Batch 180, Loss: 2.1708
Batch 190, Loss: 2.1198
Batch 200, Loss: 2.1244
Batch 210, Loss: 2.1220
Batch 220, Loss: 2.1822
Batch 230, Loss: 2.1347
Batch 240, Loss: 2.1562
Batch 250, Loss: 2.1320
Batch 260, Loss: 2.1064
Batch 270, Loss: 2.1508
Batch 280, Loss: 2.1271
Batch 290, Loss: 2.1477
Batch 300, Loss: 2.1360
Batch 310, Loss: 2.1340
Batch 320, Loss: 2.2058
Batch 330, Loss: 2.1154
Batch 340, Loss: 2.1078
Batch 350, Loss: 2.0792
Batch 360, Loss: 2.1236
Batch 370, Loss: 2.2292
Batch 380, Loss: 2.1585
Batch 390, Loss: 2.1047
Epoch 7 learning rate: 0.1
Epoch 7 time: 119.01093244552612 seconds
Epoch 7 accuracy: 9.45%
Batch 10, Loss: 2.1042
Batch 20, Loss: 2.0393
Batch 30, Loss: 2.0770
Batch 40, Loss: 2.0463
Batch 50, Loss: 2.0493
Batch 60, Loss: 2.0857
Batch 70, Loss: 2.0521
Batch 80, Loss: 2.1190
Batch 90, Loss: 2.1096
Batch 100, Loss: 2.0664
Batch 110, Loss: 2.1134
Batch 120, Loss: 2.0994
Batch 130, Loss: 2.0959
Batch 140, Loss: 2.0366
Batch 150, Loss: 2.0756
Batch 160, Loss: 2.0739
Batch 170, Loss: 2.0863
Batch 180, Loss: 2.1211
Batch 190, Loss: 2.0371
Batch 200, Loss: 2.0049
Batch 210, Loss: 2.0649
Batch 220, Loss: 2.0400
Batch 230, Loss: 2.0779
Batch 240, Loss: 2.0335
Batch 250, Loss: 2.0511
Batch 260, Loss: 2.0888
Batch 270, Loss: 2.0942
Batch 280, Loss: 2.0409
Batch 290, Loss: 2.0203
Batch 300, Loss: 2.0357
Batch 310, Loss: 2.0306
Batch 320, Loss: 2.0417
Batch 330, Loss: 2.0260
Batch 340, Loss: 2.0249
Batch 350, Loss: 2.0400
Batch 360, Loss: 2.0197
Batch 370, Loss: 2.0547
Batch 380, Loss: 2.0393
Batch 390, Loss: 2.0212
Epoch 8 learning rate: 0.1
Epoch 8 time: 119.118319272995 seconds
Epoch 8 accuracy: 9.29%
Batch 10, Loss: 1.9489
Batch 20, Loss: 2.0282
Batch 30, Loss: 1.9924
Batch 40, Loss: 2.0072
Batch 50, Loss: 2.0006
Batch 60, Loss: 1.9956
Batch 70, Loss: 1.9897
Batch 80, Loss: 2.0047
Batch 90, Loss: 1.9970
Batch 100, Loss: 1.9893
Batch 110, Loss: 1.9513
Batch 120, Loss: 1.9994
Batch 130, Loss: 1.9582
Batch 140, Loss: 1.9395
Batch 150, Loss: 1.9941
Batch 160, Loss: 1.9760
Batch 170, Loss: 2.0146
Batch 180, Loss: 1.9881
Batch 190, Loss: 1.9623
Batch 200, Loss: 1.9365
Batch 210, Loss: 1.9917
Batch 220, Loss: 1.9806
Batch 230, Loss: 1.9560
Batch 240, Loss: 1.9773
Batch 250, Loss: 1.9871
Batch 260, Loss: 2.0029
Batch 270, Loss: 1.9690
Batch 280, Loss: 1.9472
Batch 290, Loss: 1.9829
Batch 300, Loss: 1.9630
Batch 310, Loss: 1.9848
Batch 320, Loss: 1.9655
Batch 330, Loss: 1.9344
Batch 340, Loss: 1.9458
Batch 350, Loss: 1.9424
Batch 360, Loss: 1.9234
Batch 370, Loss: 1.9866
Batch 380, Loss: 1.9425
Batch 390, Loss: 1.9186
Epoch 9 learning rate: 0.1
Epoch 9 time: 119.00077557563782 seconds
Epoch 9 accuracy: 10.13%
Batch 10, Loss: 1.9060
Batch 20, Loss: 1.9493
Batch 30, Loss: 1.9214
Batch 40, Loss: 1.9578
Batch 50, Loss: 1.9448
Batch 60, Loss: 1.9081
Batch 70, Loss: 1.9087
Batch 80, Loss: 1.8968
Batch 90, Loss: 1.9301
Batch 100, Loss: 1.9002
Batch 110, Loss: 1.9237
Batch 120, Loss: 1.8978
Batch 130, Loss: 1.8916
Batch 140, Loss: 1.8953
Batch 150, Loss: 1.8716
Batch 160, Loss: 1.8935
Batch 170, Loss: 1.8990
Batch 180, Loss: 1.9164
Batch 190, Loss: 1.8936
Batch 200, Loss: 1.9181
Batch 210, Loss: 1.8809
Batch 220, Loss: 1.8706
Batch 230, Loss: 1.8921
Batch 240, Loss: 1.9232
Batch 250, Loss: 1.8965
Batch 260, Loss: 1.8894
Batch 270, Loss: 1.8816
Batch 280, Loss: 1.8748
Batch 290, Loss: 1.8668
Batch 300, Loss: 1.8606
Batch 310, Loss: 1.8475
Batch 320, Loss: 1.8521
Batch 330, Loss: 1.8684
Batch 340, Loss: 1.8741
Batch 350, Loss: 1.8436
Batch 360, Loss: 1.8550
Batch 370, Loss: 1.8809
Batch 380, Loss: 1.8657
Batch 390, Loss: 1.8682
Epoch 10 learning rate: 0.1
Epoch 10 time: 118.94326448440552 seconds
Epoch 10 accuracy: 9.5%
Batch 10, Loss: 1.8550
Batch 20, Loss: 1.8548
Batch 30, Loss: 1.8589
Batch 40, Loss: 1.8462
Batch 50, Loss: 1.8543
Batch 60, Loss: 1.8344
Batch 70, Loss: 1.8507
Batch 80, Loss: 1.8201
Batch 90, Loss: 1.8501
Batch 100, Loss: 1.8386
Batch 110, Loss: 1.8363
Batch 120, Loss: 1.8298
Batch 130, Loss: 1.8369
Batch 140, Loss: 1.8106
Batch 150, Loss: 1.8269
Batch 160, Loss: 1.8101
Batch 170, Loss: 1.8077
Batch 180, Loss: 1.7926
Batch 190, Loss: 1.8113
Batch 200, Loss: 1.8132
Batch 210, Loss: 1.8140
Batch 220, Loss: 1.8050
Batch 230, Loss: 1.7895
Batch 240, Loss: 1.8079
Batch 250, Loss: 1.7853
Batch 260, Loss: 1.7981
Batch 270, Loss: 1.7968
Batch 280, Loss: 1.7941
Batch 290, Loss: 1.7760
Batch 300, Loss: 1.7780
Batch 310, Loss: 1.7920
Batch 320, Loss: 1.7882
Batch 330, Loss: 1.7767
Batch 340, Loss: 1.7762
Batch 350, Loss: 1.7788
Batch 360, Loss: 1.7823
Batch 370, Loss: 1.7781
Batch 380, Loss: 1.7824
Batch 390, Loss: 1.7695
Epoch 11 learning rate: 0.1
Epoch 11 time: 118.93883156776428 seconds
Epoch 11 accuracy: 10.08%
Batch 10, Loss: 1.7695
Batch 20, Loss: 1.7742
Batch 30, Loss: 1.7720
Batch 40, Loss: 1.7736
Batch 50, Loss: 1.7637
Batch 60, Loss: 1.7613
Batch 70, Loss: 1.7636
Batch 80, Loss: 1.7609
Batch 90, Loss: 1.7682
Batch 100, Loss: 1.7566
Batch 110, Loss: 1.7613
Batch 120, Loss: 1.7656
Batch 130, Loss: 1.7599
Batch 140, Loss: 1.7629
Batch 150, Loss: 1.7618
Batch 160, Loss: 1.7590
Batch 170, Loss: 1.7589
Batch 180, Loss: 1.7606
Batch 190, Loss: 1.7594
Batch 200, Loss: 1.7580
Batch 210, Loss: 1.7575
Batch 220, Loss: 1.7572
Batch 230, Loss: 1.7569
Batch 240, Loss: 1.7573
Batch 250, Loss: 1.7571
Batch 260, Loss: 1.7574
Batch 270, Loss: 1.7568
Batch 280, Loss: 1.7559
Batch 290, Loss: 1.7572
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7585
Batch 320, Loss: 1.7564
Batch 330, Loss: 1.7569
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7570
Batch 360, Loss: 1.7560
Batch 370, Loss: 1.7573
Batch 380, Loss: 1.7570
Batch 390, Loss: 1.7560
Epoch 12 learning rate: 0.1
Epoch 12 time: 118.95580792427063 seconds
Epoch 12 accuracy: 9.33%
Batch 10, Loss: 1.7572
Batch 20, Loss: 1.7560
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7570
Batch 50, Loss: 1.7565
Batch 60, Loss: 1.7568
Batch 70, Loss: 1.7572
Batch 80, Loss: 1.7555
Batch 90, Loss: 1.7576
Batch 100, Loss: 1.7560
Batch 110, Loss: 1.7577
Batch 120, Loss: 1.7573
Batch 130, Loss: 1.7569
Batch 140, Loss: 1.7572
Batch 150, Loss: 1.7564
Batch 160, Loss: 1.7564
Batch 170, Loss: 1.7560
Batch 180, Loss: 1.7572
Batch 190, Loss: 1.7563
Batch 200, Loss: 1.7577
Batch 210, Loss: 1.7574
Batch 220, Loss: 1.7573
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7586
Batch 250, Loss: 1.7571
Batch 260, Loss: 1.7586
Batch 270, Loss: 1.7575
Batch 280, Loss: 1.7560
Batch 290, Loss: 1.7567
Batch 300, Loss: 1.7571
Batch 310, Loss: 1.7569
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7569
Batch 340, Loss: 1.7582
Batch 350, Loss: 1.7565
Batch 360, Loss: 1.7576
Batch 370, Loss: 1.7567
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7565
Epoch 13 learning rate: 0.1
Epoch 13 time: 118.99226999282837 seconds
Epoch 13 accuracy: 10.61%
Batch 10, Loss: 1.7565
Batch 20, Loss: 1.7565
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7558
Batch 50, Loss: 1.7565
Batch 60, Loss: 1.7569
Batch 70, Loss: 1.7550
Batch 80, Loss: 1.7589
Batch 90, Loss: 1.7567
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7561
Batch 120, Loss: 1.7560
Batch 130, Loss: 1.7562
Batch 140, Loss: 1.7532
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7549
Batch 170, Loss: 1.7567
Batch 180, Loss: 1.7562
Batch 190, Loss: 1.7559
Batch 200, Loss: 1.7572
Batch 210, Loss: 1.7558
Batch 220, Loss: 1.7562
Batch 230, Loss: 1.7569
Batch 240, Loss: 1.7589
Batch 250, Loss: 1.7554
Batch 260, Loss: 1.7570
Batch 270, Loss: 1.7564
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7568
Batch 300, Loss: 1.7568
Batch 310, Loss: 1.7569
Batch 320, Loss: 1.7565
Batch 330, Loss: 1.7589
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7568
Batch 360, Loss: 1.7561
Batch 370, Loss: 1.7569
Batch 380, Loss: 1.7566
Batch 390, Loss: 1.7560
Epoch 14 learning rate: 0.1
Epoch 14 time: 119.01992106437683 seconds
Epoch 14 accuracy: 8.97%
Batch 10, Loss: 1.7573
Batch 20, Loss: 1.7565
Batch 30, Loss: 1.7549
Batch 40, Loss: 1.7565
Batch 50, Loss: 1.7568
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7565
Batch 80, Loss: 1.7565
Batch 90, Loss: 1.7569
Batch 100, Loss: 1.7573
Batch 110, Loss: 1.7572
Batch 120, Loss: 1.7569
Batch 130, Loss: 1.7558
Batch 140, Loss: 1.7564
Batch 150, Loss: 1.7566
Batch 160, Loss: 1.7583
Batch 170, Loss: 1.7575
Batch 180, Loss: 1.7575
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7582
Batch 210, Loss: 1.7571
Batch 220, Loss: 1.7563
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7561
Batch 250, Loss: 1.7554
Batch 260, Loss: 1.7569
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7564
Batch 290, Loss: 1.7577
Batch 300, Loss: 1.7575
Batch 310, Loss: 1.7577
Batch 320, Loss: 1.7567
Batch 330, Loss: 1.7572
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7574
Batch 360, Loss: 1.7562
Batch 370, Loss: 1.7571
Batch 380, Loss: 1.7574
Batch 390, Loss: 1.7574
Epoch 15 learning rate: 0.1
Epoch 15 time: 119.01203179359436 seconds
Epoch 15 accuracy: 8.41%
Batch 10, Loss: 1.7568
Batch 20, Loss: 1.7570
Batch 30, Loss: 1.7559
Batch 40, Loss: 1.7559
Batch 50, Loss: 1.7572
Batch 60, Loss: 1.7567
Batch 70, Loss: 1.7570
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7568
Batch 110, Loss: 1.7567
Batch 120, Loss: 1.7567
Batch 130, Loss: 1.7594
Batch 140, Loss: 1.7583
Batch 150, Loss: 1.7572
Batch 160, Loss: 1.7575
Batch 170, Loss: 1.7566
Batch 180, Loss: 1.7584
Batch 190, Loss: 1.7576
Batch 200, Loss: 1.7571
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7582
Batch 230, Loss: 1.7572
Batch 240, Loss: 1.7572
Batch 250, Loss: 1.7578
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7568
Batch 280, Loss: 1.7580
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7576
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7572
Batch 340, Loss: 1.7577
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7573
Batch 370, Loss: 1.7583
Batch 380, Loss: 1.7581
Batch 390, Loss: 1.7579
Epoch 16 learning rate: 0.1
Epoch 16 time: 118.98325157165527 seconds
Epoch 16 accuracy: 9.9%
Batch 10, Loss: 1.7569
Batch 20, Loss: 1.7582
Batch 30, Loss: 1.7570
Batch 40, Loss: 1.7577
Batch 50, Loss: 1.7571
Batch 60, Loss: 1.7571
Batch 70, Loss: 1.7585
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7586
Batch 120, Loss: 1.7580
Batch 130, Loss: 1.7574
Batch 140, Loss: 1.7582
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7577
Batch 170, Loss: 1.7585
Batch 180, Loss: 1.7579
Batch 190, Loss: 1.7577
Batch 200, Loss: 1.7574
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7582
Batch 230, Loss: 1.7579
Batch 240, Loss: 1.7575
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7568
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7583
Batch 300, Loss: 1.7583
Batch 310, Loss: 1.7574
Batch 320, Loss: 1.7583
Batch 330, Loss: 1.7583
Batch 340, Loss: 1.7580
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7577
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7578
Epoch 17 learning rate: 0.1
Epoch 17 time: 118.95972943305969 seconds
Epoch 17 accuracy: 11.06%
Batch 10, Loss: 1.7575
Batch 20, Loss: 1.7584
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7578
Batch 50, Loss: 1.7581
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7576
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7573
Batch 100, Loss: 1.7574
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7585
Batch 130, Loss: 1.7576
Batch 140, Loss: 1.7573
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7579
Batch 170, Loss: 1.7578
Batch 180, Loss: 1.7586
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7576
Batch 210, Loss: 1.7575
Batch 220, Loss: 1.7574
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7582
Batch 250, Loss: 1.7592
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7575
Batch 290, Loss: 1.7575
Batch 300, Loss: 1.7582
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7583
Batch 330, Loss: 1.7579
Batch 340, Loss: 1.7574
Batch 350, Loss: 1.7582
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7576
Batch 380, Loss: 1.7578
Batch 390, Loss: 1.7582
Epoch 18 learning rate: 0.1
Epoch 18 time: 118.95624589920044 seconds
Epoch 18 accuracy: 10.14%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7580
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7581
Batch 50, Loss: 1.7582
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7580
Batch 80, Loss: 1.7577
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7579
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7576
Batch 160, Loss: 1.7573
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7577
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7581
Batch 210, Loss: 1.7580
Batch 220, Loss: 1.7589
Batch 230, Loss: 1.7583
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7574
Batch 260, Loss: 1.7581
Batch 270, Loss: 1.7580
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7582
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7574
Batch 320, Loss: 1.7579
Batch 330, Loss: 1.7584
Batch 340, Loss: 1.7576
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7575
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7578
Epoch 19 learning rate: 0.1
Epoch 19 time: 118.84200811386108 seconds
Epoch 19 accuracy: 10.15%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7582
Batch 30, Loss: 1.7577
Batch 40, Loss: 1.7580
Batch 50, Loss: 1.7579
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7579
Batch 80, Loss: 1.7579
Batch 90, Loss: 1.7582
Batch 100, Loss: 1.7578
Batch 110, Loss: 1.7576
Batch 120, Loss: 1.7579
Batch 130, Loss: 1.7581
Batch 140, Loss: 1.7578
Batch 150, Loss: 1.7582
Batch 160, Loss: 1.7582
Batch 170, Loss: 1.7575
Batch 180, Loss: 1.7584
Batch 190, Loss: 1.7586
Batch 200, Loss: 1.7581
Batch 210, Loss: 1.7579
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7575
Batch 240, Loss: 1.7583
Batch 250, Loss: 1.7581
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7579
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7581
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7583
Batch 320, Loss: 1.7577
Batch 330, Loss: 1.7578
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7579
Batch 370, Loss: 1.7576
Batch 380, Loss: 1.7582
Batch 390, Loss: 1.7580
Epoch 20 learning rate: 0.1
Epoch 20 time: 119.13579535484314 seconds
Epoch 20 accuracy: 10.0%
Total training time: 2394.460446834564 seconds
