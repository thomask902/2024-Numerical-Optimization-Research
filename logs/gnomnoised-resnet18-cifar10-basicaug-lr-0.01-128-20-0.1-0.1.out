The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-128/2024-08-18-15:59:51
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 22.4503
Batch 200, Loss: 5.5141
Batch 300, Loss: 4.2104
Noise applied in 0 out of 391 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 128.7586407661438 seconds
Epoch 1 accuracy: 10.19%
Batch 100, Loss: 3.2362
Batch 200, Loss: 2.9078
Batch 300, Loss: 2.7697
Noise applied in 0 out of 782 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 120.10265731811523 seconds
Epoch 2 accuracy: 10.9%
Batch 100, Loss: 2.4567
Batch 200, Loss: 2.3417
Batch 300, Loss: 2.3158
Noise applied in 0 out of 1173 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 119.75668048858643 seconds
Epoch 3 accuracy: 11.34%
Batch 100, Loss: 2.1845
Batch 200, Loss: 2.1501
Batch 300, Loss: 2.0673
Noise applied in 0 out of 1564 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 119.85444617271423 seconds
Epoch 4 accuracy: 11.74%
Batch 100, Loss: 2.0271
Batch 200, Loss: 1.9890
Batch 300, Loss: 1.9524
Noise applied in 0 out of 1955 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 119.92929124832153 seconds
Epoch 5 accuracy: 11.77%
Batch 100, Loss: 1.9199
Batch 200, Loss: 1.9081
Batch 300, Loss: 1.8934
Noise applied in 0 out of 2346 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 119.5603358745575 seconds
Epoch 6 accuracy: 11.71%
Batch 100, Loss: 1.8640
Batch 200, Loss: 1.8526
Batch 300, Loss: 1.8495
Noise applied in 6 out of 2737 batches, 0.22
Epoch 7 learning rate: 0.01
Epoch 7 time: 123.41772317886353 seconds
Epoch 7 accuracy: 11.7%
Batch 100, Loss: 1.8392
Batch 200, Loss: 1.8266
Batch 300, Loss: 1.8095
Noise applied in 28 out of 3128 batches, 0.90
Epoch 8 learning rate: 0.01
Epoch 8 time: 133.24724125862122 seconds
Epoch 8 accuracy: 11.68%
Batch 100, Loss: 1.8108
Batch 200, Loss: 1.8095
Batch 300, Loss: 1.8020
Noise applied in 72 out of 3519 batches, 2.05
Epoch 9 learning rate: 0.01
Epoch 9 time: 146.18085932731628 seconds
Epoch 9 accuracy: 11.07%
Batch 100, Loss: 1.7968
Batch 200, Loss: 1.7911
Batch 300, Loss: 1.7896
Noise applied in 159 out of 3910 batches, 4.07
Epoch 10 learning rate: 0.01
Epoch 10 time: 171.48032641410828 seconds
Epoch 10 accuracy: 10.91%
Batch 100, Loss: 1.7892
Batch 200, Loss: 1.7894
Batch 300, Loss: 1.7846
Noise applied in 280 out of 4301 batches, 6.51
Epoch 11 learning rate: 0.01
Epoch 11 time: 191.45390796661377 seconds
Epoch 11 accuracy: 10.88%
Batch 100, Loss: 1.7862
Batch 200, Loss: 1.7829
Batch 300, Loss: 1.7791
Noise applied in 432 out of 4692 batches, 9.21
Epoch 12 learning rate: 0.01
Epoch 12 time: 210.93902397155762 seconds
Epoch 12 accuracy: 9.81%
Batch 100, Loss: 1.7808
Batch 200, Loss: 1.7799
Batch 300, Loss: 1.7817
Noise applied in 623 out of 5083 batches, 12.26
Epoch 13 learning rate: 0.01
Epoch 13 time: 231.63857913017273 seconds
Epoch 13 accuracy: 9.79%
Batch 100, Loss: 1.7776
Batch 200, Loss: 1.7804
Batch 300, Loss: 1.7783
Noise applied in 836 out of 5474 batches, 15.27
Epoch 14 learning rate: 0.01
Epoch 14 time: 240.44912266731262 seconds
Epoch 14 accuracy: 10.05%
Batch 100, Loss: 1.7739
Batch 200, Loss: 1.7765
Batch 300, Loss: 1.7746
Noise applied in 1077 out of 5865 batches, 18.36
Epoch 15 learning rate: 0.01
Epoch 15 time: 260.9688103199005 seconds
Epoch 15 accuracy: 10.16%
Batch 100, Loss: 1.7725
Batch 200, Loss: 1.7731
Batch 300, Loss: 1.7705
Noise applied in 1322 out of 6256 batches, 21.13
Epoch 16 learning rate: 0.01
Epoch 16 time: 259.4054226875305 seconds
Epoch 16 accuracy: 10.02%
Batch 100, Loss: 1.7714
Batch 200, Loss: 1.7714
Batch 300, Loss: 1.7699
Noise applied in 1585 out of 6647 batches, 23.85
Epoch 17 learning rate: 0.01
Epoch 17 time: 265.7562928199768 seconds
Epoch 17 accuracy: 10.13%
Batch 100, Loss: 1.7683
Batch 200, Loss: 1.7688
Batch 300, Loss: 1.7694
Noise applied in 1864 out of 7038 batches, 26.48
Epoch 18 learning rate: 0.01
Epoch 18 time: 271.4168972969055 seconds
Epoch 18 accuracy: 9.93%
Batch 100, Loss: 1.7695
Batch 200, Loss: 1.7662
Batch 300, Loss: 1.7681
Noise applied in 2159 out of 7429 batches, 29.06
Epoch 19 learning rate: 0.01
Epoch 19 time: 288.4574797153473 seconds
Epoch 19 accuracy: 9.94%
Batch 100, Loss: 1.7669
Batch 200, Loss: 1.7669
Batch 300, Loss: 1.7662
Noise applied in 2450 out of 7820 batches, 31.33
Epoch 20 learning rate: 0.01
Epoch 20 time: 283.21533036231995 seconds
Epoch 20 accuracy: 10.06%
rho:  0.04 , alpha:  0.3
Total training time: 3806.0165371894836 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.1764
Norm of the Gradient: 1.1906063557e-01
Smallest Hessian Eigenvalue: -0.0831
Noise Threshold: 0.1
Noise Radius: 0.1
