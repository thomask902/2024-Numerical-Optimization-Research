The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:02:14
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 374.6432
Batch 50, Loss: 328.5913
Batch 75, Loss: 125.3015
Batch 100, Loss: 74.6024
Batch 125, Loss: 54.4242
Batch 150, Loss: 43.1309
Batch 175, Loss: 35.8285
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 306.23026967048645 seconds
Epoch 1 accuracy: 8.27%
Batch 25, Loss: 28.1396
Batch 50, Loss: 25.0716
Batch 75, Loss: 22.7064
Batch 100, Loss: 20.9702
Batch 125, Loss: 19.5606
Batch 150, Loss: 18.3759
Batch 175, Loss: 17.3570
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 262.6456973552704 seconds
Epoch 2 accuracy: 8.36%
Batch 25, Loss: 15.9296
Batch 50, Loss: 15.2113
Batch 75, Loss: 14.5632
Batch 100, Loss: 13.9731
Batch 125, Loss: 13.4322
Batch 150, Loss: 12.9342
Batch 175, Loss: 12.4766
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 263.3041157722473 seconds
Epoch 3 accuracy: 8.47%
Batch 25, Loss: 11.7823
Batch 50, Loss: 11.4092
Batch 75, Loss: 11.0582
Batch 100, Loss: 10.7269
Batch 125, Loss: 10.4136
Batch 150, Loss: 10.1173
Batch 175, Loss: 9.8349
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 263.56747484207153 seconds
Epoch 4 accuracy: 8.81%
Batch 25, Loss: 9.3910
Batch 50, Loss: 9.1398
Batch 75, Loss: 8.8972
Batch 100, Loss: 8.6634
Batch 125, Loss: 8.4391
Batch 150, Loss: 8.2242
Batch 175, Loss: 8.0188
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 263.58708596229553 seconds
Epoch 5 accuracy: 8.92%
Batch 25, Loss: 7.6938
Batch 50, Loss: 7.5124
Batch 75, Loss: 7.3401
Batch 100, Loss: 7.1756
Batch 125, Loss: 7.0195
Batch 150, Loss: 6.8706
Batch 175, Loss: 6.7281
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 263.94010400772095 seconds
Epoch 6 accuracy: 8.87%
Batch 25, Loss: 6.5035
Batch 50, Loss: 6.3789
Batch 75, Loss: 6.2605
Batch 100, Loss: 6.1468
Batch 125, Loss: 6.0375
Batch 150, Loss: 5.9330
Batch 175, Loss: 5.8330
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 263.8363244533539 seconds
Epoch 7 accuracy: 8.94%
Batch 25, Loss: 5.6733
Batch 50, Loss: 5.5822
Batch 75, Loss: 5.4937
Batch 100, Loss: 5.4075
Batch 125, Loss: 5.3240
Batch 150, Loss: 5.2438
Batch 175, Loss: 5.1660
Noise applied in 113 out of 1536 batches, 7.36
Epoch 8 learning rate: 0.01
Epoch 8 time: 316.374383687973 seconds
Epoch 8 accuracy: 11.65%
Batch 25, Loss: 5.0406
Batch 50, Loss: 4.9694
Batch 75, Loss: 4.9000
Batch 100, Loss: 4.8325
Batch 125, Loss: 4.7664
Batch 150, Loss: 4.7026
Batch 175, Loss: 4.6442
Noise applied in 305 out of 1728 batches, 17.65
Epoch 9 learning rate: 0.01
Epoch 9 time: 355.45493817329407 seconds
Epoch 9 accuracy: 9.66%
Batch 25, Loss: 4.5516
Batch 50, Loss: 4.4986
Batch 75, Loss: 4.4466
Batch 100, Loss: 4.3959
Batch 125, Loss: 4.3458
Batch 150, Loss: 4.2968
Batch 175, Loss: 4.2494
Noise applied in 497 out of 1920 batches, 25.89
Epoch 10 learning rate: 0.01
Epoch 10 time: 351.7797598838806 seconds
Epoch 10 accuracy: 8.87%
Batch 25, Loss: 4.1718
Batch 50, Loss: 4.1284
Batch 75, Loss: 4.0872
Batch 100, Loss: 4.0473
Batch 125, Loss: 4.0084
Batch 150, Loss: 3.9704
Batch 175, Loss: 3.9334
Noise applied in 689 out of 2112 batches, 32.62
Epoch 11 learning rate: 0.01
Epoch 11 time: 351.965993642807 seconds
Epoch 11 accuracy: 8.55%
Batch 25, Loss: 3.8729
Batch 50, Loss: 3.8378
Batch 75, Loss: 3.8036
Batch 100, Loss: 3.7703
Batch 125, Loss: 3.7378
Batch 150, Loss: 3.7059
Batch 175, Loss: 3.6747
Noise applied in 881 out of 2304 batches, 38.24
Epoch 12 learning rate: 0.01
Epoch 12 time: 352.7267563343048 seconds
Epoch 12 accuracy: 8.25%
Batch 25, Loss: 3.6238
Batch 50, Loss: 3.5943
Batch 75, Loss: 3.5656
Batch 100, Loss: 3.5381
Batch 125, Loss: 3.5115
Batch 150, Loss: 3.4857
Batch 175, Loss: 3.4606
Noise applied in 1073 out of 2496 batches, 42.99
Epoch 13 learning rate: 0.01
Epoch 13 time: 352.6975724697113 seconds
Epoch 13 accuracy: 8.14%
Batch 25, Loss: 3.4196
Batch 50, Loss: 3.3959
Batch 75, Loss: 3.3728
Batch 100, Loss: 3.3504
Batch 125, Loss: 3.3289
Batch 150, Loss: 3.3081
Batch 175, Loss: 3.2878
Noise applied in 1265 out of 2688 batches, 47.06
Epoch 14 learning rate: 0.01
Epoch 14 time: 354.29461097717285 seconds
Epoch 14 accuracy: 8.13%
Batch 25, Loss: 3.2545
Batch 50, Loss: 3.2352
Batch 75, Loss: 3.2162
Batch 100, Loss: 3.1974
Batch 125, Loss: 3.1789
Batch 150, Loss: 3.1609
Batch 175, Loss: 3.1433
Noise applied in 1457 out of 2880 batches, 50.59
Epoch 15 learning rate: 0.01
Epoch 15 time: 354.1149411201477 seconds
Epoch 15 accuracy: 8.13%
Batch 25, Loss: 3.1146
Batch 50, Loss: 3.0979
Batch 75, Loss: 3.0816
Batch 100, Loss: 3.0655
Batch 125, Loss: 3.0496
Batch 150, Loss: 3.0338
Batch 175, Loss: 3.0182
Noise applied in 1649 out of 3072 batches, 53.68
Epoch 16 learning rate: 0.01
Epoch 16 time: 379.0656793117523 seconds
Epoch 16 accuracy: 8.17%
Batch 25, Loss: 2.9925
Batch 50, Loss: 2.9774
Batch 75, Loss: 2.9627
Batch 100, Loss: 2.9482
Batch 125, Loss: 2.9341
Batch 150, Loss: 2.9204
Batch 175, Loss: 2.9071
Noise applied in 1841 out of 3264 batches, 56.40
Epoch 17 learning rate: 0.01
Epoch 17 time: 382.6587998867035 seconds
Epoch 17 accuracy: 8.27%
Batch 25, Loss: 2.8852
Batch 50, Loss: 2.8724
Batch 75, Loss: 2.8599
Batch 100, Loss: 2.8476
Batch 125, Loss: 2.8355
Batch 150, Loss: 2.8235
Batch 175, Loss: 2.8117
Noise applied in 2033 out of 3456 batches, 58.83
Epoch 18 learning rate: 0.01
Epoch 18 time: 356.38587856292725 seconds
Epoch 18 accuracy: 8.35%
Batch 25, Loss: 2.7921
Batch 50, Loss: 2.7805
Batch 75, Loss: 2.7692
Batch 100, Loss: 2.7580
Batch 125, Loss: 2.7469
Batch 150, Loss: 2.7359
Batch 175, Loss: 2.7250
Noise applied in 2225 out of 3648 batches, 60.99
Epoch 19 learning rate: 0.01
Epoch 19 time: 407.1570394039154 seconds
Epoch 19 accuracy: 8.54%
Batch 25, Loss: 2.7071
Batch 50, Loss: 2.6966
Batch 75, Loss: 2.6862
Batch 100, Loss: 2.6759
Batch 125, Loss: 2.6658
Batch 150, Loss: 2.6558
Batch 175, Loss: 2.6460
Noise applied in 2417 out of 3840 batches, 62.94
Epoch 20 learning rate: 0.01
Epoch 20 time: 355.09020948410034 seconds
Epoch 20 accuracy: 8.67%
rho:  0.04 , alpha:  0.3
Total training time: 6556.894729614258 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 5.5526
Norm of the Gradient: 5.5402553082e-01
Smallest Hessian Eigenvalue: -0.1638
Noise Threshold: 1.0
Noise Radius: 0.05
