The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-15:18:26
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 413.4418
Batch 50, Loss: 263.7733
Batch 75, Loss: 81.4541
Batch 100, Loss: 52.3484
Batch 125, Loss: 39.5814
Batch 150, Loss: 31.5817
Batch 175, Loss: 25.9011
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 540.4005348682404 seconds
Epoch 1 accuracy: 10.73%
Batch 25, Loss: 20.4033
Batch 50, Loss: 18.2636
Batch 75, Loss: 16.5612
Batch 100, Loss: 15.1644
Batch 125, Loss: 13.9516
Batch 150, Loss: 13.0458
Batch 175, Loss: 12.3645
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 278.35287523269653 seconds
Epoch 2 accuracy: 12.98%
Batch 25, Loss: 11.4349
Batch 50, Loss: 10.9658
Batch 75, Loss: 10.5461
Batch 100, Loss: 10.1676
Batch 125, Loss: 9.8239
Batch 150, Loss: 9.5104
Batch 175, Loss: 9.2245
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 299.0513286590576 seconds
Epoch 3 accuracy: 14.41%
Batch 25, Loss: 8.7997
Batch 50, Loss: 8.5731
Batch 75, Loss: 8.3620
Batch 100, Loss: 8.1633
Batch 125, Loss: 7.9743
Batch 150, Loss: 7.7957
Batch 175, Loss: 7.6278
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 278.76444029808044 seconds
Epoch 4 accuracy: 15.03%
Batch 25, Loss: 7.3729
Batch 50, Loss: 7.2359
Batch 75, Loss: 7.1073
Batch 100, Loss: 6.9871
Batch 125, Loss: 6.8740
Batch 150, Loss: 6.7666
Batch 175, Loss: 6.6640
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 304.8064544200897 seconds
Epoch 5 accuracy: 15.35%
Batch 25, Loss: 6.5036
Batch 50, Loss: 6.4149
Batch 75, Loss: 6.3304
Batch 100, Loss: 6.2497
Batch 125, Loss: 6.1732
Batch 150, Loss: 6.0999
Batch 175, Loss: 6.0295
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 317.17967796325684 seconds
Epoch 6 accuracy: 15.8%
Batch 25, Loss: 5.9177
Batch 50, Loss: 5.8548
Batch 75, Loss: 5.7943
Batch 100, Loss: 5.7359
Batch 125, Loss: 5.6796
Batch 150, Loss: 5.6252
Batch 175, Loss: 5.5725
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 301.0686819553375 seconds
Epoch 7 accuracy: 15.96%
Batch 25, Loss: 5.4873
Batch 50, Loss: 5.4384
Batch 75, Loss: 5.3910
Batch 100, Loss: 5.3449
Batch 125, Loss: 5.3001
Batch 150, Loss: 5.2564
Batch 175, Loss: 5.2138
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 327.48379349708557 seconds
Epoch 8 accuracy: 16.03%
Batch 25, Loss: 5.1451
Batch 50, Loss: 5.1056
Batch 75, Loss: 5.0672
Batch 100, Loss: 5.0301
Batch 125, Loss: 4.9942
Batch 150, Loss: 4.9595
Batch 175, Loss: 4.9258
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 287.31116366386414 seconds
Epoch 9 accuracy: 16.17%
Batch 25, Loss: 4.8701
Batch 50, Loss: 4.8371
Batch 75, Loss: 4.8047
Batch 100, Loss: 4.7731
Batch 125, Loss: 4.7423
Batch 150, Loss: 4.7120
Batch 175, Loss: 4.6823
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 335.68507409095764 seconds
Epoch 10 accuracy: 16.18%
Batch 25, Loss: 4.6338
Batch 50, Loss: 4.6060
Batch 75, Loss: 4.5787
Batch 100, Loss: 4.5520
Batch 125, Loss: 4.5259
Batch 150, Loss: 4.5003
Batch 175, Loss: 4.4750
Noise applied in 0 out of 192 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 277.27396178245544 seconds
Epoch 11 accuracy: 16.15%
Batch 25, Loss: 4.4334
Batch 50, Loss: 4.4091
Batch 75, Loss: 4.3852
Batch 100, Loss: 4.3616
Batch 125, Loss: 4.3385
Batch 150, Loss: 4.3157
Batch 175, Loss: 4.2933
Noise applied in 0 out of 192 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 277.2340385913849 seconds
Epoch 12 accuracy: 16.05%
Batch 25, Loss: 4.2565
Batch 50, Loss: 4.2350
Batch 75, Loss: 4.2137
Batch 100, Loss: 4.1926
Batch 125, Loss: 4.1720
Batch 150, Loss: 4.1517
Batch 175, Loss: 4.1317
Noise applied in 0 out of 192 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 304.4478976726532 seconds
Epoch 13 accuracy: 16.12%
Batch 25, Loss: 4.0987
Batch 50, Loss: 4.0795
Batch 75, Loss: 4.0605
Batch 100, Loss: 4.0418
Batch 125, Loss: 4.0234
Batch 150, Loss: 4.0052
Batch 175, Loss: 3.9873
Noise applied in 0 out of 192 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 276.59165954589844 seconds
Epoch 14 accuracy: 16.05%
Batch 25, Loss: 3.9580
Batch 50, Loss: 3.9409
Batch 75, Loss: 3.9241
Batch 100, Loss: 3.9075
Batch 125, Loss: 3.8911
Batch 150, Loss: 3.8748
Batch 175, Loss: 3.8587
Noise applied in 0 out of 192 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 275.793484210968 seconds
Epoch 15 accuracy: 16.14%
Batch 25, Loss: 3.8320
Batch 50, Loss: 3.8164
Batch 75, Loss: 3.8009
Batch 100, Loss: 3.7855
Batch 125, Loss: 3.7703
Batch 150, Loss: 3.7553
Batch 175, Loss: 3.7405
Noise applied in 0 out of 192 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 278.9700288772583 seconds
Epoch 16 accuracy: 16.09%
Batch 25, Loss: 3.7160
Batch 50, Loss: 3.7016
Batch 75, Loss: 3.6874
Batch 100, Loss: 3.6733
Batch 125, Loss: 3.6594
Batch 150, Loss: 3.6455
Batch 175, Loss: 3.6319
Noise applied in 0 out of 192 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 388.15977907180786 seconds
Epoch 17 accuracy: 16.09%
Batch 25, Loss: 3.6093
Batch 50, Loss: 3.5959
Batch 75, Loss: 3.5827
Batch 100, Loss: 3.5697
Batch 125, Loss: 3.5567
Batch 150, Loss: 3.5440
Batch 175, Loss: 3.5313
Noise applied in 0 out of 192 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 355.1508278846741 seconds
Epoch 18 accuracy: 16.04%
Batch 25, Loss: 3.5104
Batch 50, Loss: 3.4983
Batch 75, Loss: 3.4863
Batch 100, Loss: 3.4744
Batch 125, Loss: 3.4627
Batch 150, Loss: 3.4511
Batch 175, Loss: 3.4397
Noise applied in 0 out of 192 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 307.89055490493774 seconds
Epoch 19 accuracy: 16.05%
Batch 25, Loss: 3.4209
Batch 50, Loss: 3.4098
Batch 75, Loss: 3.3988
Batch 100, Loss: 3.3879
Batch 125, Loss: 3.3772
Batch 150, Loss: 3.3666
Batch 175, Loss: 3.3560
Noise applied in 0 out of 192 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 287.5282406806946 seconds
Epoch 20 accuracy: 16.08%
rho:  0.04 , alpha:  0.3
Total training time: 6299.174165725708 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.0740
Norm of the Gradient: 1.0090327263e+00
Smallest Hessian Eigenvalue: -0.2063
Noise Threshold: 0.4
Noise Radius: 0.01
