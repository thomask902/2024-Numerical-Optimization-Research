The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:02:39
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 32.2025
Batch 50, Loss: 14.8353
Batch 75, Loss: 9.0121
Batch 100, Loss: 7.4951
Batch 125, Loss: 6.7187
Batch 150, Loss: 6.1610
Batch 175, Loss: 5.7100
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 304.0137634277344 seconds
Epoch 1 accuracy: 11.15%
Batch 25, Loss: 5.1157
Batch 50, Loss: 4.8326
Batch 75, Loss: 4.5878
Batch 100, Loss: 4.3734
Batch 125, Loss: 4.1835
Batch 150, Loss: 4.0143
Batch 175, Loss: 3.8622
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 262.52042055130005 seconds
Epoch 2 accuracy: 12.85%
Batch 25, Loss: 3.6361
Batch 50, Loss: 3.5169
Batch 75, Loss: 3.4075
Batch 100, Loss: 3.3067
Batch 125, Loss: 3.2133
Batch 150, Loss: 3.1270
Batch 175, Loss: 3.0473
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 261.98089241981506 seconds
Epoch 3 accuracy: 12.36%
Batch 25, Loss: 2.9267
Batch 50, Loss: 2.8617
Batch 75, Loss: 2.8011
Batch 100, Loss: 2.7448
Batch 125, Loss: 2.6922
Batch 150, Loss: 2.6428
Batch 175, Loss: 2.5963
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 263.1087968349457 seconds
Epoch 4 accuracy: 12.47%
Batch 25, Loss: 2.5241
Batch 50, Loss: 2.4844
Batch 75, Loss: 2.4470
Batch 100, Loss: 2.4114
Batch 125, Loss: 2.3777
Batch 150, Loss: 2.3457
Batch 175, Loss: 2.3152
Noise applied in 110 out of 960 batches, 11.46
Epoch 5 learning rate: 0.01
Epoch 5 time: 314.6006381511688 seconds
Epoch 5 accuracy: 12.73%
Batch 25, Loss: 2.2673
Batch 50, Loss: 2.2407
Batch 75, Loss: 2.2152
Batch 100, Loss: 2.1910
Batch 125, Loss: 2.1678
Batch 150, Loss: 2.1455
Batch 175, Loss: 2.1242
Noise applied in 302 out of 1152 batches, 26.22
Epoch 6 learning rate: 0.01
Epoch 6 time: 353.0373251438141 seconds
Epoch 6 accuracy: 12.01%
Batch 25, Loss: 2.0903
Batch 50, Loss: 2.0711
Batch 75, Loss: 2.0527
Batch 100, Loss: 2.0350
Batch 125, Loss: 2.0181
Batch 150, Loss: 2.0019
Batch 175, Loss: 1.9865
Noise applied in 494 out of 1344 batches, 36.76
Epoch 7 learning rate: 0.01
Epoch 7 time: 352.9655418395996 seconds
Epoch 7 accuracy: 12.02%
Batch 25, Loss: 1.9626
Batch 50, Loss: 1.9496
Batch 75, Loss: 1.9377
Batch 100, Loss: 1.9266
Batch 125, Loss: 1.9164
Batch 150, Loss: 1.9069
Batch 175, Loss: 1.8980
Noise applied in 686 out of 1536 batches, 44.66
Epoch 8 learning rate: 0.01
Epoch 8 time: 352.7945020198822 seconds
Epoch 8 accuracy: 12.06%
Batch 25, Loss: 1.8844
Batch 50, Loss: 1.8769
Batch 75, Loss: 1.8699
Batch 100, Loss: 1.8631
Batch 125, Loss: 1.8567
Batch 150, Loss: 1.8505
Batch 175, Loss: 1.8446
Noise applied in 878 out of 1728 batches, 50.81
Epoch 9 learning rate: 0.01
Epoch 9 time: 353.1612808704376 seconds
Epoch 9 accuracy: 12.09%
Batch 25, Loss: 1.8350
Batch 50, Loss: 1.8295
Batch 75, Loss: 1.8242
Batch 100, Loss: 1.8191
Batch 125, Loss: 1.8142
Batch 150, Loss: 1.8095
Batch 175, Loss: 1.8051
Noise applied in 1070 out of 1920 batches, 55.73
Epoch 10 learning rate: 0.01
Epoch 10 time: 352.6196663379669 seconds
Epoch 10 accuracy: 12.05%
Batch 25, Loss: 1.7982
Batch 50, Loss: 1.7943
Batch 75, Loss: 1.7906
Batch 100, Loss: 1.7871
Batch 125, Loss: 1.7838
Batch 150, Loss: 1.7807
Batch 175, Loss: 1.7777
Noise applied in 1262 out of 2112 batches, 59.75
Epoch 11 learning rate: 0.01
Epoch 11 time: 352.50364327430725 seconds
Epoch 11 accuracy: 11.98%
Batch 25, Loss: 1.7731
Batch 50, Loss: 1.7705
Batch 75, Loss: 1.7680
Batch 100, Loss: 1.7657
Batch 125, Loss: 1.7635
Batch 150, Loss: 1.7614
Batch 175, Loss: 1.7594
Noise applied in 1454 out of 2304 batches, 63.11
Epoch 12 learning rate: 0.01
Epoch 12 time: 353.0786802768707 seconds
Epoch 12 accuracy: 11.75%
Batch 25, Loss: 1.7562
Batch 50, Loss: 1.7545
Batch 75, Loss: 1.7528
Batch 100, Loss: 1.7512
Batch 125, Loss: 1.7496
Batch 150, Loss: 1.7482
Batch 175, Loss: 1.7468
Noise applied in 1646 out of 2496 batches, 65.95
Epoch 13 learning rate: 0.01
Epoch 13 time: 353.4914827346802 seconds
Epoch 13 accuracy: 11.67%
Batch 25, Loss: 1.7447
Batch 50, Loss: 1.7435
Batch 75, Loss: 1.7424
Batch 100, Loss: 1.7413
Batch 125, Loss: 1.7402
Batch 150, Loss: 1.7393
Batch 175, Loss: 1.7383
Noise applied in 1838 out of 2688 batches, 68.38
Epoch 14 learning rate: 0.01
Epoch 14 time: 352.2480709552765 seconds
Epoch 14 accuracy: 11.69%
Batch 25, Loss: 1.7368
Batch 50, Loss: 1.7360
Batch 75, Loss: 1.7352
Batch 100, Loss: 1.7344
Batch 125, Loss: 1.7336
Batch 150, Loss: 1.7329
Batch 175, Loss: 1.7322
Noise applied in 2030 out of 2880 batches, 70.49
Epoch 15 learning rate: 0.01
Epoch 15 time: 372.2084949016571 seconds
Epoch 15 accuracy: 11.72%
Batch 25, Loss: 1.7311
Batch 50, Loss: 1.7304
Batch 75, Loss: 1.7298
Batch 100, Loss: 1.7292
Batch 125, Loss: 1.7286
Batch 150, Loss: 1.7280
Batch 175, Loss: 1.7275
Noise applied in 2222 out of 3072 batches, 72.33
Epoch 16 learning rate: 0.01
Epoch 16 time: 389.6508779525757 seconds
Epoch 16 accuracy: 11.75%
Batch 25, Loss: 1.7265
Batch 50, Loss: 1.7260
Batch 75, Loss: 1.7255
Batch 100, Loss: 1.7250
Batch 125, Loss: 1.7245
Batch 150, Loss: 1.7240
Batch 175, Loss: 1.7236
Noise applied in 2414 out of 3264 batches, 73.96
Epoch 17 learning rate: 0.01
Epoch 17 time: 354.56233978271484 seconds
Epoch 17 accuracy: 11.66%
Batch 25, Loss: 1.7228
Batch 50, Loss: 1.7224
Batch 75, Loss: 1.7219
Batch 100, Loss: 1.7215
Batch 125, Loss: 1.7211
Batch 150, Loss: 1.7207
Batch 175, Loss: 1.7203
Noise applied in 2606 out of 3456 batches, 75.41
Epoch 18 learning rate: 0.01
Epoch 18 time: 354.0677242279053 seconds
Epoch 18 accuracy: 11.6%
Batch 25, Loss: 1.7197
Batch 50, Loss: 1.7193
Batch 75, Loss: 1.7189
Batch 100, Loss: 1.7186
Batch 125, Loss: 1.7182
Batch 150, Loss: 1.7179
Batch 175, Loss: 1.7176
Noise applied in 2798 out of 3648 batches, 76.70
Epoch 19 learning rate: 0.01
Epoch 19 time: 405.97219824790955 seconds
Epoch 19 accuracy: 11.57%
Batch 25, Loss: 1.7170
Batch 50, Loss: 1.7167
Batch 75, Loss: 1.7164
Batch 100, Loss: 1.7161
Batch 125, Loss: 1.7158
Batch 150, Loss: 1.7155
Batch 175, Loss: 1.7153
Noise applied in 2990 out of 3840 batches, 77.86
Epoch 20 learning rate: 0.01
Epoch 20 time: 355.73316717147827 seconds
Epoch 20 accuracy: 11.46%
rho:  0.04 , alpha:  0.3
Total training time: 6814.3359496593475 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.7009
Norm of the Gradient: 2.8202739358e-01
Smallest Hessian Eigenvalue: -0.1427
Noise Threshold: 0.4
Noise Radius: 0.01
