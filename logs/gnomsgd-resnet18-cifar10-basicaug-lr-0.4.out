The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:187: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM
Using Gradient-Norm Only Minimization (GNOM)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 10, Loss: 14118.5232
Batch 20, Loss: 34754.3540
Batch 30, Loss: 15108.3957
Batch 40, Loss: 25674.8707
Batch 50, Loss: 15468.8040
Batch 60, Loss: 8632.0886
Batch 70, Loss: 18470.4159
Batch 80, Loss: 28076.4938
Batch 90, Loss: 23191.2584
Batch 100, Loss: 11123.7556
Batch 110, Loss: 5928.9576
Batch 120, Loss: 4296.7273
Batch 130, Loss: 2905.0834
Batch 140, Loss: 2415.8706
Batch 150, Loss: 2045.0828
Batch 160, Loss: 1526.1839
Batch 170, Loss: 1410.4975
Batch 180, Loss: 863.7690
Batch 190, Loss: 1174.2193
Batch 200, Loss: 1193.5136
Batch 210, Loss: 825.3269
Batch 220, Loss: 790.1117
Batch 230, Loss: 953.7062
Batch 240, Loss: 704.2372
Batch 250, Loss: 599.9741
Batch 260, Loss: 544.8704
Batch 270, Loss: 564.0866
Batch 280, Loss: 540.6358
Batch 290, Loss: 391.0953
Batch 300, Loss: 407.0305
Batch 310, Loss: 350.1577
Batch 320, Loss: 322.6973
Batch 330, Loss: 261.3355
Batch 340, Loss: 257.9633
Batch 350, Loss: 367.1239
Batch 360, Loss: 361.7907
Batch 370, Loss: 284.4956
Batch 380, Loss: 343.7610
Batch 390, Loss: 192.6712
Epoch 1 learning rate: 0.4
Epoch 1 time: 128.08857560157776 seconds
Epoch 1 accuracy: 10.3%
Batch 10, Loss: 141.3560
Batch 20, Loss: 133.9879
Batch 30, Loss: 117.8637
Batch 40, Loss: 91.6031
Batch 50, Loss: 97.6314
Batch 60, Loss: 81.5449
Batch 70, Loss: 89.9132
Batch 80, Loss: 99.5517
Batch 90, Loss: 79.6697
Batch 100, Loss: 66.4916
Batch 110, Loss: 60.7163
Batch 120, Loss: 72.0010
Batch 130, Loss: 77.5625
Batch 140, Loss: 66.6682
Batch 150, Loss: 59.4478
Batch 160, Loss: 54.0687
Batch 170, Loss: 58.1862
Batch 180, Loss: 51.5791
Batch 190, Loss: 63.1700
Batch 200, Loss: 38.9142
Batch 210, Loss: 56.8227
Batch 220, Loss: 42.2248
Batch 230, Loss: 59.0595
Batch 240, Loss: 46.4716
Batch 250, Loss: 44.6199
Batch 260, Loss: 43.6546
Batch 270, Loss: 40.4028
Batch 280, Loss: 47.8346
Batch 290, Loss: 34.9022
Batch 300, Loss: 36.1644
Batch 310, Loss: 37.7106
Batch 320, Loss: 34.8857
Batch 330, Loss: 30.3581
Batch 340, Loss: 35.9684
Batch 350, Loss: 28.7796
Batch 360, Loss: 24.7039
Batch 370, Loss: 24.9557
Batch 380, Loss: 23.7155
Batch 390, Loss: 25.7982
Epoch 2 learning rate: 0.4
Epoch 2 time: 119.41999244689941 seconds
Epoch 2 accuracy: 10.42%
Batch 10, Loss: 30.3365
Batch 20, Loss: 25.8382
Batch 30, Loss: 25.8012
Batch 40, Loss: 25.8614
Batch 50, Loss: 21.5842
Batch 60, Loss: 22.9918
Batch 70, Loss: 27.2279
Batch 80, Loss: 23.8488
Batch 90, Loss: 19.0201
Batch 100, Loss: 21.8696
Batch 110, Loss: 16.5500
Batch 120, Loss: 16.9137
Batch 130, Loss: 16.7946
Batch 140, Loss: 15.3652
Batch 150, Loss: 17.4412
Batch 160, Loss: 14.7570
Batch 170, Loss: 14.4770
Batch 180, Loss: 14.9872
Batch 190, Loss: 15.6467
Batch 200, Loss: 14.2370
Batch 210, Loss: 12.6756
Batch 220, Loss: 12.5548
Batch 230, Loss: 13.9258
Batch 240, Loss: 12.0084
Batch 250, Loss: 11.2194
Batch 260, Loss: 12.0885
Batch 270, Loss: 10.1058
Batch 280, Loss: 9.7255
Batch 290, Loss: 8.9629
Batch 300, Loss: 8.2837
Batch 310, Loss: 8.0432
Batch 320, Loss: 7.4304
Batch 330, Loss: 8.6194
Batch 340, Loss: 6.2049
Batch 350, Loss: 5.3852
Batch 360, Loss: 5.6667
Batch 370, Loss: 4.6770
Batch 380, Loss: 4.2427
Batch 390, Loss: 4.0409
Epoch 3 learning rate: 0.4
Epoch 3 time: 119.3545606136322 seconds
Epoch 3 accuracy: 13.1%
Batch 10, Loss: 3.7716
Batch 20, Loss: 4.0436
Batch 30, Loss: 4.3752
Batch 40, Loss: 3.7771
Batch 50, Loss: 3.7910
Batch 60, Loss: 3.3332
Batch 70, Loss: 4.1309
Batch 80, Loss: 3.5845
Batch 90, Loss: 3.4965
Batch 100, Loss: 3.2627
Batch 110, Loss: 3.5700
Batch 120, Loss: 2.9518
Batch 130, Loss: 3.2387
Batch 140, Loss: 3.6790
Batch 150, Loss: 3.1884
Batch 160, Loss: 2.7287
Batch 170, Loss: 3.2939
Batch 180, Loss: 2.6676
Batch 190, Loss: 2.7531
Batch 200, Loss: 2.7884
Batch 210, Loss: 2.9641
Batch 220, Loss: 2.6436
Batch 230, Loss: 2.9283
Batch 240, Loss: 2.5004
Batch 250, Loss: 2.3676
Batch 260, Loss: 2.5229
Batch 270, Loss: 2.2891
Batch 280, Loss: 2.2717
Batch 290, Loss: 2.2948
Batch 300, Loss: 2.3235
Batch 310, Loss: 2.2236
Batch 320, Loss: 2.1193
Batch 330, Loss: 2.1787
Batch 340, Loss: 2.1192
Batch 350, Loss: 2.2897
Batch 360, Loss: 2.1177
Batch 370, Loss: 2.0577
Batch 380, Loss: 2.0562
Batch 390, Loss: 2.1418
Epoch 4 learning rate: 0.4
Epoch 4 time: 119.29881477355957 seconds
Epoch 4 accuracy: 12.23%
Batch 10, Loss: 1.9847
Batch 20, Loss: 2.0425
Batch 30, Loss: 1.9019
Batch 40, Loss: 1.9353
Batch 50, Loss: 1.9910
Batch 60, Loss: 1.9395
Batch 70, Loss: 1.8916
Batch 80, Loss: 1.9409
Batch 90, Loss: 1.8722
Batch 100, Loss: 1.8665
Batch 110, Loss: 1.9365
Batch 120, Loss: 1.9156
Batch 130, Loss: 1.8881
Batch 140, Loss: 1.8906
Batch 150, Loss: 1.9312
Batch 160, Loss: 1.8643
Batch 170, Loss: 1.7968
Batch 180, Loss: 1.8125
Batch 190, Loss: 1.7977
Batch 200, Loss: 1.7930
Batch 210, Loss: 1.8079
Batch 220, Loss: 1.7893
Batch 230, Loss: 1.8248
Batch 240, Loss: 1.7591
Batch 250, Loss: 1.7705
Batch 260, Loss: 1.7631
Batch 270, Loss: 1.8040
Batch 280, Loss: 1.7770
Batch 290, Loss: 1.7805
Batch 300, Loss: 1.7517
Batch 310, Loss: 1.7531
Batch 320, Loss: 1.7645
Batch 330, Loss: 1.7605
Batch 340, Loss: 1.8056
Batch 350, Loss: 1.7781
Batch 360, Loss: 1.7638
Batch 370, Loss: 1.7700
Batch 380, Loss: 1.7569
Batch 390, Loss: 1.7498
Epoch 5 learning rate: 0.4
Epoch 5 time: 119.36446523666382 seconds
Epoch 5 accuracy: 10.81%
Batch 10, Loss: 1.7546
Batch 20, Loss: 1.7695
Batch 30, Loss: 1.7522
Batch 40, Loss: 1.7567
Batch 50, Loss: 1.7536
Batch 60, Loss: 1.7645
Batch 70, Loss: 1.7564
Batch 80, Loss: 1.7504
Batch 90, Loss: 1.7551
Batch 100, Loss: 1.7729
Batch 110, Loss: 1.7507
Batch 120, Loss: 1.7606
Batch 130, Loss: 1.7615
Batch 140, Loss: 1.7563
Batch 150, Loss: 1.7892
Batch 160, Loss: 1.7665
Batch 170, Loss: 1.7542
Batch 180, Loss: 1.7523
Batch 190, Loss: 1.7639
Batch 200, Loss: 1.7575
Batch 210, Loss: 1.7589
Batch 220, Loss: 1.7559
Batch 230, Loss: 1.7538
Batch 240, Loss: 1.7575
Batch 250, Loss: 1.7633
Batch 260, Loss: 1.7643
Batch 270, Loss: 1.7595
Batch 280, Loss: 1.7606
Batch 290, Loss: 1.7587
Batch 300, Loss: 1.7554
Batch 310, Loss: 1.7600
Batch 320, Loss: 1.7588
Batch 330, Loss: 1.7559
Batch 340, Loss: 1.7556
Batch 350, Loss: 1.7585
Batch 360, Loss: 1.7596
Batch 370, Loss: 1.7602
Batch 380, Loss: 1.7553
Batch 390, Loss: 1.7578
Epoch 6 learning rate: 0.4
Epoch 6 time: 119.21071338653564 seconds
Epoch 6 accuracy: 10.0%
Batch 10, Loss: 1.7584
Batch 20, Loss: 1.7561
Batch 30, Loss: 1.7557
Batch 40, Loss: 1.7563
Batch 50, Loss: 1.7568
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7575
Batch 80, Loss: 1.7562
Batch 90, Loss: 1.7611
Batch 100, Loss: 1.7558
Batch 110, Loss: 1.7591
Batch 120, Loss: 1.7588
Batch 130, Loss: 1.7571
Batch 140, Loss: 1.7575
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7585
Batch 170, Loss: 1.7567
Batch 180, Loss: 1.7625
Batch 190, Loss: 1.7563
Batch 200, Loss: 1.7578
Batch 210, Loss: 1.7585
Batch 220, Loss: 1.7592
Batch 230, Loss: 1.7578
Batch 240, Loss: 1.7580
Batch 250, Loss: 1.7575
Batch 260, Loss: 1.7574
Batch 270, Loss: 1.7591
Batch 280, Loss: 1.7569
Batch 290, Loss: 1.7597
Batch 300, Loss: 1.7593
Batch 310, Loss: 1.7582
Batch 320, Loss: 1.7573
Batch 330, Loss: 1.7585
Batch 340, Loss: 1.7583
Batch 350, Loss: 1.7585
Batch 360, Loss: 1.7577
Batch 370, Loss: 1.7572
Batch 380, Loss: 1.7586
Batch 390, Loss: 1.7601
Epoch 7 learning rate: 0.4
Epoch 7 time: 119.28542828559875 seconds
Epoch 7 accuracy: 10.0%
Batch 10, Loss: 1.7584
Batch 20, Loss: 1.7568
Batch 30, Loss: 1.7605
Batch 40, Loss: 1.7573
Batch 50, Loss: 1.7583
Batch 60, Loss: 1.7589
Batch 70, Loss: 1.7576
Batch 80, Loss: 1.7573
Batch 90, Loss: 1.7575
Batch 100, Loss: 1.7581
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7591
Batch 130, Loss: 1.7589
Batch 140, Loss: 1.7587
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7584
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7581
Batch 190, Loss: 1.7571
Batch 200, Loss: 1.7562
Batch 210, Loss: 1.7610
Batch 220, Loss: 1.7587
Batch 230, Loss: 1.7582
Batch 240, Loss: 1.7577
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7583
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7586
Batch 300, Loss: 1.7580
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7583
Batch 330, Loss: 1.7570
Batch 340, Loss: 1.7595
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7576
Batch 370, Loss: 1.7583
Batch 380, Loss: 1.7593
Batch 390, Loss: 1.7583
Epoch 8 learning rate: 0.4
Epoch 8 time: 119.20268726348877 seconds
Epoch 8 accuracy: 10.0%
Batch 10, Loss: 1.7580
Batch 20, Loss: 1.7585
Batch 30, Loss: 1.7580
Batch 40, Loss: 1.7592
Batch 50, Loss: 1.7576
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7582
Batch 80, Loss: 1.7581
Batch 90, Loss: 1.7577
Batch 100, Loss: 1.7584
Batch 110, Loss: 1.7580
Batch 120, Loss: 1.7590
Batch 130, Loss: 1.7589
Batch 140, Loss: 1.7570
Batch 150, Loss: 1.7585
Batch 160, Loss: 1.7590
Batch 170, Loss: 1.7582
Batch 180, Loss: 1.7583
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7582
Batch 210, Loss: 1.7586
Batch 220, Loss: 1.7589
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7577
Batch 250, Loss: 1.7563
Batch 260, Loss: 1.7572
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7601
Batch 290, Loss: 1.7587
Batch 300, Loss: 1.7583
Batch 310, Loss: 1.7587
Batch 320, Loss: 1.7583
Batch 330, Loss: 1.7576
Batch 340, Loss: 1.7581
Batch 350, Loss: 1.7578
Batch 360, Loss: 1.7581
Batch 370, Loss: 1.7580
Batch 380, Loss: 1.7576
Batch 390, Loss: 1.7574
Epoch 9 learning rate: 0.4
Epoch 9 time: 119.22978162765503 seconds
Epoch 9 accuracy: 10.0%
Batch 10, Loss: 1.7591
Batch 20, Loss: 1.7594
Batch 30, Loss: 1.7576
Batch 40, Loss: 1.7573
Batch 50, Loss: 1.7585
Batch 60, Loss: 1.7579
Batch 70, Loss: 1.7584
Batch 80, Loss: 1.7583
Batch 90, Loss: 1.7587
Batch 100, Loss: 1.7584
Batch 110, Loss: 1.7593
Batch 120, Loss: 1.7593
Batch 130, Loss: 1.7584
Batch 140, Loss: 1.7585
Batch 150, Loss: 1.7580
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7567
Batch 190, Loss: 1.7585
Batch 200, Loss: 1.7611
Batch 210, Loss: 1.7586
Batch 220, Loss: 1.7572
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7596
Batch 250, Loss: 1.7582
Batch 260, Loss: 1.7579
Batch 270, Loss: 1.7588
Batch 280, Loss: 1.7578
Batch 290, Loss: 1.7587
Batch 300, Loss: 1.7598
Batch 310, Loss: 1.7585
Batch 320, Loss: 1.7581
Batch 330, Loss: 1.7582
Batch 340, Loss: 1.7578
Batch 350, Loss: 1.7584
Batch 360, Loss: 1.7585
Batch 370, Loss: 1.7587
Batch 380, Loss: 1.7582
Batch 390, Loss: 1.7574
Epoch 10 learning rate: 0.4
Epoch 10 time: 119.09042143821716 seconds
Epoch 10 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7592
Batch 30, Loss: 1.7588
Batch 40, Loss: 1.7597
Batch 50, Loss: 1.7581
Batch 60, Loss: 1.7577
Batch 70, Loss: 1.7581
Batch 80, Loss: 1.7591
Batch 90, Loss: 1.7578
Batch 100, Loss: 1.7587
Batch 110, Loss: 1.7578
Batch 120, Loss: 1.7585
Batch 130, Loss: 1.7566
Batch 140, Loss: 1.7603
Batch 150, Loss: 1.7583
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7581
Batch 180, Loss: 1.7572
Batch 190, Loss: 1.7597
Batch 200, Loss: 1.7576
Batch 210, Loss: 1.7588
Batch 220, Loss: 1.7586
Batch 230, Loss: 1.7593
Batch 240, Loss: 1.7583
Batch 250, Loss: 1.7577
Batch 260, Loss: 1.7591
Batch 270, Loss: 1.7592
Batch 280, Loss: 1.7576
Batch 290, Loss: 1.7584
Batch 300, Loss: 1.7577
Batch 310, Loss: 1.7586
Batch 320, Loss: 1.7591
Batch 330, Loss: 1.7581
Batch 340, Loss: 1.7576
Batch 350, Loss: 1.7577
Batch 360, Loss: 1.7597
Batch 370, Loss: 1.7596
Batch 380, Loss: 1.7584
Batch 390, Loss: 1.7580
Epoch 11 learning rate: 0.4
Epoch 11 time: 119.32588982582092 seconds
Epoch 11 accuracy: 10.0%
Batch 10, Loss: 1.7581
Batch 20, Loss: 1.7574
Batch 30, Loss: 1.7584
Batch 40, Loss: 1.7595
Batch 50, Loss: 1.7583
Batch 60, Loss: 1.7580
Batch 70, Loss: 1.7561
Batch 80, Loss: 1.7597
Batch 90, Loss: 1.7587
Batch 100, Loss: 1.7596
Batch 110, Loss: 1.7586
Batch 120, Loss: 1.7576
Batch 130, Loss: 1.7597
Batch 140, Loss: 1.7581
Batch 150, Loss: 1.7565
Batch 160, Loss: 1.7611
Batch 170, Loss: 1.7593
Batch 180, Loss: 1.7581
Batch 190, Loss: 1.7578
Batch 200, Loss: 1.7582
Batch 210, Loss: 1.7581
Batch 220, Loss: 1.7586
Batch 230, Loss: 1.7579
Batch 240, Loss: 1.7595
Batch 250, Loss: 1.7582
Batch 260, Loss: 1.7583
Batch 270, Loss: 1.7586
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7585
Batch 300, Loss: 1.7585
Batch 310, Loss: 1.7575
Batch 320, Loss: 1.7593
Batch 330, Loss: 1.7569
Batch 340, Loss: 1.7594
Batch 350, Loss: 1.7586
Batch 360, Loss: 1.7586
Batch 370, Loss: 1.7585
Batch 380, Loss: 1.7587
Batch 390, Loss: 1.7581
Epoch 12 learning rate: 0.4
Epoch 12 time: 119.19031810760498 seconds
Epoch 12 accuracy: 10.0%
Batch 10, Loss: 1.7584
Batch 20, Loss: 1.7577
Batch 30, Loss: 1.7583
Batch 40, Loss: 1.7589
Batch 50, Loss: 1.7581
Batch 60, Loss: 1.7567
Batch 70, Loss: 1.7580
Batch 80, Loss: 1.7583
Batch 90, Loss: 1.7605
Batch 100, Loss: 1.7575
Batch 110, Loss: 1.7579
Batch 120, Loss: 1.7588
Batch 130, Loss: 1.7602
Batch 140, Loss: 1.7595
Batch 150, Loss: 1.7582
Batch 160, Loss: 1.7582
Batch 170, Loss: 1.7584
Batch 180, Loss: 1.7580
Batch 190, Loss: 1.7571
Batch 200, Loss: 1.7584
Batch 210, Loss: 1.7592
Batch 220, Loss: 1.7584
Batch 230, Loss: 1.7582
Batch 240, Loss: 1.7600
Batch 250, Loss: 1.7592
Batch 260, Loss: 1.7583
Batch 270, Loss: 1.7577
Batch 280, Loss: 1.7590
Batch 290, Loss: 1.7588
Batch 300, Loss: 1.7570
Batch 310, Loss: 1.7580
Batch 320, Loss: 1.7587
Batch 330, Loss: 1.7584
Batch 340, Loss: 1.7591
Batch 350, Loss: 1.7570
Batch 360, Loss: 1.7595
Batch 370, Loss: 1.7569
Batch 380, Loss: 1.7586
Batch 390, Loss: 1.7584
Epoch 13 learning rate: 0.4
Epoch 13 time: 119.16464138031006 seconds
Epoch 13 accuracy: 10.0%
Batch 10, Loss: 1.7581
Batch 20, Loss: 1.7592
Batch 30, Loss: 1.7580
Batch 40, Loss: 1.7583
Batch 50, Loss: 1.7582
Batch 60, Loss: 1.7583
Batch 70, Loss: 1.7584
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7580
Batch 100, Loss: 1.7579
Batch 110, Loss: 1.7573
Batch 120, Loss: 1.7588
Batch 130, Loss: 1.7593
Batch 140, Loss: 1.7577
Batch 150, Loss: 1.7579
Batch 160, Loss: 1.7590
Batch 170, Loss: 1.7591
Batch 180, Loss: 1.7590
Batch 190, Loss: 1.7583
Batch 200, Loss: 1.7570
Batch 210, Loss: 1.7584
Batch 220, Loss: 1.7597
Batch 230, Loss: 1.7583
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7588
Batch 260, Loss: 1.7577
Batch 270, Loss: 1.7584
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7580
Batch 300, Loss: 1.7571
Batch 310, Loss: 1.7598
Batch 320, Loss: 1.7594
Batch 330, Loss: 1.7585
Batch 340, Loss: 1.7582
Batch 350, Loss: 1.7582
Batch 360, Loss: 1.7584
Batch 370, Loss: 1.7589
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7582
Epoch 14 learning rate: 0.4
Epoch 14 time: 119.19013142585754 seconds
Epoch 14 accuracy: 10.0%
Batch 10, Loss: 1.7577
Batch 20, Loss: 1.7582
Batch 30, Loss: 1.7578
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7606
Batch 60, Loss: 1.7576
Batch 70, Loss: 1.7587
Batch 80, Loss: 1.7587
Batch 90, Loss: 1.7595
Batch 100, Loss: 1.7582
Batch 110, Loss: 1.7584
Batch 120, Loss: 1.7583
Batch 130, Loss: 1.7578
Batch 140, Loss: 1.7579
Batch 150, Loss: 1.7593
Batch 160, Loss: 1.7577
Batch 170, Loss: 1.7597
Batch 180, Loss: 1.7585
Batch 190, Loss: 1.7589
Batch 200, Loss: 1.7584
Batch 210, Loss: 1.7587
Batch 220, Loss: 1.7586
Batch 230, Loss: 1.7584
Batch 240, Loss: 1.7592
Batch 250, Loss: 1.7584
Batch 260, Loss: 1.7574
Batch 270, Loss: 1.7574
Batch 280, Loss: 1.7589
Batch 290, Loss: 1.7599
Batch 300, Loss: 1.7581
Batch 310, Loss: 1.7584
Batch 320, Loss: 1.7590
Batch 330, Loss: 1.7583
Batch 340, Loss: 1.7584
Batch 350, Loss: 1.7590
Batch 360, Loss: 1.7588
Batch 370, Loss: 1.7572
Batch 380, Loss: 1.7588
Batch 390, Loss: 1.7585
Epoch 15 learning rate: 0.4
Epoch 15 time: 119.24699282646179 seconds
Epoch 15 accuracy: 10.0%
Batch 10, Loss: 1.7579
Batch 20, Loss: 1.7582
Batch 30, Loss: 1.7563
Batch 40, Loss: 1.7594
Batch 50, Loss: 1.7584
Batch 60, Loss: 1.7575
Batch 70, Loss: 1.7588
Batch 80, Loss: 1.7590
Batch 90, Loss: 1.7582
Batch 100, Loss: 1.7576
Batch 110, Loss: 1.7584
Batch 120, Loss: 1.7576
Batch 130, Loss: 1.7576
Batch 140, Loss: 1.7582
Batch 150, Loss: 1.7592
Batch 160, Loss: 1.7593
Batch 170, Loss: 1.7579
Batch 180, Loss: 1.7572
Batch 190, Loss: 1.7591
Batch 200, Loss: 1.7597
Batch 210, Loss: 1.7575
Batch 220, Loss: 1.7590
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7581
Batch 250, Loss: 1.7587
Batch 260, Loss: 1.7583
Batch 270, Loss: 1.7574
Batch 280, Loss: 1.7592
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7585
Batch 310, Loss: 1.7586
Batch 320, Loss: 1.7587
Batch 330, Loss: 1.7601
Batch 340, Loss: 1.7586
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7588
Batch 370, Loss: 1.7580
Batch 380, Loss: 1.7588
Batch 390, Loss: 1.7592
Epoch 16 learning rate: 0.4
Epoch 16 time: 119.19061779975891 seconds
Epoch 16 accuracy: 10.0%
Batch 10, Loss: 1.7585
Batch 20, Loss: 1.7570
Batch 30, Loss: 1.7568
Batch 40, Loss: 1.7593
Batch 50, Loss: 1.7595
Batch 60, Loss: 1.7578
Batch 70, Loss: 1.7585
Batch 80, Loss: 1.7585
Batch 90, Loss: 1.7583
Batch 100, Loss: 1.7596
Batch 110, Loss: 1.7575
Batch 120, Loss: 1.7590
Batch 130, Loss: 1.7585
Batch 140, Loss: 1.7589
Batch 150, Loss: 1.7574
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7587
Batch 180, Loss: 1.7590
Batch 190, Loss: 1.7588
Batch 200, Loss: 1.7585
Batch 210, Loss: 1.7585
Batch 220, Loss: 1.7586
Batch 230, Loss: 1.7584
Batch 240, Loss: 1.7578
Batch 250, Loss: 1.7587
Batch 260, Loss: 1.7582
Batch 270, Loss: 1.7592
Batch 280, Loss: 1.7584
Batch 290, Loss: 1.7578
Batch 300, Loss: 1.7584
Batch 310, Loss: 1.7582
Batch 320, Loss: 1.7583
Batch 330, Loss: 1.7582
Batch 340, Loss: 1.7580
Batch 350, Loss: 1.7597
Batch 360, Loss: 1.7585
Batch 370, Loss: 1.7579
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7574
Epoch 17 learning rate: 0.4
Epoch 17 time: 119.25508999824524 seconds
Epoch 17 accuracy: 10.0%
Batch 10, Loss: 1.7606
Batch 20, Loss: 1.7584
Batch 30, Loss: 1.7579
Batch 40, Loss: 1.7579
Batch 50, Loss: 1.7581
Batch 60, Loss: 1.7585
Batch 70, Loss: 1.7582
Batch 80, Loss: 1.7572
Batch 90, Loss: 1.7593
Batch 100, Loss: 1.7585
Batch 110, Loss: 1.7582
Batch 120, Loss: 1.7578
Batch 130, Loss: 1.7585
Batch 140, Loss: 1.7592
Batch 150, Loss: 1.7578
Batch 160, Loss: 1.7589
Batch 170, Loss: 1.7588
Batch 180, Loss: 1.7581
Batch 190, Loss: 1.7580
Batch 200, Loss: 1.7586
Batch 210, Loss: 1.7578
Batch 220, Loss: 1.7581
Batch 230, Loss: 1.7585
Batch 240, Loss: 1.7585
Batch 250, Loss: 1.7593
Batch 260, Loss: 1.7584
Batch 270, Loss: 1.7581
Batch 280, Loss: 1.7590
Batch 290, Loss: 1.7589
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7578
Batch 320, Loss: 1.7588
Batch 330, Loss: 1.7586
Batch 340, Loss: 1.7591
Batch 350, Loss: 1.7581
Batch 360, Loss: 1.7585
Batch 370, Loss: 1.7592
Batch 380, Loss: 1.7577
Batch 390, Loss: 1.7565
Epoch 18 learning rate: 0.4
Epoch 18 time: 119.14449262619019 seconds
Epoch 18 accuracy: 10.0%
Batch 10, Loss: 1.7591
Batch 20, Loss: 1.7591
Batch 30, Loss: 1.7585
Batch 40, Loss: 1.7574
Batch 50, Loss: 1.7583
Batch 60, Loss: 1.7586
Batch 70, Loss: 1.7585
Batch 80, Loss: 1.7578
Batch 90, Loss: 1.7579
Batch 100, Loss: 1.7568
Batch 110, Loss: 1.7585
Batch 120, Loss: 1.7601
Batch 130, Loss: 1.7579
Batch 140, Loss: 1.7588
Batch 150, Loss: 1.7589
Batch 160, Loss: 1.7581
Batch 170, Loss: 1.7577
Batch 180, Loss: 1.7587
Batch 190, Loss: 1.7585
Batch 200, Loss: 1.7584
Batch 210, Loss: 1.7574
Batch 220, Loss: 1.7580
Batch 230, Loss: 1.7580
Batch 240, Loss: 1.7598
Batch 250, Loss: 1.7580
Batch 260, Loss: 1.7574
Batch 270, Loss: 1.7578
Batch 280, Loss: 1.7581
Batch 290, Loss: 1.7605
Batch 300, Loss: 1.7579
Batch 310, Loss: 1.7588
Batch 320, Loss: 1.7589
Batch 330, Loss: 1.7586
Batch 340, Loss: 1.7584
Batch 350, Loss: 1.7576
Batch 360, Loss: 1.7584
Batch 370, Loss: 1.7581
Batch 380, Loss: 1.7580
Batch 390, Loss: 1.7581
Epoch 19 learning rate: 0.4
Epoch 19 time: 119.15870714187622 seconds
Epoch 19 accuracy: 10.0%
Batch 10, Loss: 1.7578
Batch 20, Loss: 1.7584
Batch 30, Loss: 1.7583
Batch 40, Loss: 1.7586
Batch 50, Loss: 1.7575
Batch 60, Loss: 1.7599
Batch 70, Loss: 1.7583
Batch 80, Loss: 1.7591
Batch 90, Loss: 1.7585
Batch 100, Loss: 1.7577
Batch 110, Loss: 1.7586
Batch 120, Loss: 1.7577
Batch 130, Loss: 1.7584
Batch 140, Loss: 1.7590
Batch 150, Loss: 1.7588
Batch 160, Loss: 1.7591
Batch 170, Loss: 1.7574
Batch 180, Loss: 1.7585
Batch 190, Loss: 1.7579
Batch 200, Loss: 1.7591
Batch 210, Loss: 1.7586
Batch 220, Loss: 1.7583
Batch 230, Loss: 1.7577
Batch 240, Loss: 1.7597
Batch 250, Loss: 1.7576
Batch 260, Loss: 1.7576
Batch 270, Loss: 1.7584
Batch 280, Loss: 1.7579
Batch 290, Loss: 1.7590
Batch 300, Loss: 1.7573
Batch 310, Loss: 1.7583
Batch 320, Loss: 1.7575
Batch 330, Loss: 1.7598
Batch 340, Loss: 1.7573
Batch 350, Loss: 1.7586
Batch 360, Loss: 1.7588
Batch 370, Loss: 1.7592
Batch 380, Loss: 1.7585
Batch 390, Loss: 1.7579
Epoch 20 learning rate: 0.4
Epoch 20 time: 119.09657001495361 seconds
Epoch 20 accuracy: 10.0%
Total training time: 2393.5393919944763 seconds
