The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-08-18-16:55:59
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 80.1051
Noise applied in 0 out of 196 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 129.73062920570374 seconds
Epoch 1 accuracy: 10.01%
Batch 100, Loss: 6.3211
Noise applied in 0 out of 392 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 111.69548988342285 seconds
Epoch 2 accuracy: 10.38%
Batch 100, Loss: 5.0187
Noise applied in 0 out of 588 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 111.69638419151306 seconds
Epoch 3 accuracy: 10.28%
Batch 100, Loss: 4.4626
Noise applied in 0 out of 784 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 111.62133383750916 seconds
Epoch 4 accuracy: 10.37%
Batch 100, Loss: 4.1105
Noise applied in 0 out of 980 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 111.66263103485107 seconds
Epoch 5 accuracy: 10.29%
Batch 100, Loss: 3.8393
Noise applied in 0 out of 1176 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 111.54355812072754 seconds
Epoch 6 accuracy: 10.3%
Batch 100, Loss: 3.6024
Noise applied in 0 out of 1372 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 111.52815365791321 seconds
Epoch 7 accuracy: 10.41%
Batch 100, Loss: 3.3578
Noise applied in 0 out of 1568 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 111.6354877948761 seconds
Epoch 8 accuracy: 10.31%
Batch 100, Loss: 3.1226
Noise applied in 0 out of 1764 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 111.6529815196991 seconds
Epoch 9 accuracy: 10.34%
Batch 100, Loss: 2.8598
Noise applied in 5 out of 1960 batches, 0.26
Epoch 10 learning rate: 0.01
Epoch 10 time: 114.77596592903137 seconds
Epoch 10 accuracy: 10.32%
Batch 100, Loss: 2.4861
Noise applied in 132 out of 2156 batches, 6.12
Epoch 11 learning rate: 0.01
Epoch 11 time: 184.66696548461914 seconds
Epoch 11 accuracy: 10.25%
Batch 100, Loss: 2.1179
Noise applied in 324 out of 2352 batches, 13.78
Epoch 12 learning rate: 0.01
Epoch 12 time: 219.26633262634277 seconds
Epoch 12 accuracy: 11.69%
Batch 100, Loss: 2.0004
Noise applied in 517 out of 2548 batches, 20.29
Epoch 13 learning rate: 0.01
Epoch 13 time: 219.67888760566711 seconds
Epoch 13 accuracy: 12.56%
Batch 100, Loss: 1.9395
Noise applied in 712 out of 2744 batches, 25.95
Epoch 14 learning rate: 0.01
Epoch 14 time: 220.89566326141357 seconds
Epoch 14 accuracy: 13.78%
Batch 100, Loss: 1.9006
Noise applied in 906 out of 2940 batches, 30.82
Epoch 15 learning rate: 0.01
Epoch 15 time: 220.14010500907898 seconds
Epoch 15 accuracy: 13.99%
Batch 100, Loss: 1.8879
Noise applied in 1100 out of 3136 batches, 35.08
Epoch 16 learning rate: 0.01
Epoch 16 time: 220.3749167919159 seconds
Epoch 16 accuracy: 14.19%
Batch 100, Loss: 1.8588
Noise applied in 1294 out of 3332 batches, 38.84
Epoch 17 learning rate: 0.01
Epoch 17 time: 220.16687631607056 seconds
Epoch 17 accuracy: 14.52%
Batch 100, Loss: 1.8421
Noise applied in 1490 out of 3528 batches, 42.23
Epoch 18 learning rate: 0.01
Epoch 18 time: 221.1257050037384 seconds
Epoch 18 accuracy: 14.71%
Batch 100, Loss: 1.8333
Noise applied in 1685 out of 3724 batches, 45.25
Epoch 19 learning rate: 0.01
Epoch 19 time: 220.6973102092743 seconds
Epoch 19 accuracy: 15.03%
Batch 100, Loss: 1.8188
Noise applied in 1880 out of 3920 batches, 47.96
Epoch 20 learning rate: 0.01
Epoch 20 time: 221.23662209510803 seconds
Epoch 20 accuracy: 14.93%
rho:  0.04 , alpha:  0.3
Total training time: 3305.808072090149 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.8545
Norm of the Gradient: 1.6891315579e-01
Smallest Hessian Eigenvalue: -0.0385
Noise Threshold: 0.5
Noise Radius: 0.1
