The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-18:54:38
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 124.1667
Batch 50, Loss: 68.5263
Batch 75, Loss: 40.8270
Batch 100, Loss: 30.6877
Batch 125, Loss: 24.7603
Batch 150, Loss: 21.0880
Batch 175, Loss: 18.5286
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 292.14615297317505 seconds
Epoch 1 accuracy: 11.37%
Batch 25, Loss: 15.3522
Batch 50, Loss: 13.8789
Batch 75, Loss: 12.6766
Batch 100, Loss: 11.6744
Batch 125, Loss: 10.8281
Batch 150, Loss: 10.1134
Batch 175, Loss: 9.5017
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 280.419606924057 seconds
Epoch 2 accuracy: 10.97%
Batch 25, Loss: 8.6365
Batch 50, Loss: 8.1904
Batch 75, Loss: 7.7849
Batch 100, Loss: 7.4148
Batch 125, Loss: 7.0748
Batch 150, Loss: 6.7603
Batch 175, Loss: 6.4646
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 296.808082818985 seconds
Epoch 3 accuracy: 10.87%
Batch 25, Loss: 6.0034
Batch 50, Loss: 5.7515
Batch 75, Loss: 5.5188
Batch 100, Loss: 5.3074
Batch 125, Loss: 5.1196
Batch 150, Loss: 4.9548
Batch 175, Loss: 4.8081
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 316.20170736312866 seconds
Epoch 4 accuracy: 9.95%
Batch 25, Loss: 4.5936
Batch 50, Loss: 4.4527
Batch 75, Loss: 4.3376
Batch 100, Loss: 4.2372
Batch 125, Loss: 4.1449
Batch 150, Loss: 4.0539
Batch 175, Loss: 3.9630
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 281.6617076396942 seconds
Epoch 5 accuracy: 10.33%
Batch 25, Loss: 3.8309
Batch 50, Loss: 3.7593
Batch 75, Loss: 3.6926
Batch 100, Loss: 3.6301
Batch 125, Loss: 3.5724
Batch 150, Loss: 3.5187
Batch 175, Loss: 3.4686
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 280.0142328739166 seconds
Epoch 6 accuracy: 9.95%
Batch 25, Loss: 3.3919
Batch 50, Loss: 3.3498
Batch 75, Loss: 3.3100
Batch 100, Loss: 3.2727
Batch 125, Loss: 3.2375
Batch 150, Loss: 3.2042
Batch 175, Loss: 3.1729
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 281.2988920211792 seconds
Epoch 7 accuracy: 9.87%
Batch 25, Loss: 3.1246
Batch 50, Loss: 3.0984
Batch 75, Loss: 3.0742
Batch 100, Loss: 3.0515
Batch 125, Loss: 3.0298
Batch 150, Loss: 3.0085
Batch 175, Loss: 2.9875
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 285.6171221733093 seconds
Epoch 8 accuracy: 10.26%
Batch 25, Loss: 2.9531
Batch 50, Loss: 2.9339
Batch 75, Loss: 2.9158
Batch 100, Loss: 2.8985
Batch 125, Loss: 2.8819
Batch 150, Loss: 2.8660
Batch 175, Loss: 2.8506
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 282.9063060283661 seconds
Epoch 9 accuracy: 10.57%
Batch 25, Loss: 2.8261
Batch 50, Loss: 2.8122
Batch 75, Loss: 2.7989
Batch 100, Loss: 2.7859
Batch 125, Loss: 2.7733
Batch 150, Loss: 2.7612
Batch 175, Loss: 2.7493
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 288.09263134002686 seconds
Epoch 10 accuracy: 10.53%
Batch 25, Loss: 2.7303
Batch 50, Loss: 2.7195
Batch 75, Loss: 2.7090
Batch 100, Loss: 2.6988
Batch 125, Loss: 2.6889
Batch 150, Loss: 2.6793
Batch 175, Loss: 2.6700
Noise applied in 0 out of 192 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 280.30479288101196 seconds
Epoch 11 accuracy: 10.64%
Batch 25, Loss: 2.6551
Batch 50, Loss: 2.6466
Batch 75, Loss: 2.6383
Batch 100, Loss: 2.6302
Batch 125, Loss: 2.6223
Batch 150, Loss: 2.6146
Batch 175, Loss: 2.6070
Noise applied in 0 out of 192 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 282.1108994483948 seconds
Epoch 12 accuracy: 10.78%
Batch 25, Loss: 2.5947
Batch 50, Loss: 2.5875
Batch 75, Loss: 2.5804
Batch 100, Loss: 2.5736
Batch 125, Loss: 2.5668
Batch 150, Loss: 2.5602
Batch 175, Loss: 2.5535
Noise applied in 0 out of 192 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 285.62185525894165 seconds
Epoch 13 accuracy: 10.74%
Batch 25, Loss: 2.5423
Batch 50, Loss: 2.5359
Batch 75, Loss: 2.5296
Batch 100, Loss: 2.5235
Batch 125, Loss: 2.5175
Batch 150, Loss: 2.5116
Batch 175, Loss: 2.5058
Noise applied in 0 out of 192 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 287.27782440185547 seconds
Epoch 14 accuracy: 10.71%
Batch 25, Loss: 2.4965
Batch 50, Loss: 2.4912
Batch 75, Loss: 2.4860
Batch 100, Loss: 2.4809
Batch 125, Loss: 2.4759
Batch 150, Loss: 2.4710
Batch 175, Loss: 2.4661
Noise applied in 0 out of 192 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 287.8064682483673 seconds
Epoch 15 accuracy: 10.62%
Batch 25, Loss: 2.4581
Batch 50, Loss: 2.4534
Batch 75, Loss: 2.4488
Batch 100, Loss: 2.4442
Batch 125, Loss: 2.4397
Batch 150, Loss: 2.4353
Batch 175, Loss: 2.4309
Noise applied in 0 out of 192 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 339.4349293708801 seconds
Epoch 16 accuracy: 10.61%
Batch 25, Loss: 2.4236
Batch 50, Loss: 2.4193
Batch 75, Loss: 2.4150
Batch 100, Loss: 2.4108
Batch 125, Loss: 2.4067
Batch 150, Loss: 2.4026
Batch 175, Loss: 2.3985
Noise applied in 0 out of 192 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 287.42274236679077 seconds
Epoch 17 accuracy: 10.6%
Batch 25, Loss: 2.3918
Batch 50, Loss: 2.3878
Batch 75, Loss: 2.3839
Batch 100, Loss: 2.3800
Batch 125, Loss: 2.3762
Batch 150, Loss: 2.3724
Batch 175, Loss: 2.3686
Noise applied in 0 out of 192 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 294.0975890159607 seconds
Epoch 18 accuracy: 10.62%
Batch 25, Loss: 2.3623
Batch 50, Loss: 2.3586
Batch 75, Loss: 2.3548
Batch 100, Loss: 2.3510
Batch 125, Loss: 2.3472
Batch 150, Loss: 2.3434
Batch 175, Loss: 2.3395
Noise applied in 0 out of 192 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 317.56213569641113 seconds
Epoch 19 accuracy: 10.58%
Batch 25, Loss: 2.3327
Batch 50, Loss: 2.3285
Batch 75, Loss: 2.3242
Batch 100, Loss: 2.3194
Batch 125, Loss: 2.3139
Batch 150, Loss: 2.3078
Batch 175, Loss: 2.3020
Noise applied in 0 out of 192 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 309.4173152446747 seconds
Epoch 20 accuracy: 10.17%
rho:  0.04 , alpha:  0.3
Total training time: 5856.251065015793 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 4.0121
Norm of the Gradient: 4.8349034786e-01
Smallest Hessian Eigenvalue: -0.1738
Noise Threshold: 0.2
Noise Radius: 0.25
