The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:04:50
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 52.5576
Batch 50, Loss: 40.6760
Batch 75, Loss: 27.9380
Batch 100, Loss: 21.6825
Batch 125, Loss: 18.4237
Batch 150, Loss: 16.0925
Batch 175, Loss: 14.5860
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 294.5223271846771 seconds
Epoch 1 accuracy: 10.79%
Batch 25, Loss: 12.7792
Batch 50, Loss: 11.8962
Batch 75, Loss: 10.9834
Batch 100, Loss: 10.2900
Batch 125, Loss: 9.6202
Batch 150, Loss: 9.0116
Batch 175, Loss: 8.5398
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 261.9672222137451 seconds
Epoch 2 accuracy: 10.78%
Batch 25, Loss: 7.8908
Batch 50, Loss: 7.5088
Batch 75, Loss: 7.1925
Batch 100, Loss: 6.9124
Batch 125, Loss: 6.6559
Batch 150, Loss: 6.4197
Batch 175, Loss: 6.2022
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 261.86368775367737 seconds
Epoch 3 accuracy: 10.65%
Batch 25, Loss: 5.8721
Batch 50, Loss: 5.6944
Batch 75, Loss: 5.5294
Batch 100, Loss: 5.3759
Batch 125, Loss: 5.2327
Batch 150, Loss: 5.0985
Batch 175, Loss: 4.9729
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 262.2313027381897 seconds
Epoch 4 accuracy: 10.53%
Batch 25, Loss: 4.7799
Batch 50, Loss: 4.6748
Batch 75, Loss: 4.5763
Batch 100, Loss: 4.4837
Batch 125, Loss: 4.3963
Batch 150, Loss: 4.3138
Batch 175, Loss: 4.2357
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 262.48775267601013 seconds
Epoch 5 accuracy: 10.53%
Batch 25, Loss: 4.1136
Batch 50, Loss: 4.0464
Batch 75, Loss: 3.9828
Batch 100, Loss: 3.9226
Batch 125, Loss: 3.8656
Batch 150, Loss: 3.8113
Batch 175, Loss: 3.7596
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 262.5782980918884 seconds
Epoch 6 accuracy: 10.47%
Batch 25, Loss: 3.6779
Batch 50, Loss: 3.6323
Batch 75, Loss: 3.5887
Batch 100, Loss: 3.5471
Batch 125, Loss: 3.5072
Batch 150, Loss: 3.4691
Batch 175, Loss: 3.4327
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 262.38505601882935 seconds
Epoch 7 accuracy: 10.08%
Batch 25, Loss: 3.3751
Batch 50, Loss: 3.3427
Batch 75, Loss: 3.3115
Batch 100, Loss: 3.2811
Batch 125, Loss: 3.2497
Batch 150, Loss: 3.2128
Batch 175, Loss: 3.1967
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 262.4213511943817 seconds
Epoch 8 accuracy: 9.99%
Batch 25, Loss: 3.1695
Batch 50, Loss: 3.1540
Batch 75, Loss: 3.1390
Batch 100, Loss: 3.1246
Batch 125, Loss: 3.1106
Batch 150, Loss: 3.0971
Batch 175, Loss: 3.0839
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 266.20945477485657 seconds
Epoch 9 accuracy: 9.99%
Batch 25, Loss: 3.0624
Batch 50, Loss: 3.0500
Batch 75, Loss: 3.0378
Batch 100, Loss: 3.0259
Batch 125, Loss: 3.0140
Batch 150, Loss: 3.0024
Batch 175, Loss: 2.9908
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 263.6589095592499 seconds
Epoch 10 accuracy: 9.89%
Batch 25, Loss: 2.9712
Batch 50, Loss: 2.9599
Batch 75, Loss: 2.9490
Batch 100, Loss: 2.9382
Batch 125, Loss: 2.9276
Batch 150, Loss: 2.9171
Batch 175, Loss: 2.9068
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 262.95703196525574 seconds
Epoch 11 accuracy: 9.81%
Batch 25, Loss: 2.8898
Batch 50, Loss: 2.8798
Batch 75, Loss: 2.8699
Batch 100, Loss: 2.8602
Batch 125, Loss: 2.8506
Batch 150, Loss: 2.8411
Batch 175, Loss: 2.8317
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 263.9350731372833 seconds
Epoch 12 accuracy: 9.82%
Batch 25, Loss: 2.8162
Batch 50, Loss: 2.8072
Batch 75, Loss: 2.7984
Batch 100, Loss: 2.7896
Batch 125, Loss: 2.7810
Batch 150, Loss: 2.7725
Batch 175, Loss: 2.7640
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 263.43240237236023 seconds
Epoch 13 accuracy: 9.82%
Batch 25, Loss: 2.7501
Batch 50, Loss: 2.7419
Batch 75, Loss: 2.7338
Batch 100, Loss: 2.7258
Batch 125, Loss: 2.7179
Batch 150, Loss: 2.7101
Batch 175, Loss: 2.7024
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 263.0822172164917 seconds
Epoch 14 accuracy: 9.7%
Batch 25, Loss: 2.6897
Batch 50, Loss: 2.6822
Batch 75, Loss: 2.6748
Batch 100, Loss: 2.6675
Batch 125, Loss: 2.6602
Batch 150, Loss: 2.6530
Batch 175, Loss: 2.6459
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 263.2395284175873 seconds
Epoch 15 accuracy: 9.66%
Batch 25, Loss: 2.6341
Batch 50, Loss: 2.6271
Batch 75, Loss: 2.6202
Batch 100, Loss: 2.6134
Batch 125, Loss: 2.6066
Batch 150, Loss: 2.5999
Batch 175, Loss: 2.5933
Noise applied in 0 out of 3072 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 263.47898173332214 seconds
Epoch 16 accuracy: 9.59%
Batch 25, Loss: 2.5822
Batch 50, Loss: 2.5756
Batch 75, Loss: 2.5692
Batch 100, Loss: 2.5627
Batch 125, Loss: 2.5563
Batch 150, Loss: 2.5500
Batch 175, Loss: 2.5437
Noise applied in 0 out of 3264 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 263.0088484287262 seconds
Epoch 17 accuracy: 9.57%
Batch 25, Loss: 2.5333
Batch 50, Loss: 2.5271
Batch 75, Loss: 2.5210
Batch 100, Loss: 2.5148
Batch 125, Loss: 2.5086
Batch 150, Loss: 2.5024
Batch 175, Loss: 2.4960
Noise applied in 0 out of 3456 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 276.43514013290405 seconds
Epoch 18 accuracy: 9.8%
Batch 25, Loss: 2.4840
Batch 50, Loss: 2.4740
Batch 75, Loss: 2.4553
Batch 100, Loss: 2.4417
Batch 125, Loss: 2.4327
Batch 150, Loss: 2.4249
Batch 175, Loss: 2.4177
Noise applied in 0 out of 3648 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 309.10169982910156 seconds
Epoch 19 accuracy: 12.67%
Batch 25, Loss: 2.4065
Batch 50, Loss: 2.4003
Batch 75, Loss: 2.3943
Batch 100, Loss: 2.3886
Batch 125, Loss: 2.3829
Batch 150, Loss: 2.3773
Batch 175, Loss: 2.3718
Noise applied in 0 out of 3840 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 263.1725058555603 seconds
Epoch 20 accuracy: 13.62%
rho:  0.04 , alpha:  0.3
Total training time: 5352.184588670731 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 4.8828
Norm of the Gradient: 6.0814642906e-01
Smallest Hessian Eigenvalue: -0.1408
Noise Threshold: 0.2
Noise Radius: 0.25
