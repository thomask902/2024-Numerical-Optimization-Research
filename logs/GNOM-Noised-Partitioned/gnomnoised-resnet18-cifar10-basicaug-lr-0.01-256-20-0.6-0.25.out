The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-19:16:13
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 94.1561
Batch 50, Loss: 49.6229
Batch 75, Loss: 30.1695
Batch 100, Loss: 24.4090
Batch 125, Loss: 21.2280
Batch 150, Loss: 18.8766
Batch 175, Loss: 16.9991
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 296.586314201355 seconds
Epoch 1 accuracy: 9.77%
Batch 25, Loss: 14.6567
Batch 50, Loss: 13.6345
Batch 75, Loss: 12.7890
Batch 100, Loss: 12.1176
Batch 125, Loss: 11.5416
Batch 150, Loss: 11.0364
Batch 175, Loss: 10.5802
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 269.1126205921173 seconds
Epoch 2 accuracy: 11.99%
Batch 25, Loss: 9.8836
Batch 50, Loss: 9.5055
Batch 75, Loss: 9.1520
Batch 100, Loss: 8.8207
Batch 125, Loss: 8.5100
Batch 150, Loss: 8.2162
Batch 175, Loss: 7.9393
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 269.3708667755127 seconds
Epoch 3 accuracy: 10.12%
Batch 25, Loss: 7.5098
Batch 50, Loss: 7.2714
Batch 75, Loss: 7.0450
Batch 100, Loss: 6.8316
Batch 125, Loss: 6.6306
Batch 150, Loss: 6.4417
Batch 175, Loss: 6.2623
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 273.4040250778198 seconds
Epoch 4 accuracy: 10.05%
Batch 25, Loss: 5.9806
Batch 50, Loss: 5.8257
Batch 75, Loss: 5.6807
Batch 100, Loss: 5.5445
Batch 125, Loss: 5.4173
Batch 150, Loss: 5.2979
Batch 175, Loss: 5.1849
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 279.46304845809937 seconds
Epoch 5 accuracy: 10.14%
Batch 25, Loss: 5.0076
Batch 50, Loss: 4.9105
Batch 75, Loss: 4.8197
Batch 100, Loss: 4.7338
Batch 125, Loss: 4.6524
Batch 150, Loss: 4.5757
Batch 175, Loss: 4.5033
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 269.5972464084625 seconds
Epoch 6 accuracy: 10.1%
Batch 25, Loss: 4.3910
Batch 50, Loss: 4.3295
Batch 75, Loss: 4.2717
Batch 100, Loss: 4.2172
Batch 125, Loss: 4.1655
Batch 150, Loss: 4.1163
Batch 175, Loss: 4.0698
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 269.3820867538452 seconds
Epoch 7 accuracy: 10.11%
Batch 25, Loss: 3.9969
Batch 50, Loss: 3.9562
Batch 75, Loss: 3.9171
Batch 100, Loss: 3.8798
Batch 125, Loss: 3.8439
Batch 150, Loss: 3.8095
Batch 175, Loss: 3.7764
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 270.9430260658264 seconds
Epoch 8 accuracy: 10.22%
Batch 25, Loss: 3.7242
Batch 50, Loss: 3.6948
Batch 75, Loss: 3.6666
Batch 100, Loss: 3.6395
Batch 125, Loss: 3.6134
Batch 150, Loss: 3.5883
Batch 175, Loss: 3.5642
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 272.1652367115021 seconds
Epoch 9 accuracy: 10.19%
Batch 25, Loss: 3.5257
Batch 50, Loss: 3.5038
Batch 75, Loss: 3.4826
Batch 100, Loss: 3.4621
Batch 125, Loss: 3.4419
Batch 150, Loss: 3.4222
Batch 175, Loss: 3.4031
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 276.69663429260254 seconds
Epoch 10 accuracy: 10.27%
Batch 25, Loss: 3.3721
Batch 50, Loss: 3.3542
Batch 75, Loss: 3.3368
Batch 100, Loss: 3.3198
Batch 125, Loss: 3.3028
Batch 150, Loss: 3.2861
Batch 175, Loss: 3.2699
Noise applied in 89 out of 192 batches, 46.35
Epoch 11 learning rate: 0.01
Epoch 11 time: 330.5750663280487 seconds
Epoch 11 accuracy: 10.32%
Batch 25, Loss: 3.2436
Batch 50, Loss: 3.2287
Batch 75, Loss: 3.2139
Batch 100, Loss: 3.1992
Batch 125, Loss: 3.1847
Batch 150, Loss: 3.1704
Batch 175, Loss: 3.1565
Noise applied in 192 out of 192 batches, 100.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 436.1875078678131 seconds
Epoch 12 accuracy: 10.36%
Batch 25, Loss: 3.1339
Batch 50, Loss: 3.1206
Batch 75, Loss: 3.1076
Batch 100, Loss: 3.0947
Batch 125, Loss: 3.0823
Batch 150, Loss: 3.0698
Batch 175, Loss: 3.0578
Noise applied in 192 out of 192 batches, 100.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 400.3008842468262 seconds
Epoch 13 accuracy: 10.43%
Batch 25, Loss: 3.0377
Batch 50, Loss: 3.0258
Batch 75, Loss: 3.0140
Batch 100, Loss: 3.0027
Batch 125, Loss: 2.9916
Batch 150, Loss: 2.9803
Batch 175, Loss: 2.9694
Noise applied in 192 out of 192 batches, 100.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 415.072124004364 seconds
Epoch 14 accuracy: 10.44%
Batch 25, Loss: 2.9517
Batch 50, Loss: 2.9412
Batch 75, Loss: 2.9305
Batch 100, Loss: 2.9202
Batch 125, Loss: 2.9095
Batch 150, Loss: 2.8993
Batch 175, Loss: 2.8893
Noise applied in 192 out of 192 batches, 100.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 410.4444041252136 seconds
Epoch 15 accuracy: 10.42%
Batch 25, Loss: 2.8729
Batch 50, Loss: 2.8632
Batch 75, Loss: 2.8534
Batch 100, Loss: 2.8437
Batch 125, Loss: 2.8342
Batch 150, Loss: 2.8251
Batch 175, Loss: 2.8160
Noise applied in 192 out of 192 batches, 100.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 402.15382623672485 seconds
Epoch 16 accuracy: 10.44%
Batch 25, Loss: 2.8009
Batch 50, Loss: 2.7920
Batch 75, Loss: 2.7829
Batch 100, Loss: 2.7743
Batch 125, Loss: 2.7657
Batch 150, Loss: 2.7572
Batch 175, Loss: 2.7487
Noise applied in 192 out of 192 batches, 100.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 382.3836269378662 seconds
Epoch 17 accuracy: 10.47%
Batch 25, Loss: 2.7343
Batch 50, Loss: 2.7261
Batch 75, Loss: 2.7177
Batch 100, Loss: 2.7093
Batch 125, Loss: 2.7009
Batch 150, Loss: 2.6928
Batch 175, Loss: 2.6848
Noise applied in 305 out of 192 batches, 158.85
Epoch 18 learning rate: 0.01
Epoch 18 time: 433.55820322036743 seconds
Epoch 18 accuracy: 10.46%
Batch 25, Loss: 2.6717
Batch 50, Loss: 2.6641
Batch 75, Loss: 2.6564
Batch 100, Loss: 2.6488
Batch 125, Loss: 2.6412
Batch 150, Loss: 2.6336
Batch 175, Loss: 2.6264
Noise applied in 384 out of 192 batches, 200.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 511.0790045261383 seconds
Epoch 19 accuracy: 10.46%
Batch 25, Loss: 2.6144
Batch 50, Loss: 2.6071
Batch 75, Loss: 2.5998
Batch 100, Loss: 2.5928
Batch 125, Loss: 2.5858
Batch 150, Loss: 2.5791
Batch 175, Loss: 2.5721
Noise applied in 384 out of 192 batches, 200.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 478.8068685531616 seconds
Epoch 20 accuracy: 10.45%
rho:  0.04 , alpha:  0.3
Total training time: 6947.298618555069 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 4.6074
Norm of the Gradient: 1.8644856215e+00
Smallest Hessian Eigenvalue: -0.6558
Noise Threshold: 0.6
Noise Radius: 0.25
