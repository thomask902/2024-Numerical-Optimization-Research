The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-19:25:53
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 187.3456
Batch 50, Loss: 72.3349
Batch 75, Loss: 32.9223
Batch 100, Loss: 23.6454
Batch 125, Loss: 18.8929
Batch 150, Loss: 15.8559
Batch 175, Loss: 13.5917
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 280.95998072624207 seconds
Epoch 1 accuracy: 10.25%
Batch 25, Loss: 10.7573
Batch 50, Loss: 9.4948
Batch 75, Loss: 8.4786
Batch 100, Loss: 7.6655
Batch 125, Loss: 7.0148
Batch 150, Loss: 6.4919
Batch 175, Loss: 6.0689
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 272.0847885608673 seconds
Epoch 2 accuracy: 10.43%
Batch 25, Loss: 5.5242
Batch 50, Loss: 5.2741
Batch 75, Loss: 5.0658
Batch 100, Loss: 4.8905
Batch 125, Loss: 4.7396
Batch 150, Loss: 4.6070
Batch 175, Loss: 4.4887
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 281.906498670578 seconds
Epoch 3 accuracy: 10.56%
Batch 25, Loss: 4.3142
Batch 50, Loss: 4.2206
Batch 75, Loss: 4.1331
Batch 100, Loss: 4.0505
Batch 125, Loss: 3.9716
Batch 150, Loss: 3.8954
Batch 175, Loss: 3.8216
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 271.5630166530609 seconds
Epoch 4 accuracy: 10.57%
Batch 25, Loss: 3.7004
Batch 50, Loss: 3.6289
Batch 75, Loss: 3.5576
Batch 100, Loss: 3.4863
Batch 125, Loss: 3.4150
Batch 150, Loss: 3.3437
Batch 175, Loss: 3.2725
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 269.5919620990753 seconds
Epoch 5 accuracy: 10.57%
Batch 25, Loss: 3.1554
Batch 50, Loss: 3.0883
Batch 75, Loss: 3.0250
Batch 100, Loss: 2.9674
Batch 125, Loss: 2.9163
Batch 150, Loss: 2.8722
Batch 175, Loss: 2.8357
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 272.30453181266785 seconds
Epoch 6 accuracy: 10.45%
Batch 25, Loss: 2.7881
Batch 50, Loss: 2.7655
Batch 75, Loss: 2.7451
Batch 100, Loss: 2.7265
Batch 125, Loss: 2.7094
Batch 150, Loss: 2.6932
Batch 175, Loss: 2.6778
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 272.635617017746 seconds
Epoch 7 accuracy: 11.67%
Batch 25, Loss: 2.6532
Batch 50, Loss: 2.6392
Batch 75, Loss: 2.6255
Batch 100, Loss: 2.6119
Batch 125, Loss: 2.5984
Batch 150, Loss: 2.5854
Batch 175, Loss: 2.5732
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 279.4565577507019 seconds
Epoch 8 accuracy: 12.14%
Batch 25, Loss: 2.5538
Batch 50, Loss: 2.5428
Batch 75, Loss: 2.5320
Batch 100, Loss: 2.5215
Batch 125, Loss: 2.5111
Batch 150, Loss: 2.5009
Batch 175, Loss: 2.4909
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 279.39104771614075 seconds
Epoch 9 accuracy: 12.58%
Batch 25, Loss: 2.4749
Batch 50, Loss: 2.4658
Batch 75, Loss: 2.4567
Batch 100, Loss: 2.4479
Batch 125, Loss: 2.4397
Batch 150, Loss: 2.4321
Batch 175, Loss: 2.4246
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 328.1108572483063 seconds
Epoch 10 accuracy: 12.71%
Batch 25, Loss: 2.4120
Batch 50, Loss: 2.4046
Batch 75, Loss: 2.3972
Batch 100, Loss: 2.3902
Batch 125, Loss: 2.3835
Batch 150, Loss: 2.3770
Batch 175, Loss: 2.3707
Noise applied in 0 out of 192 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 277.3611030578613 seconds
Epoch 11 accuracy: 12.48%
Batch 25, Loss: 2.3607
Batch 50, Loss: 2.3549
Batch 75, Loss: 2.3494
Batch 100, Loss: 2.3442
Batch 125, Loss: 2.3391
Batch 150, Loss: 2.3343
Batch 175, Loss: 2.3296
Noise applied in 0 out of 192 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 288.769168138504 seconds
Epoch 12 accuracy: 12.38%
Batch 25, Loss: 2.3220
Batch 50, Loss: 2.3177
Batch 75, Loss: 2.3134
Batch 100, Loss: 2.3091
Batch 125, Loss: 2.3050
Batch 150, Loss: 2.3009
Batch 175, Loss: 2.2968
Noise applied in 0 out of 192 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 298.45862913131714 seconds
Epoch 13 accuracy: 12.41%
Batch 25, Loss: 2.2901
Batch 50, Loss: 2.2861
Batch 75, Loss: 2.2822
Batch 100, Loss: 2.2784
Batch 125, Loss: 2.2746
Batch 150, Loss: 2.2710
Batch 175, Loss: 2.2673
Noise applied in 0 out of 192 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 297.48984241485596 seconds
Epoch 14 accuracy: 12.61%
Batch 25, Loss: 2.2614
Batch 50, Loss: 2.2579
Batch 75, Loss: 2.2544
Batch 100, Loss: 2.2510
Batch 125, Loss: 2.2475
Batch 150, Loss: 2.2441
Batch 175, Loss: 2.2408
Noise applied in 0 out of 192 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 268.49234986305237 seconds
Epoch 15 accuracy: 12.62%
Batch 25, Loss: 2.2352
Batch 50, Loss: 2.2320
Batch 75, Loss: 2.2287
Batch 100, Loss: 2.2256
Batch 125, Loss: 2.2225
Batch 150, Loss: 2.2194
Batch 175, Loss: 2.2164
Noise applied in 0 out of 192 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 275.60856461524963 seconds
Epoch 16 accuracy: 12.63%
Batch 25, Loss: 2.2115
Batch 50, Loss: 2.2086
Batch 75, Loss: 2.2057
Batch 100, Loss: 2.2029
Batch 125, Loss: 2.2000
Batch 150, Loss: 2.1972
Batch 175, Loss: 2.1945
Noise applied in 0 out of 192 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 288.2909288406372 seconds
Epoch 17 accuracy: 12.73%
Batch 25, Loss: 2.1899
Batch 50, Loss: 2.1872
Batch 75, Loss: 2.1846
Batch 100, Loss: 2.1820
Batch 125, Loss: 2.1794
Batch 150, Loss: 2.1769
Batch 175, Loss: 2.1743
Noise applied in 0 out of 192 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 270.50537371635437 seconds
Epoch 18 accuracy: 12.83%
Batch 25, Loss: 2.1702
Batch 50, Loss: 2.1677
Batch 75, Loss: 2.1653
Batch 100, Loss: 2.1629
Batch 125, Loss: 2.1605
Batch 150, Loss: 2.1581
Batch 175, Loss: 2.1558
Noise applied in 0 out of 192 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 271.3149309158325 seconds
Epoch 19 accuracy: 12.84%
Batch 25, Loss: 2.1519
Batch 50, Loss: 2.1497
Batch 75, Loss: 2.1474
Batch 100, Loss: 2.1452
Batch 125, Loss: 2.1430
Batch 150, Loss: 2.1408
Batch 175, Loss: 2.1387
Noise applied in 0 out of 192 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 292.95642471313477 seconds
Epoch 20 accuracy: 12.98%
rho:  0.04 , alpha:  0.3
Total training time: 5637.269014120102 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 4.2431
Norm of the Gradient: 2.5133940578e-01
Smallest Hessian Eigenvalue: -0.0830
Noise Threshold: 0.2
Noise Radius: 0.5
