The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:04:51
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 412.4919
Batch 50, Loss: 229.1593
Batch 75, Loss: 81.1750
Batch 100, Loss: 41.5231
Batch 125, Loss: 28.3232
Batch 150, Loss: 22.4642
Batch 175, Loss: 18.9429
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 295.89855194091797 seconds
Epoch 1 accuracy: 11.28%
Batch 25, Loss: 15.3801
Batch 50, Loss: 13.9941
Batch 75, Loss: 12.9251
Batch 100, Loss: 12.0697
Batch 125, Loss: 11.3650
Batch 150, Loss: 10.7759
Batch 175, Loss: 10.2710
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 263.36341857910156 seconds
Epoch 2 accuracy: 11.22%
Batch 25, Loss: 9.5757
Batch 50, Loss: 9.2361
Batch 75, Loss: 8.9426
Batch 100, Loss: 8.6935
Batch 125, Loss: 8.4783
Batch 150, Loss: 8.2893
Batch 175, Loss: 8.1213
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 263.16302251815796 seconds
Epoch 3 accuracy: 11.1%
Batch 25, Loss: 7.8740
Batch 50, Loss: 7.7440
Batch 75, Loss: 7.6244
Batch 100, Loss: 7.5137
Batch 125, Loss: 7.4113
Batch 150, Loss: 7.3147
Batch 175, Loss: 7.2235
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 263.44812178611755 seconds
Epoch 4 accuracy: 10.98%
Batch 25, Loss: 7.0797
Batch 50, Loss: 6.9997
Batch 75, Loss: 6.9238
Batch 100, Loss: 6.8516
Batch 125, Loss: 6.7826
Batch 150, Loss: 6.7167
Batch 175, Loss: 6.6532
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 263.418251991272 seconds
Epoch 5 accuracy: 10.85%
Batch 25, Loss: 6.5514
Batch 50, Loss: 6.4935
Batch 75, Loss: 6.4378
Batch 100, Loss: 6.3837
Batch 125, Loss: 6.3316
Batch 150, Loss: 6.2809
Batch 175, Loss: 6.2315
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 263.77545261383057 seconds
Epoch 6 accuracy: 11.07%
Batch 25, Loss: 6.1512
Batch 50, Loss: 6.1049
Batch 75, Loss: 6.0600
Batch 100, Loss: 6.0160
Batch 125, Loss: 5.9729
Batch 150, Loss: 5.9307
Batch 175, Loss: 5.8892
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 264.0215632915497 seconds
Epoch 7 accuracy: 11.48%
Batch 25, Loss: 5.8211
Batch 50, Loss: 5.7814
Batch 75, Loss: 5.7420
Batch 100, Loss: 5.7030
Batch 125, Loss: 5.6644
Batch 150, Loss: 5.6261
Batch 175, Loss: 5.5879
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 263.9468882083893 seconds
Epoch 8 accuracy: 11.7%
Batch 25, Loss: 5.5243
Batch 50, Loss: 5.4868
Batch 75, Loss: 5.4497
Batch 100, Loss: 5.4129
Batch 125, Loss: 5.3764
Batch 150, Loss: 5.3403
Batch 175, Loss: 5.3043
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 267.0733871459961 seconds
Epoch 9 accuracy: 11.63%
Batch 25, Loss: 5.2442
Batch 50, Loss: 5.2086
Batch 75, Loss: 5.1733
Batch 100, Loss: 5.1382
Batch 125, Loss: 5.1035
Batch 150, Loss: 5.0689
Batch 175, Loss: 5.0343
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 264.3386170864105 seconds
Epoch 10 accuracy: 11.58%
Batch 25, Loss: 4.9767
Batch 50, Loss: 4.9425
Batch 75, Loss: 4.9083
Batch 100, Loss: 4.8741
Batch 125, Loss: 4.8402
Batch 150, Loss: 4.8066
Batch 175, Loss: 4.7736
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 264.6095268726349 seconds
Epoch 11 accuracy: 11.57%
Batch 25, Loss: 4.7189
Batch 50, Loss: 4.6866
Batch 75, Loss: 4.6547
Batch 100, Loss: 4.6232
Batch 125, Loss: 4.5922
Batch 150, Loss: 4.5617
Batch 175, Loss: 4.5317
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 265.1119668483734 seconds
Epoch 12 accuracy: 11.37%
Batch 25, Loss: 4.4822
Batch 50, Loss: 4.4531
Batch 75, Loss: 4.4241
Batch 100, Loss: 4.3951
Batch 125, Loss: 4.3660
Batch 150, Loss: 4.3371
Batch 175, Loss: 4.3082
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 264.8389210700989 seconds
Epoch 13 accuracy: 11.3%
Batch 25, Loss: 4.2597
Batch 50, Loss: 4.2310
Batch 75, Loss: 4.2028
Batch 100, Loss: 4.1748
Batch 125, Loss: 4.1473
Batch 150, Loss: 4.1201
Batch 175, Loss: 4.0933
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 264.96350145339966 seconds
Epoch 14 accuracy: 11.22%
Batch 25, Loss: 4.0485
Batch 50, Loss: 4.0220
Batch 75, Loss: 3.9956
Batch 100, Loss: 3.9695
Batch 125, Loss: 3.9436
Batch 150, Loss: 3.9179
Batch 175, Loss: 3.8924
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 263.89019417762756 seconds
Epoch 15 accuracy: 11.11%
Batch 25, Loss: 3.8499
Batch 50, Loss: 3.8249
Batch 75, Loss: 3.8001
Batch 100, Loss: 3.7755
Batch 125, Loss: 3.7510
Batch 150, Loss: 3.7267
Batch 175, Loss: 3.7025
Noise applied in 0 out of 3072 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 264.3181664943695 seconds
Epoch 16 accuracy: 11.1%
Batch 25, Loss: 3.6618
Batch 50, Loss: 3.6376
Batch 75, Loss: 3.6133
Batch 100, Loss: 3.5891
Batch 125, Loss: 3.5649
Batch 150, Loss: 3.5406
Batch 175, Loss: 3.5163
Noise applied in 0 out of 3264 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 264.7036786079407 seconds
Epoch 17 accuracy: 11.1%
Batch 25, Loss: 3.4755
Batch 50, Loss: 3.4512
Batch 75, Loss: 3.4268
Batch 100, Loss: 3.4022
Batch 125, Loss: 3.3773
Batch 150, Loss: 3.3522
Batch 175, Loss: 3.3269
Noise applied in 0 out of 3456 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 286.5415108203888 seconds
Epoch 18 accuracy: 11.12%
Batch 25, Loss: 3.2840
Batch 50, Loss: 3.2578
Batch 75, Loss: 3.2311
Batch 100, Loss: 3.2040
Batch 125, Loss: 3.1761
Batch 150, Loss: 3.1476
Batch 175, Loss: 3.1183
Noise applied in 0 out of 3648 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 302.49785685539246 seconds
Epoch 19 accuracy: 11.1%
Batch 25, Loss: 3.0672
Batch 50, Loss: 3.0358
Batch 75, Loss: 3.0034
Batch 100, Loss: 2.9701
Batch 125, Loss: 2.9359
Batch 150, Loss: 2.9007
Batch 175, Loss: 2.8649
Noise applied in 0 out of 3840 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 264.7364993095398 seconds
Epoch 20 accuracy: 11.13%
rho:  0.04 , alpha:  0.3
Total training time: 5378.675405502319 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.8451
Norm of the Gradient: 1.2323147058e+00
Smallest Hessian Eigenvalue: -0.4914
Noise Threshold: 0.4
Noise Radius: 0.25
