The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:04:52
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 167.9843
Batch 50, Loss: 83.5802
Batch 75, Loss: 43.4283
Batch 100, Loss: 31.2751
Batch 125, Loss: 25.3427
Batch 150, Loss: 21.7659
Batch 175, Loss: 19.1764
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 299.1383726596832 seconds
Epoch 1 accuracy: 10.3%
Batch 25, Loss: 15.9744
Batch 50, Loss: 14.5237
Batch 75, Loss: 13.2971
Batch 100, Loss: 12.2331
Batch 125, Loss: 11.3097
Batch 150, Loss: 10.5189
Batch 175, Loss: 9.8328
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 266.2434620857239 seconds
Epoch 2 accuracy: 10.21%
Batch 25, Loss: 8.8645
Batch 50, Loss: 8.3919
Batch 75, Loss: 7.9840
Batch 100, Loss: 7.6251
Batch 125, Loss: 7.3072
Batch 150, Loss: 7.0255
Batch 175, Loss: 6.7746
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 267.248215675354 seconds
Epoch 3 accuracy: 10.31%
Batch 25, Loss: 6.4083
Batch 50, Loss: 6.2194
Batch 75, Loss: 6.0498
Batch 100, Loss: 5.8959
Batch 125, Loss: 5.7543
Batch 150, Loss: 5.6248
Batch 175, Loss: 5.5065
Noise applied in 150 out of 768 batches, 19.53
Epoch 4 learning rate: 0.01
Epoch 4 time: 336.74049282073975 seconds
Epoch 4 accuracy: 10.25%
Batch 25, Loss: 5.3313
Batch 50, Loss: 5.2376
Batch 75, Loss: 5.1507
Batch 100, Loss: 5.0704
Batch 125, Loss: 4.9959
Batch 150, Loss: 4.9260
Batch 175, Loss: 4.8606
Noise applied in 342 out of 960 batches, 35.62
Epoch 5 learning rate: 0.01
Epoch 5 time: 356.42571806907654 seconds
Epoch 5 accuracy: 10.34%
Batch 25, Loss: 4.7609
Batch 50, Loss: 4.7069
Batch 75, Loss: 4.6562
Batch 100, Loss: 4.6082
Batch 125, Loss: 4.5631
Batch 150, Loss: 4.5198
Batch 175, Loss: 4.4782
Noise applied in 534 out of 1152 batches, 46.35
Epoch 6 learning rate: 0.01
Epoch 6 time: 356.2395849227905 seconds
Epoch 6 accuracy: 10.43%
Batch 25, Loss: 4.4119
Batch 50, Loss: 4.3743
Batch 75, Loss: 4.3377
Batch 100, Loss: 4.3027
Batch 125, Loss: 4.2685
Batch 150, Loss: 4.2357
Batch 175, Loss: 4.2044
Noise applied in 726 out of 1344 batches, 54.02
Epoch 7 learning rate: 0.01
Epoch 7 time: 356.38865971565247 seconds
Epoch 7 accuracy: 10.44%
Batch 25, Loss: 4.1534
Batch 50, Loss: 4.1240
Batch 75, Loss: 4.0952
Batch 100, Loss: 4.0672
Batch 125, Loss: 4.0398
Batch 150, Loss: 4.0135
Batch 175, Loss: 3.9881
Noise applied in 918 out of 1536 batches, 59.77
Epoch 8 learning rate: 0.01
Epoch 8 time: 359.63180708885193 seconds
Epoch 8 accuracy: 10.52%
Batch 25, Loss: 3.9462
Batch 50, Loss: 3.9220
Batch 75, Loss: 3.8981
Batch 100, Loss: 3.8749
Batch 125, Loss: 3.8523
Batch 150, Loss: 3.8304
Batch 175, Loss: 3.8088
Noise applied in 1110 out of 1728 batches, 64.24
Epoch 9 learning rate: 0.01
Epoch 9 time: 356.8341290950775 seconds
Epoch 9 accuracy: 10.51%
Batch 25, Loss: 3.7741
Batch 50, Loss: 3.7541
Batch 75, Loss: 3.7344
Batch 100, Loss: 3.7152
Batch 125, Loss: 3.6966
Batch 150, Loss: 3.6786
Batch 175, Loss: 3.6607
Noise applied in 1302 out of 1920 batches, 67.81
Epoch 10 learning rate: 0.01
Epoch 10 time: 356.0004587173462 seconds
Epoch 10 accuracy: 10.51%
Batch 25, Loss: 3.6311
Batch 50, Loss: 3.6141
Batch 75, Loss: 3.5976
Batch 100, Loss: 3.5818
Batch 125, Loss: 3.5663
Batch 150, Loss: 3.5513
Batch 175, Loss: 3.5365
Noise applied in 1494 out of 2112 batches, 70.74
Epoch 11 learning rate: 0.01
Epoch 11 time: 356.6425054073334 seconds
Epoch 11 accuracy: 10.54%
Batch 25, Loss: 3.5121
Batch 50, Loss: 3.4983
Batch 75, Loss: 3.4848
Batch 100, Loss: 3.4714
Batch 125, Loss: 3.4579
Batch 150, Loss: 3.4448
Batch 175, Loss: 3.4317
Noise applied in 1686 out of 2304 batches, 73.18
Epoch 12 learning rate: 0.01
Epoch 12 time: 356.80378222465515 seconds
Epoch 12 accuracy: 10.53%
Batch 25, Loss: 3.4104
Batch 50, Loss: 3.3983
Batch 75, Loss: 3.3863
Batch 100, Loss: 3.3745
Batch 125, Loss: 3.3629
Batch 150, Loss: 3.3517
Batch 175, Loss: 3.3405
Noise applied in 1878 out of 2496 batches, 75.24
Epoch 13 learning rate: 0.01
Epoch 13 time: 355.79269337654114 seconds
Epoch 13 accuracy: 10.45%
Batch 25, Loss: 3.3222
Batch 50, Loss: 3.3116
Batch 75, Loss: 3.3011
Batch 100, Loss: 3.2906
Batch 125, Loss: 3.2805
Batch 150, Loss: 3.2707
Batch 175, Loss: 3.2609
Noise applied in 2070 out of 2688 batches, 77.01
Epoch 14 learning rate: 0.01
Epoch 14 time: 366.6598560810089 seconds
Epoch 14 accuracy: 10.46%
Batch 25, Loss: 3.2448
Batch 50, Loss: 3.2353
Batch 75, Loss: 3.2258
Batch 100, Loss: 3.2165
Batch 125, Loss: 3.2072
Batch 150, Loss: 3.1980
Batch 175, Loss: 3.1888
Noise applied in 2262 out of 2880 batches, 78.54
Epoch 15 learning rate: 0.01
Epoch 15 time: 401.9800751209259 seconds
Epoch 15 accuracy: 10.47%
Batch 25, Loss: 3.1739
Batch 50, Loss: 3.1654
Batch 75, Loss: 3.1567
Batch 100, Loss: 3.1480
Batch 125, Loss: 3.1397
Batch 150, Loss: 3.1315
Batch 175, Loss: 3.1234
Noise applied in 2454 out of 3072 batches, 79.88
Epoch 16 learning rate: 0.01
Epoch 16 time: 358.2363164424896 seconds
Epoch 16 accuracy: 10.44%
Batch 25, Loss: 3.1099
Batch 50, Loss: 3.1020
Batch 75, Loss: 3.0943
Batch 100, Loss: 3.0862
Batch 125, Loss: 3.0786
Batch 150, Loss: 3.0712
Batch 175, Loss: 3.0639
Noise applied in 2646 out of 3264 batches, 81.07
Epoch 17 learning rate: 0.01
Epoch 17 time: 356.9910988807678 seconds
Epoch 17 accuracy: 10.45%
Batch 25, Loss: 3.0513
Batch 50, Loss: 3.0439
Batch 75, Loss: 3.0363
Batch 100, Loss: 3.0291
Batch 125, Loss: 3.0220
Batch 150, Loss: 3.0150
Batch 175, Loss: 3.0080
Noise applied in 2838 out of 3456 batches, 82.12
Epoch 18 learning rate: 0.01
Epoch 18 time: 409.03153371810913 seconds
Epoch 18 accuracy: 10.47%
Batch 25, Loss: 2.9963
Batch 50, Loss: 2.9895
Batch 75, Loss: 2.9825
Batch 100, Loss: 2.9759
Batch 125, Loss: 2.9691
Batch 150, Loss: 2.9624
Batch 175, Loss: 2.9559
Noise applied in 3030 out of 3648 batches, 83.06
Epoch 19 learning rate: 0.01
Epoch 19 time: 357.2998135089874 seconds
Epoch 19 accuracy: 10.47%
Batch 25, Loss: 2.9452
Batch 50, Loss: 2.9386
Batch 75, Loss: 2.9323
Batch 100, Loss: 2.9258
Batch 125, Loss: 2.9196
Batch 150, Loss: 2.9136
Batch 175, Loss: 2.9074
Noise applied in 3222 out of 3840 batches, 83.91
Epoch 20 learning rate: 0.01
Epoch 20 time: 359.635057926178 seconds
Epoch 20 accuracy: 10.48%
rho:  0.04 , alpha:  0.3
Total training time: 6989.981405496597 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 3.9491
Norm of the Gradient: 7.8283590078e-01
Smallest Hessian Eigenvalue: -0.2589
Noise Threshold: 1.0
Noise Radius: 0.25
