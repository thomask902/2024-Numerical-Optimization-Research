The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-19:33:30
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 42.5095
Batch 50, Loss: 34.0659
Batch 75, Loss: 16.7841
Batch 100, Loss: 12.6640
Batch 125, Loss: 9.8639
Batch 150, Loss: 8.6375
Batch 175, Loss: 7.7372
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 293.7833499908447 seconds
Epoch 1 accuracy: 11.32%
Batch 25, Loss: 6.5685
Batch 50, Loss: 6.1401
Batch 75, Loss: 5.8199
Batch 100, Loss: 5.5427
Batch 125, Loss: 5.2928
Batch 150, Loss: 5.0636
Batch 175, Loss: 4.8539
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 285.5781750679016 seconds
Epoch 2 accuracy: 10.38%
Batch 25, Loss: 4.5467
Batch 50, Loss: 4.3866
Batch 75, Loss: 4.2399
Batch 100, Loss: 4.1046
Batch 125, Loss: 3.9795
Batch 150, Loss: 3.8635
Batch 175, Loss: 3.7560
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 276.61183285713196 seconds
Epoch 3 accuracy: 10.99%
Batch 25, Loss: 3.5931
Batch 50, Loss: 3.5062
Batch 75, Loss: 3.4270
Batch 100, Loss: 3.3556
Batch 125, Loss: 3.2918
Batch 150, Loss: 3.2393
Batch 175, Loss: 3.1947
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 276.7757043838501 seconds
Epoch 4 accuracy: 8.93%
Batch 25, Loss: 3.1281
Batch 50, Loss: 3.0927
Batch 75, Loss: 3.0594
Batch 100, Loss: 3.0278
Batch 125, Loss: 2.9977
Batch 150, Loss: 2.9689
Batch 175, Loss: 2.9412
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 279.2549774646759 seconds
Epoch 5 accuracy: 9.23%
Batch 25, Loss: 2.8970
Batch 50, Loss: 2.8719
Batch 75, Loss: 2.8476
Batch 100, Loss: 2.8242
Batch 125, Loss: 2.8015
Batch 150, Loss: 2.7797
Batch 175, Loss: 2.7586
Noise applied in 40 out of 192 batches, 20.83
Epoch 6 learning rate: 0.01
Epoch 6 time: 308.5696964263916 seconds
Epoch 6 accuracy: 9.24%
Batch 25, Loss: 2.7249
Batch 50, Loss: 2.7056
Batch 75, Loss: 2.6868
Batch 100, Loss: 2.6686
Batch 125, Loss: 2.6510
Batch 150, Loss: 2.6341
Batch 175, Loss: 2.6174
Noise applied in 192 out of 192 batches, 100.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 407.4278516769409 seconds
Epoch 7 accuracy: 9.21%
Batch 25, Loss: 2.5906
Batch 50, Loss: 2.5752
Batch 75, Loss: 2.5603
Batch 100, Loss: 2.5459
Batch 125, Loss: 2.5316
Batch 150, Loss: 2.5180
Batch 175, Loss: 2.5048
Noise applied in 192 out of 192 batches, 100.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 439.0667247772217 seconds
Epoch 8 accuracy: 9.25%
Batch 25, Loss: 2.4832
Batch 50, Loss: 2.4708
Batch 75, Loss: 2.4588
Batch 100, Loss: 2.4470
Batch 125, Loss: 2.4352
Batch 150, Loss: 2.4239
Batch 175, Loss: 2.4122
Noise applied in 192 out of 192 batches, 100.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 403.45642948150635 seconds
Epoch 9 accuracy: 9.19%
Batch 25, Loss: 2.3941
Batch 50, Loss: 2.3836
Batch 75, Loss: 2.3731
Batch 100, Loss: 2.3631
Batch 125, Loss: 2.3531
Batch 150, Loss: 2.3433
Batch 175, Loss: 2.3337
Noise applied in 192 out of 192 batches, 100.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 435.62927436828613 seconds
Epoch 10 accuracy: 9.3%
Batch 25, Loss: 2.3179
Batch 50, Loss: 2.3089
Batch 75, Loss: 2.3001
Batch 100, Loss: 2.2917
Batch 125, Loss: 2.2832
Batch 150, Loss: 2.2749
Batch 175, Loss: 2.2663
Noise applied in 192 out of 192 batches, 100.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 400.8710150718689 seconds
Epoch 11 accuracy: 9.31%
Batch 25, Loss: 2.2525
Batch 50, Loss: 2.2445
Batch 75, Loss: 2.2365
Batch 100, Loss: 2.2288
Batch 125, Loss: 2.2210
Batch 150, Loss: 2.2135
Batch 175, Loss: 2.2062
Noise applied in 251 out of 192 batches, 130.73
Epoch 12 learning rate: 0.01
Epoch 12 time: 433.2223880290985 seconds
Epoch 12 accuracy: 9.13%
Batch 25, Loss: 2.1941
Batch 50, Loss: 2.1868
Batch 75, Loss: 2.1798
Batch 100, Loss: 2.1733
Batch 125, Loss: 2.1665
Batch 150, Loss: 2.1598
Batch 175, Loss: 2.1531
Noise applied in 384 out of 192 batches, 200.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 478.0767722129822 seconds
Epoch 13 accuracy: 9.22%
Batch 25, Loss: 2.1424
Batch 50, Loss: 2.1362
Batch 75, Loss: 2.1300
Batch 100, Loss: 2.1240
Batch 125, Loss: 2.1180
Batch 150, Loss: 2.1121
Batch 175, Loss: 2.1060
Noise applied in 384 out of 192 batches, 200.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 496.1096315383911 seconds
Epoch 14 accuracy: 9.82%
Batch 25, Loss: 2.0971
Batch 50, Loss: 2.0925
Batch 75, Loss: 2.0877
Batch 100, Loss: 2.0829
Batch 125, Loss: 2.0785
Batch 150, Loss: 2.0741
Batch 175, Loss: 2.0697
Noise applied in 384 out of 192 batches, 200.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 501.1413064002991 seconds
Epoch 15 accuracy: 9.65%
Batch 25, Loss: 2.0627
Batch 50, Loss: 2.0585
Batch 75, Loss: 2.0546
Batch 100, Loss: 2.0507
Batch 125, Loss: 2.0472
Batch 150, Loss: 2.0434
Batch 175, Loss: 2.0394
Noise applied in 384 out of 192 batches, 200.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 484.1471438407898 seconds
Epoch 16 accuracy: 9.59%
Batch 25, Loss: 2.0332
Batch 50, Loss: 2.0295
Batch 75, Loss: 2.0257
Batch 100, Loss: 2.0222
Batch 125, Loss: 2.0190
Batch 150, Loss: 2.0158
Batch 175, Loss: 2.0123
Noise applied in 384 out of 192 batches, 200.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 496.3950800895691 seconds
Epoch 17 accuracy: 9.59%
Batch 25, Loss: 2.0065
Batch 50, Loss: 2.0034
Batch 75, Loss: 2.0002
Batch 100, Loss: 1.9972
Batch 125, Loss: 1.9939
Batch 150, Loss: 1.9909
Batch 175, Loss: 1.9879
Noise applied in 384 out of 192 batches, 200.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 480.54070568084717 seconds
Epoch 18 accuracy: 9.7%
Batch 25, Loss: 1.9825
Batch 50, Loss: 1.9797
Batch 75, Loss: 1.9768
Batch 100, Loss: 1.9740
Batch 125, Loss: 1.9713
Batch 150, Loss: 1.9686
Batch 175, Loss: 1.9657
Noise applied in 384 out of 192 batches, 200.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 526.6496028900146 seconds
Epoch 19 accuracy: 9.74%
Batch 25, Loss: 1.9610
Batch 50, Loss: 1.9585
Batch 75, Loss: 1.9561
Batch 100, Loss: 1.9538
Batch 125, Loss: 1.9514
Batch 150, Loss: 1.9491
Batch 175, Loss: 1.9468
Noise applied in 384 out of 192 batches, 200.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 475.8748242855072 seconds
Epoch 20 accuracy: 9.81%
rho:  0.04 , alpha:  0.3
Total training time: 8179.200308084488 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 13.6567
Norm of the Gradient: 9.6219434738e+00
Smallest Hessian Eigenvalue: -3.0319
Noise Threshold: 0.4
Noise Radius: 0.5
