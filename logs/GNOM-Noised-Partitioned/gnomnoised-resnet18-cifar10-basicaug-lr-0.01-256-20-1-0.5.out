The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:07:48
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 52.9046
Batch 50, Loss: 48.0012
Batch 75, Loss: 27.8123
Batch 100, Loss: 20.6556
Batch 125, Loss: 17.5944
Batch 150, Loss: 16.0529
Batch 175, Loss: 14.9068
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 278.2943391799927 seconds
Epoch 1 accuracy: 9.22%
Batch 25, Loss: 13.4138
Batch 50, Loss: 12.7087
Batch 75, Loss: 12.0897
Batch 100, Loss: 11.5383
Batch 125, Loss: 11.0428
Batch 150, Loss: 10.5949
Batch 175, Loss: 10.1869
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 266.8575677871704 seconds
Epoch 2 accuracy: 9.46%
Batch 25, Loss: 9.5751
Batch 50, Loss: 9.2494
Batch 75, Loss: 8.9486
Batch 100, Loss: 8.6697
Batch 125, Loss: 8.4107
Batch 150, Loss: 8.1685
Batch 175, Loss: 7.9407
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 266.49484038352966 seconds
Epoch 3 accuracy: 9.79%
Batch 25, Loss: 7.5888
Batch 50, Loss: 7.3958
Batch 75, Loss: 7.2141
Batch 100, Loss: 7.0424
Batch 125, Loss: 6.8804
Batch 150, Loss: 6.7277
Batch 175, Loss: 6.5807
Noise applied in 94 out of 768 batches, 12.24
Epoch 4 learning rate: 0.01
Epoch 4 time: 310.4894366264343 seconds
Epoch 4 accuracy: 9.91%
Batch 25, Loss: 6.3513
Batch 50, Loss: 6.2235
Batch 75, Loss: 6.1028
Batch 100, Loss: 5.9866
Batch 125, Loss: 5.8752
Batch 150, Loss: 5.7693
Batch 175, Loss: 5.6691
Noise applied in 286 out of 960 batches, 29.79
Epoch 5 learning rate: 0.01
Epoch 5 time: 356.31386041641235 seconds
Epoch 5 accuracy: 9.96%
Batch 25, Loss: 5.5095
Batch 50, Loss: 5.4195
Batch 75, Loss: 5.3315
Batch 100, Loss: 5.2463
Batch 125, Loss: 5.1657
Batch 150, Loss: 5.0867
Batch 175, Loss: 5.0101
Noise applied in 478 out of 1152 batches, 41.49
Epoch 6 learning rate: 0.01
Epoch 6 time: 356.79881620407104 seconds
Epoch 6 accuracy: 10.02%
Batch 25, Loss: 4.8887
Batch 50, Loss: 4.8197
Batch 75, Loss: 4.7530
Batch 100, Loss: 4.6881
Batch 125, Loss: 4.6248
Batch 150, Loss: 4.5634
Batch 175, Loss: 4.5037
Noise applied in 670 out of 1344 batches, 49.85
Epoch 7 learning rate: 0.01
Epoch 7 time: 358.8381578922272 seconds
Epoch 7 accuracy: 9.9%
Batch 25, Loss: 4.4066
Batch 50, Loss: 4.3518
Batch 75, Loss: 4.2993
Batch 100, Loss: 4.2475
Batch 125, Loss: 4.1967
Batch 150, Loss: 4.1475
Batch 175, Loss: 4.1003
Noise applied in 862 out of 1536 batches, 56.12
Epoch 8 learning rate: 0.01
Epoch 8 time: 356.98742508888245 seconds
Epoch 8 accuracy: 9.78%
Batch 25, Loss: 4.0224
Batch 50, Loss: 3.9782
Batch 75, Loss: 3.9347
Batch 100, Loss: 3.8923
Batch 125, Loss: 3.8518
Batch 150, Loss: 3.8119
Batch 175, Loss: 3.7732
Noise applied in 1054 out of 1728 batches, 61.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 356.44588255882263 seconds
Epoch 9 accuracy: 9.83%
Batch 25, Loss: 3.7099
Batch 50, Loss: 3.6744
Batch 75, Loss: 3.6400
Batch 100, Loss: 3.6070
Batch 125, Loss: 3.5742
Batch 150, Loss: 3.5420
Batch 175, Loss: 3.5097
Noise applied in 1246 out of 1920 batches, 64.90
Epoch 10 learning rate: 0.01
Epoch 10 time: 356.23670291900635 seconds
Epoch 10 accuracy: 9.96%
Batch 25, Loss: 3.4587
Batch 50, Loss: 3.4291
Batch 75, Loss: 3.4005
Batch 100, Loss: 3.3719
Batch 125, Loss: 3.3442
Batch 150, Loss: 3.3177
Batch 175, Loss: 3.2915
Noise applied in 1438 out of 2112 batches, 68.09
Epoch 11 learning rate: 0.01
Epoch 11 time: 356.3964285850525 seconds
Epoch 11 accuracy: 10.06%
Batch 25, Loss: 3.2487
Batch 50, Loss: 3.2235
Batch 75, Loss: 3.1996
Batch 100, Loss: 3.1757
Batch 125, Loss: 3.1531
Batch 150, Loss: 3.1303
Batch 175, Loss: 3.1079
Noise applied in 1630 out of 2304 batches, 70.75
Epoch 12 learning rate: 0.01
Epoch 12 time: 357.15980315208435 seconds
Epoch 12 accuracy: 10.2%
Batch 25, Loss: 3.0719
Batch 50, Loss: 3.0512
Batch 75, Loss: 3.0307
Batch 100, Loss: 3.0105
Batch 125, Loss: 2.9904
Batch 150, Loss: 2.9710
Batch 175, Loss: 2.9525
Noise applied in 1822 out of 2496 batches, 73.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 357.17482113838196 seconds
Epoch 13 accuracy: 10.24%
Batch 25, Loss: 2.9222
Batch 50, Loss: 2.9047
Batch 75, Loss: 2.8869
Batch 100, Loss: 2.8696
Batch 125, Loss: 2.8524
Batch 150, Loss: 2.8361
Batch 175, Loss: 2.8200
Noise applied in 2014 out of 2688 batches, 74.93
Epoch 14 learning rate: 0.01
Epoch 14 time: 377.8348939418793 seconds
Epoch 14 accuracy: 10.11%
Batch 25, Loss: 2.7943
Batch 50, Loss: 2.7788
Batch 75, Loss: 2.7633
Batch 100, Loss: 2.7481
Batch 125, Loss: 2.7338
Batch 150, Loss: 2.7198
Batch 175, Loss: 2.7057
Noise applied in 2206 out of 2880 batches, 76.60
Epoch 15 learning rate: 0.01
Epoch 15 time: 391.5834255218506 seconds
Epoch 15 accuracy: 10.16%
Batch 25, Loss: 2.6829
Batch 50, Loss: 2.6696
Batch 75, Loss: 2.6570
Batch 100, Loss: 2.6446
Batch 125, Loss: 2.6320
Batch 150, Loss: 2.6197
Batch 175, Loss: 2.6075
Noise applied in 2398 out of 3072 batches, 78.06
Epoch 16 learning rate: 0.01
Epoch 16 time: 360.0846691131592 seconds
Epoch 16 accuracy: 9.81%
Batch 25, Loss: 2.5873
Batch 50, Loss: 2.5756
Batch 75, Loss: 2.5642
Batch 100, Loss: 2.5529
Batch 125, Loss: 2.5418
Batch 150, Loss: 2.5309
Batch 175, Loss: 2.5199
Noise applied in 2590 out of 3264 batches, 79.35
Epoch 17 learning rate: 0.01
Epoch 17 time: 408.739146232605 seconds
Epoch 17 accuracy: 10.67%
Batch 25, Loss: 2.5015
Batch 50, Loss: 2.4910
Batch 75, Loss: 2.4806
Batch 100, Loss: 2.4703
Batch 125, Loss: 2.4603
Batch 150, Loss: 2.4506
Batch 175, Loss: 2.4410
Noise applied in 2782 out of 3456 batches, 80.50
Epoch 18 learning rate: 0.01
Epoch 18 time: 359.0532805919647 seconds
Epoch 18 accuracy: 10.86%
Batch 25, Loss: 2.4248
Batch 50, Loss: 2.4154
Batch 75, Loss: 2.4060
Batch 100, Loss: 2.3967
Batch 125, Loss: 2.3876
Batch 150, Loss: 2.3784
Batch 175, Loss: 2.3693
Noise applied in 2974 out of 3648 batches, 81.52
Epoch 19 learning rate: 0.01
Epoch 19 time: 362.2393057346344 seconds
Epoch 19 accuracy: 10.36%
Batch 25, Loss: 2.3540
Batch 50, Loss: 2.3453
Batch 75, Loss: 2.3368
Batch 100, Loss: 2.3286
Batch 125, Loss: 2.3201
Batch 150, Loss: 2.3117
Batch 175, Loss: 2.3040
Noise applied in 3166 out of 3840 batches, 82.45
Epoch 20 learning rate: 0.01
Epoch 20 time: 388.05655312538147 seconds
Epoch 20 accuracy: 10.08%
rho:  0.04 , alpha:  0.3
Total training time: 6982.096405982971 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.8306
Norm of the Gradient: 1.5528689623e+00
Smallest Hessian Eigenvalue: -0.5570
Noise Threshold: 1.0
Noise Radius: 0.5
