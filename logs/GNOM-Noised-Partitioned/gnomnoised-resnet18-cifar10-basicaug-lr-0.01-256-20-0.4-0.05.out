The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:02:59
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 112.5884
Batch 50, Loss: 19.3784
Batch 75, Loss: 9.7679
Batch 100, Loss: 7.8649
Batch 125, Loss: 6.9924
Batch 150, Loss: 6.4548
Batch 175, Loss: 6.0359
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 305.0012159347534 seconds
Epoch 1 accuracy: 8.88%
Batch 25, Loss: 5.4714
Batch 50, Loss: 5.1914
Batch 75, Loss: 4.9450
Batch 100, Loss: 4.7290
Batch 125, Loss: 4.5390
Batch 150, Loss: 4.3735
Batch 175, Loss: 4.2310
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 265.7478234767914 seconds
Epoch 2 accuracy: 10.04%
Batch 25, Loss: 4.0334
Batch 50, Loss: 3.9331
Batch 75, Loss: 3.8412
Batch 100, Loss: 3.7559
Batch 125, Loss: 3.6763
Batch 150, Loss: 3.6013
Batch 175, Loss: 3.5305
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 266.06639790534973 seconds
Epoch 3 accuracy: 10.46%
Batch 25, Loss: 3.4198
Batch 50, Loss: 3.3584
Batch 75, Loss: 3.3002
Batch 100, Loss: 3.2452
Batch 125, Loss: 3.1931
Batch 150, Loss: 3.1437
Batch 175, Loss: 3.0968
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 267.37772512435913 seconds
Epoch 4 accuracy: 10.58%
Batch 25, Loss: 3.0232
Batch 50, Loss: 2.9822
Batch 75, Loss: 2.9432
Batch 100, Loss: 2.9060
Batch 125, Loss: 2.8705
Batch 150, Loss: 2.8366
Batch 175, Loss: 2.8043
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 267.1741144657135 seconds
Epoch 5 accuracy: 11.61%
Batch 25, Loss: 2.7539
Batch 50, Loss: 2.7260
Batch 75, Loss: 2.6993
Batch 100, Loss: 2.6737
Batch 125, Loss: 2.6493
Batch 150, Loss: 2.6260
Batch 175, Loss: 2.6036
Noise applied in 60 out of 1152 batches, 5.21
Epoch 6 learning rate: 0.01
Epoch 6 time: 295.33197689056396 seconds
Epoch 6 accuracy: 11.43%
Batch 25, Loss: 2.5683
Batch 50, Loss: 2.5483
Batch 75, Loss: 2.5290
Batch 100, Loss: 2.5103
Batch 125, Loss: 2.4922
Batch 150, Loss: 2.4747
Batch 175, Loss: 2.4575
Noise applied in 252 out of 1344 batches, 18.75
Epoch 7 learning rate: 0.01
Epoch 7 time: 357.26144576072693 seconds
Epoch 7 accuracy: 11.36%
Batch 25, Loss: 2.4297
Batch 50, Loss: 2.4136
Batch 75, Loss: 2.3979
Batch 100, Loss: 2.3827
Batch 125, Loss: 2.3678
Batch 150, Loss: 2.3532
Batch 175, Loss: 2.3388
Noise applied in 444 out of 1536 batches, 28.91
Epoch 8 learning rate: 0.01
Epoch 8 time: 357.91548109054565 seconds
Epoch 8 accuracy: 11.42%
Batch 25, Loss: 2.3149
Batch 50, Loss: 2.3010
Batch 75, Loss: 2.2875
Batch 100, Loss: 2.2741
Batch 125, Loss: 2.2609
Batch 150, Loss: 2.2480
Batch 175, Loss: 2.2352
Noise applied in 636 out of 1728 batches, 36.81
Epoch 9 learning rate: 0.01
Epoch 9 time: 359.01707220077515 seconds
Epoch 9 accuracy: 11.24%
Batch 25, Loss: 2.2141
Batch 50, Loss: 2.2019
Batch 75, Loss: 2.1900
Batch 100, Loss: 2.1782
Batch 125, Loss: 2.1668
Batch 150, Loss: 2.1556
Batch 175, Loss: 2.1447
Noise applied in 828 out of 1920 batches, 43.12
Epoch 10 learning rate: 0.01
Epoch 10 time: 357.0119755268097 seconds
Epoch 10 accuracy: 11.38%
Batch 25, Loss: 2.1271
Batch 50, Loss: 2.1172
Batch 75, Loss: 2.1076
Batch 100, Loss: 2.0984
Batch 125, Loss: 2.0897
Batch 150, Loss: 2.0814
Batch 175, Loss: 2.0735
Noise applied in 1020 out of 2112 batches, 48.30
Epoch 11 learning rate: 0.01
Epoch 11 time: 357.55909037590027 seconds
Epoch 11 accuracy: 11.6%
Batch 25, Loss: 2.0611
Batch 50, Loss: 2.0543
Batch 75, Loss: 2.0477
Batch 100, Loss: 2.0414
Batch 125, Loss: 2.0355
Batch 150, Loss: 2.0298
Batch 175, Loss: 2.0243
Noise applied in 1212 out of 2304 batches, 52.60
Epoch 12 learning rate: 0.01
Epoch 12 time: 357.92034673690796 seconds
Epoch 12 accuracy: 11.96%
Batch 25, Loss: 2.0157
Batch 50, Loss: 2.0109
Batch 75, Loss: 2.0063
Batch 100, Loss: 2.0019
Batch 125, Loss: 1.9976
Batch 150, Loss: 1.9935
Batch 175, Loss: 1.9895
Noise applied in 1404 out of 2496 batches, 56.25
Epoch 13 learning rate: 0.01
Epoch 13 time: 357.79872250556946 seconds
Epoch 13 accuracy: 12.05%
Batch 25, Loss: 1.9831
Batch 50, Loss: 1.9794
Batch 75, Loss: 1.9758
Batch 100, Loss: 1.9723
Batch 125, Loss: 1.9690
Batch 150, Loss: 1.9657
Batch 175, Loss: 1.9626
Noise applied in 1596 out of 2688 batches, 59.38
Epoch 14 learning rate: 0.01
Epoch 14 time: 357.73118138313293 seconds
Epoch 14 accuracy: 10.75%
Batch 25, Loss: 1.9574
Batch 50, Loss: 1.9545
Batch 75, Loss: 1.9516
Batch 100, Loss: 1.9488
Batch 125, Loss: 1.9460
Batch 150, Loss: 1.9433
Batch 175, Loss: 1.9406
Noise applied in 1788 out of 2880 batches, 62.08
Epoch 15 learning rate: 0.01
Epoch 15 time: 369.7662696838379 seconds
Epoch 15 accuracy: 10.3%
Batch 25, Loss: 1.9363
Batch 50, Loss: 1.9337
Batch 75, Loss: 1.9312
Batch 100, Loss: 1.9288
Batch 125, Loss: 1.9264
Batch 150, Loss: 1.9241
Batch 175, Loss: 1.9218
Noise applied in 1980 out of 3072 batches, 64.45
Epoch 16 learning rate: 0.01
Epoch 16 time: 400.17259311676025 seconds
Epoch 16 accuracy: 10.08%
Batch 25, Loss: 1.9181
Batch 50, Loss: 1.9159
Batch 75, Loss: 1.9137
Batch 100, Loss: 1.9116
Batch 125, Loss: 1.9096
Batch 150, Loss: 1.9075
Batch 175, Loss: 1.9055
Noise applied in 2172 out of 3264 batches, 66.54
Epoch 17 learning rate: 0.01
Epoch 17 time: 358.7139849662781 seconds
Epoch 17 accuracy: 10.04%
Batch 25, Loss: 1.9023
Batch 50, Loss: 1.9003
Batch 75, Loss: 1.8984
Batch 100, Loss: 1.8965
Batch 125, Loss: 1.8947
Batch 150, Loss: 1.8929
Batch 175, Loss: 1.8911
Noise applied in 2364 out of 3456 batches, 68.40
Epoch 18 learning rate: 0.01
Epoch 18 time: 358.98360228538513 seconds
Epoch 18 accuracy: 10.09%
Batch 25, Loss: 1.8881
Batch 50, Loss: 1.8864
Batch 75, Loss: 1.8847
Batch 100, Loss: 1.8830
Batch 125, Loss: 1.8814
Batch 150, Loss: 1.8797
Batch 175, Loss: 1.8781
Noise applied in 2556 out of 3648 batches, 70.07
Epoch 19 learning rate: 0.01
Epoch 19 time: 409.92202520370483 seconds
Epoch 19 accuracy: 10.17%
Batch 25, Loss: 1.8754
Batch 50, Loss: 1.8739
Batch 75, Loss: 1.8723
Batch 100, Loss: 1.8708
Batch 125, Loss: 1.8693
Batch 150, Loss: 1.8679
Batch 175, Loss: 1.8664
Noise applied in 2748 out of 3840 batches, 71.56
Epoch 20 learning rate: 0.01
Epoch 20 time: 359.7637960910797 seconds
Epoch 20 accuracy: 10.19%
rho:  0.04 , alpha:  0.3
Total training time: 6786.254353761673 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 2.8019
Norm of the Gradient: 3.2259052992e-01
Smallest Hessian Eigenvalue: -0.1235
Noise Threshold: 0.4
Noise Radius: 0.05
