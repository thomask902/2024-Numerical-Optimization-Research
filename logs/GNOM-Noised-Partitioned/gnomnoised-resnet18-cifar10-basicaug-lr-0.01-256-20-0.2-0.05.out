The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:02:14
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 1650.1968
Batch 50, Loss: 193.7074
Batch 75, Loss: 45.1218
Batch 100, Loss: 32.6152
Batch 125, Loss: 26.4528
Batch 150, Loss: 22.7385
Batch 175, Loss: 20.2158
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 312.77174711227417 seconds
Epoch 1 accuracy: 9.0%
Batch 25, Loss: 17.4409
Batch 50, Loss: 16.2032
Batch 75, Loss: 15.1617
Batch 100, Loss: 14.2708
Batch 125, Loss: 13.4969
Batch 150, Loss: 12.8132
Batch 175, Loss: 12.2040
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 264.0654637813568 seconds
Epoch 2 accuracy: 9.32%
Batch 25, Loss: 11.3216
Batch 50, Loss: 10.8630
Batch 75, Loss: 10.4466
Batch 100, Loss: 10.0659
Batch 125, Loss: 9.7131
Batch 150, Loss: 9.3859
Batch 175, Loss: 9.0826
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 263.6934287548065 seconds
Epoch 3 accuracy: 8.92%
Batch 25, Loss: 8.6195
Batch 50, Loss: 8.3671
Batch 75, Loss: 8.1306
Batch 100, Loss: 7.9052
Batch 125, Loss: 7.6889
Batch 150, Loss: 7.4816
Batch 175, Loss: 7.2840
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 264.6629810333252 seconds
Epoch 4 accuracy: 8.81%
Batch 25, Loss: 6.9727
Batch 50, Loss: 6.7968
Batch 75, Loss: 6.6273
Batch 100, Loss: 6.4637
Batch 125, Loss: 6.3058
Batch 150, Loss: 6.1533
Batch 175, Loss: 6.0056
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 265.07751965522766 seconds
Epoch 5 accuracy: 9.33%
Batch 25, Loss: 5.7697
Batch 50, Loss: 5.6381
Batch 75, Loss: 5.5135
Batch 100, Loss: 5.3968
Batch 125, Loss: 5.2880
Batch 150, Loss: 5.1861
Batch 175, Loss: 5.0906
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 264.2499408721924 seconds
Epoch 6 accuracy: 9.33%
Batch 25, Loss: 4.9424
Batch 50, Loss: 4.8604
Batch 75, Loss: 4.7824
Batch 100, Loss: 4.7074
Batch 125, Loss: 4.6350
Batch 150, Loss: 4.5645
Batch 175, Loss: 4.4955
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 264.98576974868774 seconds
Epoch 7 accuracy: 10.09%
Batch 25, Loss: 4.3827
Batch 50, Loss: 4.3179
Batch 75, Loss: 4.2557
Batch 100, Loss: 4.1956
Batch 125, Loss: 4.1382
Batch 150, Loss: 4.0834
Batch 175, Loss: 4.0309
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 265.08629989624023 seconds
Epoch 8 accuracy: 10.45%
Batch 25, Loss: 3.9480
Batch 50, Loss: 3.9022
Batch 75, Loss: 3.8583
Batch 100, Loss: 3.8163
Batch 125, Loss: 3.7761
Batch 150, Loss: 3.7377
Batch 175, Loss: 3.7009
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 264.5964939594269 seconds
Epoch 9 accuracy: 10.36%
Batch 25, Loss: 3.6421
Batch 50, Loss: 3.6084
Batch 75, Loss: 3.5760
Batch 100, Loss: 3.5447
Batch 125, Loss: 3.5143
Batch 150, Loss: 3.4849
Batch 175, Loss: 3.4562
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 267.18395256996155 seconds
Epoch 10 accuracy: 10.25%
Batch 25, Loss: 3.4099
Batch 50, Loss: 3.3835
Batch 75, Loss: 3.3575
Batch 100, Loss: 3.3320
Batch 125, Loss: 3.3071
Batch 150, Loss: 3.2827
Batch 175, Loss: 3.2589
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 265.36810636520386 seconds
Epoch 11 accuracy: 10.35%
Batch 25, Loss: 3.2200
Batch 50, Loss: 3.1976
Batch 75, Loss: 3.1754
Batch 100, Loss: 3.1536
Batch 125, Loss: 3.1317
Batch 150, Loss: 3.1098
Batch 175, Loss: 3.0879
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 265.6145980358124 seconds
Epoch 12 accuracy: 10.4%
Batch 25, Loss: 3.0499
Batch 50, Loss: 3.0271
Batch 75, Loss: 3.0067
Batch 100, Loss: 2.9869
Batch 125, Loss: 2.9674
Batch 150, Loss: 2.9484
Batch 175, Loss: 2.9299
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 265.0703008174896 seconds
Epoch 13 accuracy: 9.96%
Batch 25, Loss: 2.8992
Batch 50, Loss: 2.8807
Batch 75, Loss: 2.8621
Batch 100, Loss: 2.8434
Batch 125, Loss: 2.8245
Batch 150, Loss: 2.8054
Batch 175, Loss: 2.7862
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 264.9360432624817 seconds
Epoch 14 accuracy: 10.6%
Batch 25, Loss: 2.7557
Batch 50, Loss: 2.7387
Batch 75, Loss: 2.7223
Batch 100, Loss: 2.7065
Batch 125, Loss: 2.6910
Batch 150, Loss: 2.6759
Batch 175, Loss: 2.6611
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 265.22135281562805 seconds
Epoch 15 accuracy: 10.37%
Batch 25, Loss: 2.6367
Batch 50, Loss: 2.6225
Batch 75, Loss: 2.6085
Batch 100, Loss: 2.5947
Batch 125, Loss: 2.5811
Batch 150, Loss: 2.5679
Batch 175, Loss: 2.5549
Noise applied in 0 out of 3072 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 265.14585065841675 seconds
Epoch 16 accuracy: 10.55%
Batch 25, Loss: 2.5338
Batch 50, Loss: 2.5216
Batch 75, Loss: 2.5098
Batch 100, Loss: 2.4981
Batch 125, Loss: 2.4865
Batch 150, Loss: 2.4751
Batch 175, Loss: 2.4637
Noise applied in 0 out of 3264 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 264.85746121406555 seconds
Epoch 17 accuracy: 10.33%
Batch 25, Loss: 2.4450
Batch 50, Loss: 2.4342
Batch 75, Loss: 2.4236
Batch 100, Loss: 2.4131
Batch 125, Loss: 2.4029
Batch 150, Loss: 2.3928
Batch 175, Loss: 2.3829
Noise applied in 0 out of 3456 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 269.2660195827484 seconds
Epoch 18 accuracy: 10.23%
Batch 25, Loss: 2.3667
Batch 50, Loss: 2.3572
Batch 75, Loss: 2.3478
Batch 100, Loss: 2.3386
Batch 125, Loss: 2.3295
Batch 150, Loss: 2.3205
Batch 175, Loss: 2.3116
Noise applied in 0 out of 3648 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 291.2781882286072 seconds
Epoch 19 accuracy: 10.08%
Batch 25, Loss: 2.2971
Batch 50, Loss: 2.2886
Batch 75, Loss: 2.2803
Batch 100, Loss: 2.2721
Batch 125, Loss: 2.2641
Batch 150, Loss: 2.2562
Batch 175, Loss: 2.2484
Noise applied in 0 out of 3840 batches, 0.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 294.6692109107971 seconds
Epoch 20 accuracy: 10.07%
rho:  0.04 , alpha:  0.3
Total training time: 5407.820250511169 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 5.7807
Norm of the Gradient: 3.5539761186e-01
Smallest Hessian Eigenvalue: -0.0689
Noise Threshold: 0.2
Noise Radius: 0.05
