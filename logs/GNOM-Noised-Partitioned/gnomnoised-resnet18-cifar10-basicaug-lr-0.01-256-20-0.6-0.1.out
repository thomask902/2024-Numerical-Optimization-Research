The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:04:49
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 668.3460
Batch 50, Loss: 128.1859
Batch 75, Loss: 70.9311
Batch 100, Loss: 50.2649
Batch 125, Loss: 39.4511
Batch 150, Loss: 32.5896
Batch 175, Loss: 28.0008
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 294.37251234054565 seconds
Epoch 1 accuracy: 13.33%
Batch 25, Loss: 22.5915
Batch 50, Loss: 20.2651
Batch 75, Loss: 18.3846
Batch 100, Loss: 16.8269
Batch 125, Loss: 15.5049
Batch 150, Loss: 14.3505
Batch 175, Loss: 13.3025
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 262.20468163490295 seconds
Epoch 2 accuracy: 13.32%
Batch 25, Loss: 11.8543
Batch 50, Loss: 11.2266
Batch 75, Loss: 10.6778
Batch 100, Loss: 10.1876
Batch 125, Loss: 9.7468
Batch 150, Loss: 9.3506
Batch 175, Loss: 8.9915
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 262.78107738494873 seconds
Epoch 3 accuracy: 15.52%
Batch 25, Loss: 8.4524
Batch 50, Loss: 8.1627
Batch 75, Loss: 7.8930
Batch 100, Loss: 7.6403
Batch 125, Loss: 7.3997
Batch 150, Loss: 7.1639
Batch 175, Loss: 6.9588
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 262.8874623775482 seconds
Epoch 4 accuracy: 15.56%
Batch 25, Loss: 6.6460
Batch 50, Loss: 6.4433
Batch 75, Loss: 6.2793
Batch 100, Loss: 6.1675
Batch 125, Loss: 6.0671
Batch 150, Loss: 5.9737
Batch 175, Loss: 5.8858
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 262.71410965919495 seconds
Epoch 5 accuracy: 15.31%
Batch 25, Loss: 5.7484
Batch 50, Loss: 5.6719
Batch 75, Loss: 5.5990
Batch 100, Loss: 5.5291
Batch 125, Loss: 5.4622
Batch 150, Loss: 5.3979
Batch 175, Loss: 5.3361
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 262.9035723209381 seconds
Epoch 6 accuracy: 15.48%
Batch 25, Loss: 5.2375
Batch 50, Loss: 5.1817
Batch 75, Loss: 5.1278
Batch 100, Loss: 5.0755
Batch 125, Loss: 5.0245
Batch 150, Loss: 4.9750
Batch 175, Loss: 4.9268
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 263.1260061264038 seconds
Epoch 7 accuracy: 15.42%
Batch 25, Loss: 4.8485
Batch 50, Loss: 4.8034
Batch 75, Loss: 4.7592
Batch 100, Loss: 4.7161
Batch 125, Loss: 4.6740
Batch 150, Loss: 4.6329
Batch 175, Loss: 4.5928
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 263.51176404953003 seconds
Epoch 8 accuracy: 15.54%
Batch 25, Loss: 4.5277
Batch 50, Loss: 4.4901
Batch 75, Loss: 4.4533
Batch 100, Loss: 4.4174
Batch 125, Loss: 4.3823
Batch 150, Loss: 4.3479
Batch 175, Loss: 4.3144
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 265.8940260410309 seconds
Epoch 9 accuracy: 15.42%
Batch 25, Loss: 4.2593
Batch 50, Loss: 4.2273
Batch 75, Loss: 4.1959
Batch 100, Loss: 4.1649
Batch 125, Loss: 4.1342
Batch 150, Loss: 4.1038
Batch 175, Loss: 4.0738
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 263.0205030441284 seconds
Epoch 10 accuracy: 15.52%
Batch 25, Loss: 4.0243
Batch 50, Loss: 3.9953
Batch 75, Loss: 3.9663
Batch 100, Loss: 3.9376
Batch 125, Loss: 3.9092
Batch 150, Loss: 3.8810
Batch 175, Loss: 3.8529
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 262.57853150367737 seconds
Epoch 11 accuracy: 15.66%
Batch 25, Loss: 3.8061
Batch 50, Loss: 3.7786
Batch 75, Loss: 3.7513
Batch 100, Loss: 3.7242
Batch 125, Loss: 3.6973
Batch 150, Loss: 3.6706
Batch 175, Loss: 3.6442
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 263.34222292900085 seconds
Epoch 12 accuracy: 15.67%
Batch 25, Loss: 3.6002
Batch 50, Loss: 3.5743
Batch 75, Loss: 3.5486
Batch 100, Loss: 3.5231
Batch 125, Loss: 3.4977
Batch 150, Loss: 3.4725
Batch 175, Loss: 3.4475
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 263.285356760025 seconds
Epoch 13 accuracy: 15.7%
Batch 25, Loss: 3.4057
Batch 50, Loss: 3.3811
Batch 75, Loss: 3.3568
Batch 100, Loss: 3.3329
Batch 125, Loss: 3.3093
Batch 150, Loss: 3.2860
Batch 175, Loss: 3.2630
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 262.9341514110565 seconds
Epoch 14 accuracy: 15.85%
Batch 25, Loss: 3.2249
Batch 50, Loss: 3.2028
Batch 75, Loss: 3.1811
Batch 100, Loss: 3.1598
Batch 125, Loss: 3.1389
Batch 150, Loss: 3.1185
Batch 175, Loss: 3.0986
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 263.14308404922485 seconds
Epoch 15 accuracy: 16.08%
Batch 25, Loss: 3.0659
Batch 50, Loss: 3.0471
Batch 75, Loss: 3.0288
Batch 100, Loss: 3.0108
Batch 125, Loss: 2.9932
Batch 150, Loss: 2.9761
Batch 175, Loss: 2.9593
Noise applied in 163 out of 3072 batches, 5.31
Epoch 16 learning rate: 0.01
Epoch 16 time: 340.52661085128784 seconds
Epoch 16 accuracy: 16.35%
Batch 25, Loss: 2.9319
Batch 50, Loss: 2.9163
Batch 75, Loss: 2.9010
Batch 100, Loss: 2.8861
Batch 125, Loss: 2.8717
Batch 150, Loss: 2.8575
Batch 175, Loss: 2.8438
Noise applied in 355 out of 3264 batches, 10.88
Epoch 17 learning rate: 0.01
Epoch 17 time: 356.2820100784302 seconds
Epoch 17 accuracy: 16.36%
Batch 25, Loss: 2.8216
Batch 50, Loss: 2.8087
Batch 75, Loss: 2.7961
Batch 100, Loss: 2.7837
Batch 125, Loss: 2.7717
Batch 150, Loss: 2.7599
Batch 175, Loss: 2.7483
Noise applied in 547 out of 3456 batches, 15.83
Epoch 18 learning rate: 0.01
Epoch 18 time: 406.8017899990082 seconds
Epoch 18 accuracy: 16.46%
Batch 25, Loss: 2.7292
Batch 50, Loss: 2.7181
Batch 75, Loss: 2.7073
Batch 100, Loss: 2.6966
Batch 125, Loss: 2.6861
Batch 150, Loss: 2.6758
Batch 175, Loss: 2.6657
Noise applied in 739 out of 3648 batches, 20.26
Epoch 19 learning rate: 0.01
Epoch 19 time: 354.2261266708374 seconds
Epoch 19 accuracy: 16.32%
Batch 25, Loss: 2.6493
Batch 50, Loss: 2.6397
Batch 75, Loss: 2.6303
Batch 100, Loss: 2.6211
Batch 125, Loss: 2.6120
Batch 150, Loss: 2.6029
Batch 175, Loss: 2.5942
Noise applied in 931 out of 3840 batches, 24.24
Epoch 20 learning rate: 0.01
Epoch 20 time: 355.7820019721985 seconds
Epoch 20 accuracy: 16.23%
rho:  0.04 , alpha:  0.3
Total training time: 5792.334104537964 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.6335
Norm of the Gradient: 6.5081894398e-01
Smallest Hessian Eigenvalue: -0.0812
Noise Threshold: 0.6
Noise Radius: 0.1
