The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:02:13
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 154.4254
Batch 50, Loss: 73.0178
Batch 75, Loss: 42.0532
Batch 100, Loss: 31.6202
Batch 125, Loss: 25.9334
Batch 150, Loss: 22.0558
Batch 175, Loss: 19.2012
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 320.89053606987 seconds
Epoch 1 accuracy: 10.96%
Batch 25, Loss: 15.6562
Batch 50, Loss: 13.9230
Batch 75, Loss: 12.3687
Batch 100, Loss: 11.0806
Batch 125, Loss: 10.1332
Batch 150, Loss: 9.3754
Batch 175, Loss: 8.7424
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 266.35801696777344 seconds
Epoch 2 accuracy: 12.0%
Batch 25, Loss: 7.8718
Batch 50, Loss: 7.4330
Batch 75, Loss: 7.0427
Batch 100, Loss: 6.6940
Batch 125, Loss: 6.3772
Batch 150, Loss: 6.0904
Batch 175, Loss: 5.8368
Noise applied in 0 out of 576 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 264.62150740623474 seconds
Epoch 3 accuracy: 12.94%
Batch 25, Loss: 5.4616
Batch 50, Loss: 5.2530
Batch 75, Loss: 5.0648
Batch 100, Loss: 4.8928
Batch 125, Loss: 4.7320
Batch 150, Loss: 4.5800
Batch 175, Loss: 4.4356
Noise applied in 0 out of 768 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 267.8095688819885 seconds
Epoch 4 accuracy: 14.55%
Batch 25, Loss: 4.2108
Batch 50, Loss: 4.0903
Batch 75, Loss: 3.9806
Batch 100, Loss: 3.8843
Batch 125, Loss: 3.7990
Batch 150, Loss: 3.7234
Batch 175, Loss: 3.6548
Noise applied in 0 out of 960 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 263.8591220378876 seconds
Epoch 5 accuracy: 13.38%
Batch 25, Loss: 3.5450
Batch 50, Loss: 3.4775
Batch 75, Loss: 3.4086
Batch 100, Loss: 3.3416
Batch 125, Loss: 3.2805
Batch 150, Loss: 3.2271
Batch 175, Loss: 3.1794
Noise applied in 0 out of 1152 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 265.8143079280853 seconds
Epoch 6 accuracy: 12.03%
Batch 25, Loss: 3.1099
Batch 50, Loss: 3.0725
Batch 75, Loss: 3.0375
Batch 100, Loss: 3.0057
Batch 125, Loss: 2.9762
Batch 150, Loss: 2.9482
Batch 175, Loss: 2.9213
Noise applied in 0 out of 1344 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 263.35276222229004 seconds
Epoch 7 accuracy: 11.83%
Batch 25, Loss: 2.8773
Batch 50, Loss: 2.8527
Batch 75, Loss: 2.8302
Batch 100, Loss: 2.8096
Batch 125, Loss: 2.7904
Batch 150, Loss: 2.7721
Batch 175, Loss: 2.7548
Noise applied in 0 out of 1536 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 265.2041654586792 seconds
Epoch 8 accuracy: 12.22%
Batch 25, Loss: 2.7280
Batch 50, Loss: 2.7134
Batch 75, Loss: 2.6999
Batch 100, Loss: 2.6872
Batch 125, Loss: 2.6750
Batch 150, Loss: 2.6632
Batch 175, Loss: 2.6518
Noise applied in 0 out of 1728 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 265.66359281539917 seconds
Epoch 9 accuracy: 12.36%
Batch 25, Loss: 2.6333
Batch 50, Loss: 2.6226
Batch 75, Loss: 2.6122
Batch 100, Loss: 2.6020
Batch 125, Loss: 2.5921
Batch 150, Loss: 2.5825
Batch 175, Loss: 2.5731
Noise applied in 0 out of 1920 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 266.7414073944092 seconds
Epoch 10 accuracy: 12.34%
Batch 25, Loss: 2.5580
Batch 50, Loss: 2.5493
Batch 75, Loss: 2.5407
Batch 100, Loss: 2.5323
Batch 125, Loss: 2.5239
Batch 150, Loss: 2.5155
Batch 175, Loss: 2.5072
Noise applied in 0 out of 2112 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 264.74279141426086 seconds
Epoch 11 accuracy: 12.41%
Batch 25, Loss: 2.4935
Batch 50, Loss: 2.4856
Batch 75, Loss: 2.4777
Batch 100, Loss: 2.4698
Batch 125, Loss: 2.4621
Batch 150, Loss: 2.4545
Batch 175, Loss: 2.4469
Noise applied in 0 out of 2304 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 265.6417717933655 seconds
Epoch 12 accuracy: 12.41%
Batch 25, Loss: 2.4341
Batch 50, Loss: 2.4263
Batch 75, Loss: 2.4185
Batch 100, Loss: 2.4110
Batch 125, Loss: 2.4035
Batch 150, Loss: 2.3958
Batch 175, Loss: 2.3880
Noise applied in 0 out of 2496 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 266.28994512557983 seconds
Epoch 13 accuracy: 12.61%
Batch 25, Loss: 2.3749
Batch 50, Loss: 2.3675
Batch 75, Loss: 2.3603
Batch 100, Loss: 2.3532
Batch 125, Loss: 2.3462
Batch 150, Loss: 2.3395
Batch 175, Loss: 2.3331
Noise applied in 0 out of 2688 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 265.5965483188629 seconds
Epoch 14 accuracy: 12.55%
Batch 25, Loss: 2.3231
Batch 50, Loss: 2.3175
Batch 75, Loss: 2.3119
Batch 100, Loss: 2.3063
Batch 125, Loss: 2.3007
Batch 150, Loss: 2.2952
Batch 175, Loss: 2.2897
Noise applied in 0 out of 2880 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 264.3003749847412 seconds
Epoch 15 accuracy: 12.47%
Batch 25, Loss: 2.2805
Batch 50, Loss: 2.2749
Batch 75, Loss: 2.2690
Batch 100, Loss: 2.2630
Batch 125, Loss: 2.2566
Batch 150, Loss: 2.2499
Batch 175, Loss: 2.2433
Noise applied in 0 out of 3072 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 267.1275894641876 seconds
Epoch 16 accuracy: 12.41%
Batch 25, Loss: 2.2336
Batch 50, Loss: 2.2288
Batch 75, Loss: 2.2246
Batch 100, Loss: 2.2207
Batch 125, Loss: 2.2172
Batch 150, Loss: 2.2137
Batch 175, Loss: 2.2105
Noise applied in 0 out of 3264 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 265.43312668800354 seconds
Epoch 17 accuracy: 12.67%
Batch 25, Loss: 2.2052
Batch 50, Loss: 2.2022
Batch 75, Loss: 2.1993
Batch 100, Loss: 2.1966
Batch 125, Loss: 2.1939
Batch 150, Loss: 2.1913
Batch 175, Loss: 2.1888
Noise applied in 0 out of 3456 batches, 0.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 269.08840131759644 seconds
Epoch 18 accuracy: 12.73%
Batch 25, Loss: 2.1845
Batch 50, Loss: 2.1820
Batch 75, Loss: 2.1795
Batch 100, Loss: 2.1771
Batch 125, Loss: 2.1746
Batch 150, Loss: 2.1721
Batch 175, Loss: 2.1696
Noise applied in 0 out of 3648 batches, 0.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 293.6907320022583 seconds
Epoch 19 accuracy: 12.72%
Batch 25, Loss: 2.1654
Batch 50, Loss: 2.1630
Batch 75, Loss: 2.1607
Batch 100, Loss: 2.1583
Batch 125, Loss: 2.1560
Batch 150, Loss: 2.1537
Batch 175, Loss: 2.1514
Noise applied in 164 out of 3840 batches, 4.27
Epoch 20 learning rate: 0.01
Epoch 20 time: 377.9537410736084 seconds
Epoch 20 accuracy: 12.97%
rho:  0.04 , alpha:  0.3
Total training time: 5510.210067033768 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 7.4858
Norm of the Gradient: 3.3271092176e-01
Smallest Hessian Eigenvalue: -0.0737
Noise Threshold: 0.2
Noise Radius: 0.01
