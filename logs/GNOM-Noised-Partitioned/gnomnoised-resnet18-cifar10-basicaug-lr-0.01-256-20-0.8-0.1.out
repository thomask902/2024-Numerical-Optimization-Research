The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-05-14:04:51
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 72.5934
Batch 50, Loss: 16.6556
Batch 75, Loss: 7.8386
Batch 100, Loss: 6.0554
Batch 125, Loss: 5.2591
Batch 150, Loss: 4.7825
Batch 175, Loss: 4.4440
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 294.67113757133484 seconds
Epoch 1 accuracy: 10.86%
Batch 25, Loss: 4.0336
Batch 50, Loss: 3.8471
Batch 75, Loss: 3.6845
Batch 100, Loss: 3.5371
Batch 125, Loss: 3.4015
Batch 150, Loss: 3.2755
Batch 175, Loss: 3.1580
Noise applied in 0 out of 384 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 262.25392150878906 seconds
Epoch 2 accuracy: 10.85%
Batch 25, Loss: 2.9782
Batch 50, Loss: 2.8805
Batch 75, Loss: 2.7898
Batch 100, Loss: 2.7059
Batch 125, Loss: 2.6291
Batch 150, Loss: 2.5596
Batch 175, Loss: 2.4975
Noise applied in 171 out of 576 batches, 29.69
Epoch 3 learning rate: 0.01
Epoch 3 time: 343.8925576210022 seconds
Epoch 3 accuracy: 10.63%
Batch 25, Loss: 2.4091
Batch 50, Loss: 2.3651
Batch 75, Loss: 2.3264
Batch 100, Loss: 2.2924
Batch 125, Loss: 2.2622
Batch 150, Loss: 2.2348
Batch 175, Loss: 2.2100
Noise applied in 363 out of 768 batches, 47.27
Epoch 4 learning rate: 0.01
Epoch 4 time: 354.2490622997284 seconds
Epoch 4 accuracy: 10.57%
Batch 25, Loss: 2.1726
Batch 50, Loss: 2.1527
Batch 75, Loss: 2.1343
Batch 100, Loss: 2.1172
Batch 125, Loss: 2.1014
Batch 150, Loss: 2.0867
Batch 175, Loss: 2.0732
Noise applied in 555 out of 960 batches, 57.81
Epoch 5 learning rate: 0.01
Epoch 5 time: 354.3087499141693 seconds
Epoch 5 accuracy: 10.58%
Batch 25, Loss: 2.0524
Batch 50, Loss: 2.0411
Batch 75, Loss: 2.0305
Batch 100, Loss: 2.0204
Batch 125, Loss: 2.0110
Batch 150, Loss: 2.0022
Batch 175, Loss: 1.9939
Noise applied in 747 out of 1152 batches, 64.84
Epoch 6 learning rate: 0.01
Epoch 6 time: 354.65414547920227 seconds
Epoch 6 accuracy: 10.41%
Batch 25, Loss: 1.9810
Batch 50, Loss: 1.9739
Batch 75, Loss: 1.9673
Batch 100, Loss: 1.9610
Batch 125, Loss: 1.9549
Batch 150, Loss: 1.9493
Batch 175, Loss: 1.9439
Noise applied in 939 out of 1344 batches, 69.87
Epoch 7 learning rate: 0.01
Epoch 7 time: 355.2049653530121 seconds
Epoch 7 accuracy: 10.59%
Batch 25, Loss: 1.9354
Batch 50, Loss: 1.9307
Batch 75, Loss: 1.9262
Batch 100, Loss: 1.9219
Batch 125, Loss: 1.9178
Batch 150, Loss: 1.9138
Batch 175, Loss: 1.9099
Noise applied in 1131 out of 1536 batches, 73.63
Epoch 8 learning rate: 0.01
Epoch 8 time: 355.6768867969513 seconds
Epoch 8 accuracy: 10.47%
Batch 25, Loss: 1.9037
Batch 50, Loss: 1.9002
Batch 75, Loss: 1.8969
Batch 100, Loss: 1.8936
Batch 125, Loss: 1.8905
Batch 150, Loss: 1.8875
Batch 175, Loss: 1.8846
Noise applied in 1323 out of 1728 batches, 76.56
Epoch 9 learning rate: 0.01
Epoch 9 time: 355.1458578109741 seconds
Epoch 9 accuracy: 10.17%
Batch 25, Loss: 1.8798
Batch 50, Loss: 1.8771
Batch 75, Loss: 1.8745
Batch 100, Loss: 1.8720
Batch 125, Loss: 1.8696
Batch 150, Loss: 1.8672
Batch 175, Loss: 1.8650
Noise applied in 1515 out of 1920 batches, 78.91
Epoch 10 learning rate: 0.01
Epoch 10 time: 354.7381052970886 seconds
Epoch 10 accuracy: 10.55%
Batch 25, Loss: 1.8613
Batch 50, Loss: 1.8592
Batch 75, Loss: 1.8572
Batch 100, Loss: 1.8552
Batch 125, Loss: 1.8532
Batch 150, Loss: 1.8513
Batch 175, Loss: 1.8494
Noise applied in 1707 out of 2112 batches, 80.82
Epoch 11 learning rate: 0.01
Epoch 11 time: 354.8560252189636 seconds
Epoch 11 accuracy: 10.86%
Batch 25, Loss: 1.8463
Batch 50, Loss: 1.8445
Batch 75, Loss: 1.8428
Batch 100, Loss: 1.8411
Batch 125, Loss: 1.8394
Batch 150, Loss: 1.8378
Batch 175, Loss: 1.8362
Noise applied in 1899 out of 2304 batches, 82.42
Epoch 12 learning rate: 0.01
Epoch 12 time: 355.3072624206543 seconds
Epoch 12 accuracy: 11.44%
Batch 25, Loss: 1.8336
Batch 50, Loss: 1.8321
Batch 75, Loss: 1.8306
Batch 100, Loss: 1.8292
Batch 125, Loss: 1.8278
Batch 150, Loss: 1.8263
Batch 175, Loss: 1.8250
Noise applied in 2091 out of 2496 batches, 83.77
Epoch 13 learning rate: 0.01
Epoch 13 time: 355.14066672325134 seconds
Epoch 13 accuracy: 11.76%
Batch 25, Loss: 1.8227
Batch 50, Loss: 1.8214
Batch 75, Loss: 1.8201
Batch 100, Loss: 1.8189
Batch 125, Loss: 1.8176
Batch 150, Loss: 1.8164
Batch 175, Loss: 1.8152
Noise applied in 2283 out of 2688 batches, 84.93
Epoch 14 learning rate: 0.01
Epoch 14 time: 375.34836411476135 seconds
Epoch 14 accuracy: 11.94%
Batch 25, Loss: 1.8132
Batch 50, Loss: 1.8121
Batch 75, Loss: 1.8109
Batch 100, Loss: 1.8098
Batch 125, Loss: 1.8087
Batch 150, Loss: 1.8077
Batch 175, Loss: 1.8066
Noise applied in 2475 out of 2880 batches, 85.94
Epoch 15 learning rate: 0.01
Epoch 15 time: 388.7116506099701 seconds
Epoch 15 accuracy: 12.17%
Batch 25, Loss: 1.8049
Batch 50, Loss: 1.8039
Batch 75, Loss: 1.8029
Batch 100, Loss: 1.8019
Batch 125, Loss: 1.8009
Batch 150, Loss: 1.8000
Batch 175, Loss: 1.7990
Noise applied in 2667 out of 3072 batches, 86.82
Epoch 16 learning rate: 0.01
Epoch 16 time: 355.5341258049011 seconds
Epoch 16 accuracy: 12.18%
Batch 25, Loss: 1.7975
Batch 50, Loss: 1.7965
Batch 75, Loss: 1.7956
Batch 100, Loss: 1.7948
Batch 125, Loss: 1.7939
Batch 150, Loss: 1.7931
Batch 175, Loss: 1.7922
Noise applied in 2859 out of 3264 batches, 87.59
Epoch 17 learning rate: 0.01
Epoch 17 time: 355.17047095298767 seconds
Epoch 17 accuracy: 12.24%
Batch 25, Loss: 1.7908
Batch 50, Loss: 1.7901
Batch 75, Loss: 1.7893
Batch 100, Loss: 1.7885
Batch 125, Loss: 1.7878
Batch 150, Loss: 1.7870
Batch 175, Loss: 1.7863
Noise applied in 3051 out of 3456 batches, 88.28
Epoch 18 learning rate: 0.01
Epoch 18 time: 407.2627441883087 seconds
Epoch 18 accuracy: 12.21%
Batch 25, Loss: 1.7851
Batch 50, Loss: 1.7844
Batch 75, Loss: 1.7837
Batch 100, Loss: 1.7830
Batch 125, Loss: 1.7823
Batch 150, Loss: 1.7816
Batch 175, Loss: 1.7809
Noise applied in 3243 out of 3648 batches, 88.90
Epoch 19 learning rate: 0.01
Epoch 19 time: 357.4518163204193 seconds
Epoch 19 accuracy: 12.15%
Batch 25, Loss: 1.7798
Batch 50, Loss: 1.7791
Batch 75, Loss: 1.7785
Batch 100, Loss: 1.7779
Batch 125, Loss: 1.7773
Batch 150, Loss: 1.7767
Batch 175, Loss: 1.7761
Noise applied in 3435 out of 3840 batches, 89.45
Epoch 20 learning rate: 0.01
Epoch 20 time: 358.46007323265076 seconds
Epoch 20 accuracy: 12.25%
rho:  0.04 , alpha:  0.3
Total training time: 7048.055264472961 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 1.2060
Norm of the Gradient: 6.5193521976e-01
Smallest Hessian Eigenvalue: -0.1686
Noise Threshold: 0.8
Noise Radius: 0.1
