The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:223: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.1/batchsize-256/2024-08-18-16:59:11
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 100, Loss: 769.5882
Noise applied in 0 out of 196 batches, 0.00
Epoch 1 learning rate: 0.1
Epoch 1 time: 124.04736733436584 seconds
Epoch 1 accuracy: 11.61%
Batch 100, Loss: 58.3661
Noise applied in 0 out of 392 batches, 0.00
Epoch 2 learning rate: 0.1
Epoch 2 time: 111.7194082736969 seconds
Epoch 2 accuracy: 11.12%
Batch 100, Loss: 33.8700
Noise applied in 0 out of 588 batches, 0.00
Epoch 3 learning rate: 0.1
Epoch 3 time: 111.73067903518677 seconds
Epoch 3 accuracy: 11.44%
Batch 100, Loss: 27.6665
Noise applied in 0 out of 784 batches, 0.00
Epoch 4 learning rate: 0.1
Epoch 4 time: 111.67831444740295 seconds
Epoch 4 accuracy: 11.39%
Batch 100, Loss: 23.5648
Noise applied in 0 out of 980 batches, 0.00
Epoch 5 learning rate: 0.1
Epoch 5 time: 111.73731994628906 seconds
Epoch 5 accuracy: 11.64%
Batch 100, Loss: 19.8337
Noise applied in 0 out of 1176 batches, 0.00
Epoch 6 learning rate: 0.1
Epoch 6 time: 111.66097450256348 seconds
Epoch 6 accuracy: 11.71%
Batch 100, Loss: 16.8856
Noise applied in 0 out of 1372 batches, 0.00
Epoch 7 learning rate: 0.1
Epoch 7 time: 111.66962885856628 seconds
Epoch 7 accuracy: 11.82%
Batch 100, Loss: 14.2562
Noise applied in 0 out of 1568 batches, 0.00
Epoch 8 learning rate: 0.1
Epoch 8 time: 111.72716641426086 seconds
Epoch 8 accuracy: 12.04%
Batch 100, Loss: 12.0799
Noise applied in 0 out of 1764 batches, 0.00
Epoch 9 learning rate: 0.1
Epoch 9 time: 111.63491559028625 seconds
Epoch 9 accuracy: 12.48%
Batch 100, Loss: 10.8161
Noise applied in 0 out of 1960 batches, 0.00
Epoch 10 learning rate: 0.1
Epoch 10 time: 111.70867109298706 seconds
Epoch 10 accuracy: 12.35%
Batch 100, Loss: 9.6041
Noise applied in 0 out of 2156 batches, 0.00
Epoch 11 learning rate: 0.1
Epoch 11 time: 111.66563391685486 seconds
Epoch 11 accuracy: 11.89%
Batch 100, Loss: 8.4685
Noise applied in 0 out of 2352 batches, 0.00
Epoch 12 learning rate: 0.1
Epoch 12 time: 111.64852952957153 seconds
Epoch 12 accuracy: 12.23%
Batch 100, Loss: 7.2869
Noise applied in 0 out of 2548 batches, 0.00
Epoch 13 learning rate: 0.1
Epoch 13 time: 111.62631702423096 seconds
Epoch 13 accuracy: 11.86%
Batch 100, Loss: 6.0381
Noise applied in 0 out of 2744 batches, 0.00
Epoch 14 learning rate: 0.1
Epoch 14 time: 111.69506740570068 seconds
Epoch 14 accuracy: 12.39%
Batch 100, Loss: 4.4365
Noise applied in 122 out of 2940 batches, 4.15
Epoch 15 learning rate: 0.1
Epoch 15 time: 182.3720681667328 seconds
Epoch 15 accuracy: 13.08%
Batch 100, Loss: 3.5074
Noise applied in 312 out of 3136 batches, 9.95
Epoch 16 learning rate: 0.1
Epoch 16 time: 218.66509914398193 seconds
Epoch 16 accuracy: 12.49%
Batch 100, Loss: 2.9362
Noise applied in 508 out of 3332 batches, 15.25
Epoch 17 learning rate: 0.1
Epoch 17 time: 221.9305181503296 seconds
Epoch 17 accuracy: 12.58%
Batch 100, Loss: 2.5891
Noise applied in 704 out of 3528 batches, 19.95
Epoch 18 learning rate: 0.1
Epoch 18 time: 222.05977034568787 seconds
Epoch 18 accuracy: 11.88%
Batch 100, Loss: 2.3764
Noise applied in 900 out of 3724 batches, 24.17
Epoch 19 learning rate: 0.1
Epoch 19 time: 221.80644297599792 seconds
Epoch 19 accuracy: 11.51%
Batch 100, Loss: 2.2173
Noise applied in 1096 out of 3920 batches, 27.96
Epoch 20 learning rate: 0.1
Epoch 20 time: 221.60252141952515 seconds
Epoch 20 accuracy: 11.41%
rho:  0.04 , alpha:  0.3
Total training time: 2864.40363574028 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.2635
Norm of the Gradient: 2.4483685195e-01
Smallest Hessian Eigenvalue: -0.0700
Noise Threshold: 0.5
Noise Radius: 0.1
