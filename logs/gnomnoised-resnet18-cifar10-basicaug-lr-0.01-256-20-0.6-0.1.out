The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-18:15:19
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 3069.7345
Batch 50, Loss: 2160.9324
Batch 75, Loss: 324.3691
Batch 100, Loss: 158.3522
Batch 125, Loss: 109.8161
Batch 150, Loss: 85.2545
Batch 175, Loss: 69.6109
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 282.820739030838 seconds
Epoch 1 accuracy: 9.97%
Batch 25, Loss: 51.1160
Batch 50, Loss: 42.7939
Batch 75, Loss: 36.3120
Batch 100, Loss: 31.0675
Batch 125, Loss: 26.6195
Batch 150, Loss: 23.0105
Batch 175, Loss: 20.1722
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 277.81547117233276 seconds
Epoch 2 accuracy: 11.25%
Batch 25, Loss: 16.5797
Batch 50, Loss: 14.9701
Batch 75, Loss: 13.6901
Batch 100, Loss: 12.6874
Batch 125, Loss: 11.8962
Batch 150, Loss: 11.2564
Batch 175, Loss: 10.7313
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 277.948459148407 seconds
Epoch 3 accuracy: 10.98%
Batch 25, Loss: 10.0271
Batch 50, Loss: 9.6903
Batch 75, Loss: 9.4004
Batch 100, Loss: 9.1496
Batch 125, Loss: 8.9304
Batch 150, Loss: 8.7359
Batch 175, Loss: 8.5615
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 277.51130080223083 seconds
Epoch 4 accuracy: 10.78%
Batch 25, Loss: 8.3057
Batch 50, Loss: 8.1712
Batch 75, Loss: 8.0473
Batch 100, Loss: 7.9325
Batch 125, Loss: 7.8256
Batch 150, Loss: 7.7253
Batch 175, Loss: 7.6305
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 276.56186294555664 seconds
Epoch 5 accuracy: 10.56%
Batch 25, Loss: 7.4815
Batch 50, Loss: 7.3987
Batch 75, Loss: 7.3197
Batch 100, Loss: 7.2441
Batch 125, Loss: 7.1715
Batch 150, Loss: 7.1015
Batch 175, Loss: 7.0341
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 276.24152731895447 seconds
Epoch 6 accuracy: 10.6%
Batch 25, Loss: 6.9261
Batch 50, Loss: 6.8643
Batch 75, Loss: 6.8042
Batch 100, Loss: 6.7459
Batch 125, Loss: 6.6893
Batch 150, Loss: 6.6344
Batch 175, Loss: 6.5810
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 277.0644528865814 seconds
Epoch 7 accuracy: 10.66%
Batch 25, Loss: 6.4944
Batch 50, Loss: 6.4446
Batch 75, Loss: 6.3964
Batch 100, Loss: 6.3494
Batch 125, Loss: 6.3035
Batch 150, Loss: 6.2586
Batch 175, Loss: 6.2146
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 280.04569721221924 seconds
Epoch 8 accuracy: 10.67%
Batch 25, Loss: 6.1423
Batch 50, Loss: 6.1001
Batch 75, Loss: 6.0585
Batch 100, Loss: 6.0177
Batch 125, Loss: 5.9775
Batch 150, Loss: 5.9382
Batch 175, Loss: 5.8994
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 281.51520895957947 seconds
Epoch 9 accuracy: 10.85%
Batch 25, Loss: 5.8353
Batch 50, Loss: 5.7978
Batch 75, Loss: 5.7608
Batch 100, Loss: 5.7242
Batch 125, Loss: 5.6880
Batch 150, Loss: 5.6523
Batch 175, Loss: 5.6170
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 285.40503120422363 seconds
Epoch 10 accuracy: 10.86%
Batch 25, Loss: 5.5583
Batch 50, Loss: 5.5238
Batch 75, Loss: 5.4894
Batch 100, Loss: 5.4554
Batch 125, Loss: 5.4217
Batch 150, Loss: 5.3881
Batch 175, Loss: 5.3548
Noise applied in 0 out of 192 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 282.39920687675476 seconds
Epoch 11 accuracy: 10.9%
Batch 25, Loss: 5.2994
Batch 50, Loss: 5.2666
Batch 75, Loss: 5.2339
Batch 100, Loss: 5.2014
Batch 125, Loss: 5.1689
Batch 150, Loss: 5.1366
Batch 175, Loss: 5.1044
Noise applied in 0 out of 192 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 294.79696345329285 seconds
Epoch 12 accuracy: 10.91%
Batch 25, Loss: 5.0508
Batch 50, Loss: 5.0191
Batch 75, Loss: 4.9877
Batch 100, Loss: 4.9565
Batch 125, Loss: 4.9254
Batch 150, Loss: 4.8944
Batch 175, Loss: 4.8636
Noise applied in 0 out of 192 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 316.98769760131836 seconds
Epoch 13 accuracy: 10.96%
Batch 25, Loss: 4.8118
Batch 50, Loss: 4.7811
Batch 75, Loss: 4.7505
Batch 100, Loss: 4.7198
Batch 125, Loss: 4.6892
Batch 150, Loss: 4.6585
Batch 175, Loss: 4.6280
Noise applied in 0 out of 192 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 283.38005900382996 seconds
Epoch 14 accuracy: 10.98%
Batch 25, Loss: 4.5769
Batch 50, Loss: 4.5463
Batch 75, Loss: 4.5158
Batch 100, Loss: 4.4851
Batch 125, Loss: 4.4543
Batch 150, Loss: 4.4234
Batch 175, Loss: 4.3923
Noise applied in 0 out of 192 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 280.60269951820374 seconds
Epoch 15 accuracy: 11.0%
Batch 25, Loss: 4.3398
Batch 50, Loss: 4.3083
Batch 75, Loss: 4.2766
Batch 100, Loss: 4.2447
Batch 125, Loss: 4.2125
Batch 150, Loss: 4.1801
Batch 175, Loss: 4.1474
Noise applied in 0 out of 192 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 282.37955021858215 seconds
Epoch 16 accuracy: 11.04%
Batch 25, Loss: 4.0916
Batch 50, Loss: 4.0578
Batch 75, Loss: 4.0235
Batch 100, Loss: 3.9886
Batch 125, Loss: 3.9533
Batch 150, Loss: 3.9174
Batch 175, Loss: 3.8808
Noise applied in 0 out of 192 batches, 0.00
Epoch 17 learning rate: 0.01
Epoch 17 time: 283.96040081977844 seconds
Epoch 17 accuracy: 10.99%
Batch 25, Loss: 3.8179
Batch 50, Loss: 3.7793
Batch 75, Loss: 3.7396
Batch 100, Loss: 3.6989
Batch 125, Loss: 3.6570
Batch 150, Loss: 3.6141
Batch 175, Loss: 3.5701
Noise applied in 27 out of 192 batches, 14.06
Epoch 18 learning rate: 0.01
Epoch 18 time: 302.4827435016632 seconds
Epoch 18 accuracy: 10.95%
Batch 25, Loss: 3.4930
Batch 50, Loss: 3.4451
Batch 75, Loss: 3.3958
Batch 100, Loss: 3.3454
Batch 125, Loss: 3.2936
Batch 150, Loss: 3.2409
Batch 175, Loss: 3.1872
Noise applied in 275 out of 192 batches, 143.23
Epoch 19 learning rate: 0.01
Epoch 19 time: 427.9762673377991 seconds
Epoch 19 accuracy: 10.97%
Batch 25, Loss: 3.0958
Batch 50, Loss: 3.0421
Batch 75, Loss: 2.9890
Batch 100, Loss: 2.9377
Batch 125, Loss: 2.8891
Batch 150, Loss: 2.8441
Batch 175, Loss: 2.8028
Noise applied in 384 out of 192 batches, 200.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 495.47596406936646 seconds
Epoch 20 accuracy: 11.04%
rho:  0.04 , alpha:  0.3
Total training time: 6043.400996208191 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 2.8879
Norm of the Gradient: 5.2831852436e-01
Smallest Hessian Eigenvalue: -0.1781
Noise Threshold: 0.6
Noise Radius: 0.1
