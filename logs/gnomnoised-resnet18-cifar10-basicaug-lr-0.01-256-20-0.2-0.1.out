The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig        6)  ucx/1.14.1         11) flexiblas/3.3.1
  2) gentoo/2023     7)  libfabric/1.18.0   12) imkl/2023.2.0
  3) gcccore/.12.3   8)  pmix/4.2.4         13) StdEnv/2023
  4) gcc/12.3        9)  ucc/1.2.0
  5) hwloc/2.9.1     10) openmpi/4.1.5
/project/6070520/tkleinkn/Vanilla-GAM/main_cifar.py:225: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
model name space ['resnet101_c', 'resnet152_c', 'resnet18_c', 'resnet34_c', 'resnet50_c']
Use GPU: 0 for training
=> creating model 'resnet18_c'
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Gradient Approximation Dataset: 1024
Train Dataset: 48976
tensorboard dir ./results/CIFAR10/GNOM_noised/basicaug/lr-0.01/batchsize-256/2024-09-04-18:10:14
Using Gradient-Norm Only Minimization with noise (GNOM_noised)
Gradient Approximation Samples: 1024
Number of Gradient Approximation Accumulation Batches: 4
/home/tkleinkn/GAMtest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /home/coulombc/wheels_builder/tmp.2617/python-3.11/torch/torch/csrc/autograd/engine.cpp:1203.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Batch 25, Loss: 76.7961
Batch 50, Loss: 43.5474
Batch 75, Loss: 17.9824
Batch 100, Loss: 12.2372
Batch 125, Loss: 10.0215
Batch 150, Loss: 8.6292
Batch 175, Loss: 7.6018
Noise applied in 0 out of 192 batches, 0.00
Epoch 1 learning rate: 0.01
Epoch 1 time: 302.6385521888733 seconds
Epoch 1 accuracy: 12.16%
Batch 25, Loss: 6.3347
Batch 50, Loss: 5.7481
Batch 75, Loss: 5.2511
Batch 100, Loss: 4.8304
Batch 125, Loss: 4.4756
Batch 150, Loss: 4.1716
Batch 175, Loss: 3.9076
Noise applied in 0 out of 192 batches, 0.00
Epoch 2 learning rate: 0.01
Epoch 2 time: 273.3138220310211 seconds
Epoch 2 accuracy: 11.4%
Batch 25, Loss: 3.5293
Batch 50, Loss: 3.3274
Batch 75, Loss: 3.1743
Batch 100, Loss: 3.0780
Batch 125, Loss: 3.0038
Batch 150, Loss: 2.9425
Batch 175, Loss: 2.8887
Noise applied in 0 out of 192 batches, 0.00
Epoch 3 learning rate: 0.01
Epoch 3 time: 273.41056299209595 seconds
Epoch 3 accuracy: 13.6%
Batch 25, Loss: 2.8117
Batch 50, Loss: 2.7720
Batch 75, Loss: 2.7359
Batch 100, Loss: 2.7032
Batch 125, Loss: 2.6737
Batch 150, Loss: 2.6468
Batch 175, Loss: 2.6223
Noise applied in 0 out of 192 batches, 0.00
Epoch 4 learning rate: 0.01
Epoch 4 time: 274.4308512210846 seconds
Epoch 4 accuracy: 13.49%
Batch 25, Loss: 2.5855
Batch 50, Loss: 2.5658
Batch 75, Loss: 2.5475
Batch 100, Loss: 2.5304
Batch 125, Loss: 2.5143
Batch 150, Loss: 2.4991
Batch 175, Loss: 2.4848
Noise applied in 0 out of 192 batches, 0.00
Epoch 5 learning rate: 0.01
Epoch 5 time: 272.3515508174896 seconds
Epoch 5 accuracy: 13.66%
Batch 25, Loss: 2.4623
Batch 50, Loss: 2.4498
Batch 75, Loss: 2.4378
Batch 100, Loss: 2.4264
Batch 125, Loss: 2.4155
Batch 150, Loss: 2.4053
Batch 175, Loss: 2.3954
Noise applied in 0 out of 192 batches, 0.00
Epoch 6 learning rate: 0.01
Epoch 6 time: 273.3772315979004 seconds
Epoch 6 accuracy: 13.79%
Batch 25, Loss: 2.3798
Batch 50, Loss: 2.3710
Batch 75, Loss: 2.3627
Batch 100, Loss: 2.3547
Batch 125, Loss: 2.3469
Batch 150, Loss: 2.3394
Batch 175, Loss: 2.3321
Noise applied in 0 out of 192 batches, 0.00
Epoch 7 learning rate: 0.01
Epoch 7 time: 273.25909900665283 seconds
Epoch 7 accuracy: 7.96%
Batch 25, Loss: 2.3205
Batch 50, Loss: 2.3140
Batch 75, Loss: 2.3076
Batch 100, Loss: 2.3015
Batch 125, Loss: 2.2956
Batch 150, Loss: 2.2899
Batch 175, Loss: 2.2844
Noise applied in 0 out of 192 batches, 0.00
Epoch 8 learning rate: 0.01
Epoch 8 time: 274.68146991729736 seconds
Epoch 8 accuracy: 8.23%
Batch 25, Loss: 2.2754
Batch 50, Loss: 2.2703
Batch 75, Loss: 2.2653
Batch 100, Loss: 2.2604
Batch 125, Loss: 2.2557
Batch 150, Loss: 2.2511
Batch 175, Loss: 2.2466
Noise applied in 0 out of 192 batches, 0.00
Epoch 9 learning rate: 0.01
Epoch 9 time: 276.50441551208496 seconds
Epoch 9 accuracy: 8.33%
Batch 25, Loss: 2.2392
Batch 50, Loss: 2.2350
Batch 75, Loss: 2.2309
Batch 100, Loss: 2.2269
Batch 125, Loss: 2.2229
Batch 150, Loss: 2.2191
Batch 175, Loss: 2.2153
Noise applied in 0 out of 192 batches, 0.00
Epoch 10 learning rate: 0.01
Epoch 10 time: 278.58431124687195 seconds
Epoch 10 accuracy: 8.37%
Batch 25, Loss: 2.2091
Batch 50, Loss: 2.2056
Batch 75, Loss: 2.2021
Batch 100, Loss: 2.1986
Batch 125, Loss: 2.1952
Batch 150, Loss: 2.1919
Batch 175, Loss: 2.1886
Noise applied in 0 out of 192 batches, 0.00
Epoch 11 learning rate: 0.01
Epoch 11 time: 280.128457069397 seconds
Epoch 11 accuracy: 8.37%
Batch 25, Loss: 2.1832
Batch 50, Loss: 2.1801
Batch 75, Loss: 2.1770
Batch 100, Loss: 2.1740
Batch 125, Loss: 2.1710
Batch 150, Loss: 2.1681
Batch 175, Loss: 2.1652
Noise applied in 0 out of 192 batches, 0.00
Epoch 12 learning rate: 0.01
Epoch 12 time: 278.5456097126007 seconds
Epoch 12 accuracy: 8.32%
Batch 25, Loss: 2.1604
Batch 50, Loss: 2.1577
Batch 75, Loss: 2.1550
Batch 100, Loss: 2.1523
Batch 125, Loss: 2.1497
Batch 150, Loss: 2.1471
Batch 175, Loss: 2.1445
Noise applied in 0 out of 192 batches, 0.00
Epoch 13 learning rate: 0.01
Epoch 13 time: 290.6816945075989 seconds
Epoch 13 accuracy: 8.3%
Batch 25, Loss: 2.1403
Batch 50, Loss: 2.1378
Batch 75, Loss: 2.1354
Batch 100, Loss: 2.1330
Batch 125, Loss: 2.1306
Batch 150, Loss: 2.1283
Batch 175, Loss: 2.1259
Noise applied in 0 out of 192 batches, 0.00
Epoch 14 learning rate: 0.01
Epoch 14 time: 313.3590886592865 seconds
Epoch 14 accuracy: 8.34%
Batch 25, Loss: 2.1221
Batch 50, Loss: 2.1199
Batch 75, Loss: 2.1177
Batch 100, Loss: 2.1155
Batch 125, Loss: 2.1134
Batch 150, Loss: 2.1112
Batch 175, Loss: 2.1092
Noise applied in 0 out of 192 batches, 0.00
Epoch 15 learning rate: 0.01
Epoch 15 time: 283.33714985847473 seconds
Epoch 15 accuracy: 14.31%
Batch 25, Loss: 2.1057
Batch 50, Loss: 2.1037
Batch 75, Loss: 2.1017
Batch 100, Loss: 2.0997
Batch 125, Loss: 2.0978
Batch 150, Loss: 2.0959
Batch 175, Loss: 2.0940
Noise applied in 0 out of 192 batches, 0.00
Epoch 16 learning rate: 0.01
Epoch 16 time: 277.76522850990295 seconds
Epoch 16 accuracy: 14.12%
Batch 25, Loss: 2.0909
Batch 50, Loss: 2.0890
Batch 75, Loss: 2.0872
Batch 100, Loss: 2.0854
Batch 125, Loss: 2.0837
Batch 150, Loss: 2.0820
Batch 175, Loss: 2.0802
Noise applied in 106 out of 192 batches, 55.21
Epoch 17 learning rate: 0.01
Epoch 17 time: 340.3811676502228 seconds
Epoch 17 accuracy: 13.85%
Batch 25, Loss: 2.0773
Batch 50, Loss: 2.0757
Batch 75, Loss: 2.0741
Batch 100, Loss: 2.0724
Batch 125, Loss: 2.0707
Batch 150, Loss: 2.0691
Batch 175, Loss: 2.0674
Noise applied in 192 out of 192 batches, 100.00
Epoch 18 learning rate: 0.01
Epoch 18 time: 396.57387948036194 seconds
Epoch 18 accuracy: 13.67%
Batch 25, Loss: 2.0648
Batch 50, Loss: 2.0632
Batch 75, Loss: 2.0617
Batch 100, Loss: 2.0602
Batch 125, Loss: 2.0586
Batch 150, Loss: 2.0572
Batch 175, Loss: 2.0557
Noise applied in 192 out of 192 batches, 100.00
Epoch 19 learning rate: 0.01
Epoch 19 time: 403.7988679409027 seconds
Epoch 19 accuracy: 13.6%
Batch 25, Loss: 2.0532
Batch 50, Loss: 2.0517
Batch 75, Loss: 2.0502
Batch 100, Loss: 2.0487
Batch 125, Loss: 2.0473
Batch 150, Loss: 2.0459
Batch 175, Loss: 2.0445
Noise applied in 192 out of 192 batches, 100.00
Epoch 20 learning rate: 0.01
Epoch 20 time: 392.0811285972595 seconds
Epoch 20 accuracy: 13.52%
rho:  0.04 , alpha:  0.3
Total training time: 6029.232927799225 seconds
/project/6070520/tkleinkn/Vanilla-GAM/utils/density_plot.py:68: ComplexWarning: Casting complex values to real discards the imaginary part
  density_output[i, j] = np.sum(tmp_result * weights[i, :])
Largest Hessian Eigenvalue: 0.4908
Norm of the Gradient: 2.5851950049e-01
Smallest Hessian Eigenvalue: -0.0965
Noise Threshold: 0.2
Noise Radius: 0.1
