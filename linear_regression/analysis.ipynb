{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNOM Testing with Classical ML - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 Epochs, Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_file_path = \"wine/GD/lr-0.01/200/no_batching/2024-09-20-14:36:32/results.csv\"\n",
    "\n",
    "GD_stats = pd.read_csv(GD_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNOM_file_path = \"wine/GNOM/lr-0.01/200/no_batching/2024-09-20-14:35:46/results.csv\"\n",
    "GNOM_stats = pd.read_csv(GNOM_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with batch size = 1\n",
    "SGD_1_file = \"wine/GD/lr-0.01/200/1/2024-09-20-14:50:48/results.csv\"\n",
    "SGD_1 = pd.read_csv(SGD_1_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with batch size = 32\n",
    "SGD_32_file = \"wine/GD/lr-0.01/200/32/2024-09-20-14:40:37/results.csv\"\n",
    "SGD_32 = pd.read_csv(SGD_32_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with batch size = 128\n",
    "SGD_128_file = \"wine/GD/lr-0.01/200/128/2024-09-20-14:40:13/results.csv\"\n",
    "SGD_128 = pd.read_csv(SGD_128_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with batch size = 512\n",
    "SGD_512_file = \"wine/GD/lr-0.01/200/512/2024-09-20-14:39:58/results.csv\"\n",
    "SGD_512 = pd.read_csv(SGD_512_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 Epochs, Learning Rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_file_path = \"wine/GD/lr-0.01/200/2024-09-18-16:15:08/results.csv\"\n",
    "\n",
    "GD_stats = pd.read_csv(GD_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNOM_file_path = \"wine/GNOM/lr-0.01/200/2024-09-18-16:18:09/results.csv\"\n",
    "GNOM_stats = pd.read_csv(GNOM_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNOM_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 500 Epochs, Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_file_path = \"wine/GD/lr-0.01/500/2024-09-18-16:34:38/results.csv\"\n",
    "GD_stats = pd.read_csv(GD_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNOM_file_path = \"wine/GNOM/lr-0.01/500/2024-09-18-16:33:30/results.csv\"\n",
    "GNOM_stats = pd.read_csv(GNOM_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD vs. GNOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from DataFrames\n",
    "epochs = GD_stats['Epoch']\n",
    "num_epochs = len(epochs)\n",
    "\n",
    "GD_test_loss = GD_stats['Test Loss']\n",
    "GNOM_test_loss = GNOM_stats['Test Loss']\n",
    "\n",
    "GD_training_loss = GD_stats['Training Loss']\n",
    "GNOM_training_loss = GNOM_stats['Training Loss']\n",
    "\n",
    "GD_train_norm = GD_stats['Training Gradient Norm']\n",
    "GNOM_train_norm = GNOM_stats['Training Gradient Norm']\n",
    "\n",
    "GD_test_norm = GD_stats['Test Gradient Norm']\n",
    "GNOM_test_norm = GNOM_stats['Test Gradient Norm']\n",
    "\n",
    "# Calculate average training time (expanding mean)\n",
    "GD_avg_training_time = GD_stats['Training Time (s)'].expanding().mean()\n",
    "GNOM_avg_training_time = GNOM_stats['Training Time (s)'].expanding().mean()\n",
    "\n",
    "# Calculate total training time (expanding mean)\n",
    "GD_total_training_time = GD_stats['Training Time (s)'].expanding().sum()\n",
    "GNOM_total_training_time = GNOM_stats['Training Time (s)'].expanding().sum()\n",
    "\n",
    "# Create subplots (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Test Loss\n",
    "axes[0, 0].plot(epochs, GD_test_loss, label='GD', color='blue')\n",
    "axes[0, 0].plot(epochs, GNOM_test_loss, label='GNOM', color='red')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Test Loss (MSE)')\n",
    "axes[0, 0].set_title(f'Test Loss over {num_epochs} Epochs')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "axes[0, 1].plot(epochs, GD_training_loss, label='GD', color='blue')\n",
    "axes[0, 1].plot(epochs, GNOM_training_loss, label='GNOM', color='red')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Training Loss')\n",
    "axes[0, 1].set_title(f'Training Loss over {num_epochs} Epochs')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Training Gradient Norm\n",
    "axes[1, 1].plot(epochs, GD_train_norm, label='GD', color='blue')\n",
    "axes[1, 1].plot(epochs, GNOM_train_norm, label='GNOM', color='red')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Gradient Norm')\n",
    "axes[1, 1].set_title(f'Training Gradient Norm over {num_epochs} Epochs')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Plot 4: Test Gradient Norm\n",
    "axes[1, 0].plot(epochs, GD_test_norm, label='GD', color='blue')\n",
    "axes[1, 0].plot(epochs, GNOM_test_norm, label='GNOM', color='red')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title(f'Test Gradient Norm over {num_epochs} Epochs')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 5: Average Training Time\n",
    "axes[2, 0].plot(epochs, GD_avg_training_time, label='GD', color='blue')\n",
    "axes[2, 0].plot(epochs, GNOM_avg_training_time, label='GNOM', color='red')\n",
    "axes[2, 0].set_xlabel('Epoch')\n",
    "axes[2, 0].set_ylabel('Average Training Time Per Epoch (s)')\n",
    "axes[2, 0].set_title(f'Average Epoch Training Time over {num_epochs} Epochs')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# Plot 5: Average Training Time\n",
    "axes[2, 1].plot(epochs, GD_total_training_time, label='GD', color='blue')\n",
    "axes[2, 1].plot(epochs, GNOM_total_training_time, label='GNOM', color='red')\n",
    "axes[2, 1].set_xlabel('Epoch')\n",
    "axes[2, 1].set_ylabel('Total Training Time (s)')\n",
    "axes[2, 1].set_title(f'Total Training Time over {num_epochs} Epochs')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD vs. GNOM vs. SGD with batch E {1, 32, 128, 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for various SGD batch sizes\n",
    "SGD_1_test_loss = SGD_1['Test Loss']\n",
    "SGD_32_test_loss = SGD_32['Test Loss']\n",
    "SGD_128_test_loss = SGD_128['Test Loss']\n",
    "SGD_512_test_loss = SGD_512['Test Loss']\n",
    "\n",
    "SGD_1_training_loss = SGD_1['Training Loss']\n",
    "SGD_32_training_loss = SGD_32['Training Loss']\n",
    "SGD_128_training_loss = SGD_128['Training Loss']\n",
    "SGD_512_training_loss = SGD_512['Training Loss']\n",
    "\n",
    "SGD_1_train_norm = SGD_1['Training Gradient Norm']\n",
    "SGD_32_train_norm = SGD_32['Training Gradient Norm']\n",
    "SGD_128_train_norm = SGD_128['Training Gradient Norm']\n",
    "SGD_512_train_norm = SGD_512['Training Gradient Norm']\n",
    "\n",
    "SGD_1_test_norm = SGD_1['Test Gradient Norm']\n",
    "SGD_32_test_norm = SGD_32['Test Gradient Norm']\n",
    "SGD_128_test_norm = SGD_128['Test Gradient Norm']\n",
    "SGD_512_test_norm = SGD_512['Test Gradient Norm']\n",
    "\n",
    "# Calculate average training time (expanding mean) for SGD variants\n",
    "SGD_1_avg_training_time = SGD_1['Training Time (s)'].expanding().mean()\n",
    "SGD_32_avg_training_time = SGD_32['Training Time (s)'].expanding().mean()\n",
    "SGD_128_avg_training_time = SGD_128['Training Time (s)'].expanding().mean()\n",
    "SGD_512_avg_training_time = SGD_512['Training Time (s)'].expanding().mean()\n",
    "\n",
    "# Calculate total training time (expanding sum) for SGD variants\n",
    "SGD_1_total_training_time = SGD_1['Training Time (s)'].expanding().sum()\n",
    "SGD_32_total_training_time = SGD_32['Training Time (s)'].expanding().sum()\n",
    "SGD_128_total_training_time = SGD_128['Training Time (s)'].expanding().sum()\n",
    "SGD_512_total_training_time = SGD_512['Training Time (s)'].expanding().sum()\n",
    "\n",
    "# Create subplots (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Test Loss\n",
    "axes[0, 0].plot(epochs, GD_test_loss, label='GD', color='blue')\n",
    "axes[0, 0].plot(epochs, GNOM_test_loss, label='GNOM', color='red')\n",
    "axes[0, 0].plot(epochs, SGD_1_test_loss, label='SGD, Batch Size = 1', color='orange')\n",
    "axes[0, 0].plot(epochs, SGD_32_test_loss, label='SGD, Batch Size = 32', color='green')\n",
    "axes[0, 0].plot(epochs, SGD_128_test_loss, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[0, 0].plot(epochs, SGD_512_test_loss, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Test Loss (MSE)')\n",
    "axes[0, 0].set_title(f'Test Loss over {num_epochs} Epochs')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "axes[0, 1].plot(epochs, GD_training_loss, label='GD', color='blue')\n",
    "axes[0, 1].plot(epochs, GNOM_training_loss, label='GNOM', color='red')\n",
    "axes[0, 1].plot(epochs, SGD_1_training_loss, label='SGD, Batch Size = 1', color='orange')\n",
    "axes[0, 1].plot(epochs, SGD_32_training_loss, label='SGD, Batch Size = 32', color='green')\n",
    "axes[0, 1].plot(epochs, SGD_128_training_loss, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[0, 1].plot(epochs, SGD_512_training_loss, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Training Loss')\n",
    "axes[0, 1].set_title(f'Training Loss over {num_epochs} Epochs')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Training Gradient Norm\n",
    "axes[1, 1].plot(epochs, GD_train_norm, label='GD', color='blue')\n",
    "axes[1, 1].plot(epochs, GNOM_train_norm, label='GNOM', color='red')\n",
    "axes[1, 1].plot(epochs, SGD_1_train_norm, label='SGD, Batch Size = 1', color='orange')\n",
    "axes[1, 1].plot(epochs, SGD_32_train_norm, label='SGD, Batch Size = 32', color='green')\n",
    "axes[1, 1].plot(epochs, SGD_128_train_norm, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[1, 1].plot(epochs, SGD_512_train_norm, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Gradient Norm')\n",
    "axes[1, 1].set_title(f'Training Gradient Norm over {num_epochs} Epochs')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Plot 4: Test Gradient Norm\n",
    "axes[1, 0].plot(epochs, GD_test_norm, label='GD', color='blue')\n",
    "axes[1, 0].plot(epochs, GNOM_test_norm, label='GNOM', color='red')\n",
    "axes[1, 0].plot(epochs, SGD_1_test_norm, label='SGD, Batch Size = 1', color='orange')\n",
    "axes[1, 0].plot(epochs, SGD_32_test_norm, label='SGD, Batch Size = 32', color='green')\n",
    "axes[1, 0].plot(epochs, SGD_128_test_norm, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[1, 0].plot(epochs, SGD_512_test_norm, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title(f'Test Gradient Norm over {num_epochs} Epochs')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 5: Average Training Time\n",
    "axes[2, 0].plot(epochs, GD_avg_training_time, label='GD', color='blue')\n",
    "axes[2, 0].plot(epochs, GNOM_avg_training_time, label='GNOM', color='red')\n",
    "axes[2, 0].plot(epochs, SGD_1_avg_training_time, label='SGD, Batch Size = 1', color='orange')\n",
    "axes[2, 0].plot(epochs, SGD_32_avg_training_time, label='SGD, Batch Size = 32', color='green')\n",
    "axes[2, 0].plot(epochs, SGD_128_avg_training_time, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[2, 0].plot(epochs, SGD_512_avg_training_time, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[2, 0].set_xlabel('Epoch')\n",
    "axes[2, 0].set_ylabel('Average Training Time Per Epoch (s)')\n",
    "axes[2, 0].set_title(f'Average Epoch Training Time over {num_epochs} Epochs')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# Plot 6: Total Training Time\n",
    "axes[2, 1].plot(epochs, GD_total_training_time, label='GD', color='blue')\n",
    "axes[2, 1].plot(epochs, GNOM_total_training_time, label='GNOM', color='red')\n",
    "axes[2, 1].plot(epochs, SGD_1_total_training_time, label='SGD, Batch Size = 1', color='orange')\n",
    "axes[2, 1].plot(epochs, SGD_32_total_training_time, label='SGD, Batch Size = 32', color='green')\n",
    "axes[2, 1].plot(epochs, SGD_128_total_training_time, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[2, 1].plot(epochs, SGD_512_total_training_time, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[2, 1].set_xlabel('Epoch')\n",
    "axes[2, 1].set_ylabel('Total Training Time (s)')\n",
    "axes[2, 1].set_title(f'Total Training Time over {num_epochs} Epochs')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNOM vs. GD vs. SGD (No batch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Test Loss\n",
    "axes[0, 0].plot(epochs, GD_test_loss, label='GD', color='blue')\n",
    "axes[0, 0].plot(epochs, GNOM_test_loss, label='GNOM', color='red')\n",
    "axes[0, 0].plot(epochs, SGD_32_test_loss, label='SGD, Batch Size = 32', color='green')\n",
    "axes[0, 0].plot(epochs, SGD_128_test_loss, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[0, 0].plot(epochs, SGD_512_test_loss, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Test Loss (MSE)')\n",
    "axes[0, 0].set_title(f'Test Loss over {num_epochs} Epochs')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "axes[0, 1].plot(epochs, GD_training_loss, label='GD', color='blue')\n",
    "axes[0, 1].plot(epochs, GNOM_training_loss, label='GNOM', color='red')\n",
    "axes[0, 1].plot(epochs, SGD_32_training_loss, label='SGD, Batch Size = 32', color='green')\n",
    "axes[0, 1].plot(epochs, SGD_128_training_loss, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[0, 1].plot(epochs, SGD_512_training_loss, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Training Loss')\n",
    "axes[0, 1].set_title(f'Training Loss over {num_epochs} Epochs')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Training Gradient Norm\n",
    "axes[1, 1].plot(epochs, GD_train_norm, label='GD', color='blue')\n",
    "axes[1, 1].plot(epochs, GNOM_train_norm, label='GNOM', color='red')\n",
    "axes[1, 1].plot(epochs, SGD_32_train_norm, label='SGD, Batch Size = 32', color='green')\n",
    "axes[1, 1].plot(epochs, SGD_128_train_norm, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[1, 1].plot(epochs, SGD_512_train_norm, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Gradient Norm')\n",
    "axes[1, 1].set_title(f'Training Gradient Norm over {num_epochs} Epochs')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Plot 4: Test Gradient Norm\n",
    "axes[1, 0].plot(epochs, GD_test_norm, label='GD', color='blue')\n",
    "axes[1, 0].plot(epochs, GNOM_test_norm, label='GNOM', color='red')\n",
    "axes[1, 0].plot(epochs, SGD_32_test_norm, label='SGD, Batch Size = 32', color='green')\n",
    "axes[1, 0].plot(epochs, SGD_128_test_norm, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[1, 0].plot(epochs, SGD_512_test_norm, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title(f'Test Gradient Norm over {num_epochs} Epochs')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 5: Average Training Time\n",
    "axes[2, 0].plot(epochs, GD_avg_training_time, label='GD', color='blue')\n",
    "axes[2, 0].plot(epochs, GNOM_avg_training_time, label='GNOM', color='red')\n",
    "axes[2, 0].plot(epochs, SGD_32_avg_training_time, label='SGD, Batch Size = 32', color='green')\n",
    "axes[2, 0].plot(epochs, SGD_128_avg_training_time, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[2, 0].plot(epochs, SGD_512_avg_training_time, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[2, 0].set_xlabel('Epoch')\n",
    "axes[2, 0].set_ylabel('Average Training Time Per Epoch (s)')\n",
    "axes[2, 0].set_title(f'Average Epoch Training Time over {num_epochs} Epochs')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# Plot 6: Total Training Time\n",
    "axes[2, 1].plot(epochs, GD_total_training_time, label='GD', color='blue')\n",
    "axes[2, 1].plot(epochs, GNOM_total_training_time, label='GNOM', color='red')\n",
    "axes[2, 1].plot(epochs, SGD_32_total_training_time, label='SGD, Batch Size = 32', color='green')\n",
    "axes[2, 1].plot(epochs, SGD_128_total_training_time, label='SGD, Batch Size = 128', color='purple')\n",
    "axes[2, 1].plot(epochs, SGD_512_total_training_time, label='SGD, Batch Size = 512', color='brown')\n",
    "axes[2, 1].set_xlabel('Epoch')\n",
    "axes[2, 1].set_ylabel('Total Training Time (s)')\n",
    "axes[2, 1].set_title(f'Total Training Time over {num_epochs} Epochs')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNOM vs. GD vs. SGD Batch 512 Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 5197 / 512\n",
    "SGD_512_normalized = SGD_512.copy()  # Copy the original DataFrame\n",
    "SGD_512_normalized['Epoch'] = SGD_512_normalized['Epoch'] * ratio\n",
    "SGD_512_normalized = SGD_512_normalized[SGD_512_normalized['Epoch'] <= 200]\n",
    "\n",
    "\n",
    "SGD_512_normalized_test_loss = SGD_512_normalized['Test Loss']\n",
    "SGD_512_normalized_training_loss = SGD_512_normalized['Training Loss']\n",
    "SGD_512_normalized_train_norm = SGD_512_normalized['Training Gradient Norm']\n",
    "SGD_512_normalized_test_norm = SGD_512_normalized['Test Gradient Norm']\n",
    "SGD_512_normalized_avg_training_time = SGD_512_normalized['Training Time (s)'].expanding().mean()\n",
    "SGD_512_normalized_total_training_time = SGD_512_normalized['Training Time (s)'].expanding().sum()\n",
    "\n",
    "# Create subplots (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Test Loss\n",
    "axes[0, 0].plot(epochs, GD_test_loss, label='GD', color='blue')\n",
    "axes[0, 0].plot(epochs, GNOM_test_loss, label='GNOM', color='red')\n",
    "axes[0, 0].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_test_loss, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Test Loss (MSE)')\n",
    "axes[0, 0].set_title(f'Test Loss over {num_epochs} Iterations')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "axes[0, 1].plot(epochs, GD_training_loss, label='GD', color='blue')\n",
    "axes[0, 1].plot(epochs, GNOM_training_loss, label='GNOM', color='red')\n",
    "axes[0, 1].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_training_loss, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Training Loss')\n",
    "axes[0, 1].set_title(f'Training Loss over {num_epochs} Iterations')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Training Gradient Norm\n",
    "axes[1, 1].plot(epochs, GD_train_norm, label='GD', color='blue')\n",
    "axes[1, 1].plot(epochs, GNOM_train_norm, label='GNOM', color='red')\n",
    "axes[1, 1].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_train_norm, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Gradient Norm')\n",
    "axes[1, 1].set_title(f'Training Gradient Norm over {num_epochs} Iterations')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Plot 4: Test Gradient Norm\n",
    "axes[1, 0].plot(epochs, GD_test_norm, label='GD', color='blue')\n",
    "axes[1, 0].plot(epochs, GNOM_test_norm, label='GNOM', color='red')\n",
    "axes[1, 0].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_test_norm, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title(f'Test Gradient Norm over {num_epochs} Iterations')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 5: Average Training Time\n",
    "axes[2, 0].plot(epochs, GD_avg_training_time, label='GD', color='blue')\n",
    "axes[2, 0].plot(epochs, GNOM_avg_training_time, label='GNOM', color='red')\n",
    "axes[2, 0].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_avg_training_time, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[2, 0].set_xlabel('Iteration')\n",
    "axes[2, 0].set_ylabel('Average Training Time Per Iteration (s)')\n",
    "axes[2, 0].set_title(f'Average Epoch Training Time over {num_epochs} Iterations')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# Plot 6: Total Training Time\n",
    "axes[2, 1].plot(epochs, GD_total_training_time, label='GD', color='blue')\n",
    "axes[2, 1].plot(epochs, GNOM_total_training_time, label='GNOM', color='red')\n",
    "axes[2, 1].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_total_training_time, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[2, 1].set_xlabel('Iterations')\n",
    "axes[2, 1].set_ylabel('Total Training Time (s)')\n",
    "axes[2, 1].set_title(f'Total Training Time over {num_epochs} Iterations')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNOM vs GD vs SGD Normalized (512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ratio for normalization\n",
    "ratio_512 = 5197 / 512\n",
    "ratio_128 = 5197 / 128\n",
    "\n",
    "# Normalize SGD_512\n",
    "SGD_512_normalized = SGD_512.copy()  # Copy the original DataFrame\n",
    "SGD_512_normalized['Epoch'] = SGD_512_normalized['Epoch'] * ratio_512\n",
    "SGD_512_normalized = SGD_512_normalized[SGD_512_normalized['Epoch'] <= 200]\n",
    "\n",
    "# Normalize SGD_128\n",
    "SGD_128_normalized = SGD_128.copy()  # Copy the original DataFrame\n",
    "SGD_128_normalized['Epoch'] = SGD_128_normalized['Epoch'] * ratio_128\n",
    "SGD_128_normalized = SGD_128_normalized[SGD_128_normalized['Epoch'] <= 200]\n",
    "\n",
    "# Extract test loss, training loss, norms, and training times for the normalized data\n",
    "SGD_512_normalized_test_loss = SGD_512_normalized['Test Loss']\n",
    "SGD_512_normalized_training_loss = SGD_512_normalized['Training Loss']\n",
    "SGD_512_normalized_train_norm = SGD_512_normalized['Training Gradient Norm']\n",
    "SGD_512_normalized_test_norm = SGD_512_normalized['Test Gradient Norm']\n",
    "SGD_512_normalized_avg_training_time = SGD_512_normalized['Training Time (s)'].expanding().mean()\n",
    "SGD_512_normalized_total_training_time = SGD_512_normalized['Training Time (s)'].expanding().sum()\n",
    "\n",
    "SGD_128_normalized_test_loss = SGD_128_normalized['Test Loss']\n",
    "SGD_128_normalized_training_loss = SGD_128_normalized['Training Loss']\n",
    "SGD_128_normalized_train_norm = SGD_128_normalized['Training Gradient Norm']\n",
    "SGD_128_normalized_test_norm = SGD_128_normalized['Test Gradient Norm']\n",
    "SGD_128_normalized_avg_training_time = SGD_128_normalized['Training Time (s)'].expanding().mean()\n",
    "SGD_128_normalized_total_training_time = SGD_128_normalized['Training Time (s)'].expanding().sum()\n",
    "\n",
    "# Create subplots (3 rows, 2 columns)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Test Loss\n",
    "axes[0, 0].plot(epochs, GD_test_loss, label='GD', color='blue')\n",
    "axes[0, 0].plot(epochs, GNOM_test_loss, label='GNOM', color='red')\n",
    "axes[0, 0].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_test_loss, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[0, 0].plot(SGD_128_normalized['Epoch'], SGD_128_normalized_test_loss, label='SGD, Batch Size = 128 (Normalized)', color='purple')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Test Loss (MSE)')\n",
    "axes[0, 0].set_title(f'Test Loss over {num_epochs} Iterations')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Training Loss\n",
    "axes[0, 1].plot(epochs, GD_training_loss, label='GD', color='blue')\n",
    "axes[0, 1].plot(epochs, GNOM_training_loss, label='GNOM', color='red')\n",
    "axes[0, 1].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_training_loss, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[0, 1].plot(SGD_128_normalized['Epoch'], SGD_128_normalized_training_loss, label='SGD, Batch Size = 128 (Normalized)', color='purple')\n",
    "axes[0, 1].set_ylabel('Training Loss')\n",
    "axes[0, 1].set_title(f'Training Loss over {num_epochs} Iterations')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Training Gradient Norm\n",
    "axes[1, 1].plot(epochs, GD_train_norm, label='GD', color='blue')\n",
    "axes[1, 1].plot(epochs, GNOM_train_norm, label='GNOM', color='red')\n",
    "axes[1, 1].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_train_norm, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[1, 1].plot(SGD_128_normalized['Epoch'], SGD_128_normalized_train_norm, label='SGD, Batch Size = 128 (Normalized)', color='purple')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Gradient Norm')\n",
    "axes[1, 1].set_title(f'Training Gradient Norm over {num_epochs} Iterations')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Plot 4: Test Gradient Norm\n",
    "axes[1, 0].plot(epochs, GD_test_norm, label='GD', color='blue')\n",
    "axes[1, 0].plot(epochs, GNOM_test_norm, label='GNOM', color='red')\n",
    "axes[1, 0].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_test_norm, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[1, 0].plot(SGD_128_normalized['Epoch'], SGD_128_normalized_test_norm, label='SGD, Batch Size = 128 (Normalized)', color='purple')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title(f'Test Gradient Norm over {num_epochs} Iterations')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 5: Average Training Time\n",
    "axes[2, 0].plot(epochs, GD_avg_training_time, label='GD', color='blue')\n",
    "axes[2, 0].plot(epochs, GNOM_avg_training_time, label='GNOM', color='red')\n",
    "axes[2, 0].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_avg_training_time, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[2, 0].plot(SGD_128_normalized['Epoch'], SGD_128_normalized_avg_training_time, label='SGD, Batch Size = 128 (Normalized)', color='purple')\n",
    "axes[2, 0].set_xlabel('Iteration')\n",
    "axes[2, 0].set_ylabel('Average Training Time Per Iteration (s)')\n",
    "axes[2, 0].set_title(f'Average Epoch Training Time over {num_epochs} Iterations')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# Plot 6: Total Training Time\n",
    "axes[2, 1].plot(epochs, GD_total_training_time, label='GD', color='blue')\n",
    "axes[2, 1].plot(epochs, GNOM_total_training_time, label='GNOM', color='red')\n",
    "axes[2, 1].plot(SGD_512_normalized['Epoch'], SGD_512_normalized_total_training_time, label='SGD, Batch Size = 512 (Normalized)', color='green')\n",
    "axes[2, 1].plot(SGD_128_normalized['Epoch'], SGD_128_normalized_total_training_time, label='SGD, Batch Size = 128 (Normalized)', color='purple')\n",
    "axes[2, 1].set_xlabel('Iterations')\n",
    "axes[2, 1].set_ylabel('Total Training Time (s)')\n",
    "axes[2, 1].set_title(f'Total Training Time over {num_epochs} Iterations')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54ee8ad09054042b765fe01c0487e6cc18abb01d522b7848423350aaa59332bc"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit ('gam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
