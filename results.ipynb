{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and Displaying run results\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract information from the file content\n",
    "def extract_info_from_file(file_path):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract total time and final accuracy\n",
    "    total_time = None\n",
    "    final_accuracy = None\n",
    "    for line in reversed(lines):\n",
    "        if 'Total training time:' in line:\n",
    "            total_time = float(line.strip().split(': ')[1].split(' ')[0])\n",
    "        if 'accuracy:' in line:\n",
    "            final_accuracy = float(line.strip().split(': ')[1].replace('%', ''))\n",
    "            break  # Stop after the final accuracy is found as it is the first instance from the end\n",
    "\n",
    "    return total_time, final_accuracy\n",
    "\n",
    "# Function to parse file name\n",
    "def parse_filename(file_name):\n",
    "    parts = file_name.split('-')\n",
    "    if len(parts) == 5:\n",
    "        optimizer = parts[0]\n",
    "        model = parts[1]\n",
    "        dataset = parts[2]\n",
    "        augmentation = parts[3].split('.')[0]  # Remove file extension, prob need to add run number here\n",
    "    else:\n",
    "        optimizer, model, dataset, augmentation = None, None, None, None\n",
    "\n",
    "    return optimizer, model, dataset, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory\n",
    "folder_path = 'logs'  # Change 'folder_name' to the name of your folder\n",
    "\n",
    "# DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Optimizer', 'Model Architecture', 'Dataset', 'Data Augmentation', 'Total Time (s)', 'Final Accuracy (%)'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file in the directory\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.out'):  # Check if the file is a .out file\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        total_time, final_accuracy = extract_info_from_file(file_path)\n",
    "        optimizer, model, dataset, augmentation = parse_filename(file)\n",
    "        \n",
    "        # Append the results to the DataFrame\n",
    "        results_df = results_df._append({\n",
    "            'Optimizer': optimizer,\n",
    "            'Model Architecture': model,\n",
    "            'Dataset': dataset,\n",
    "            'Data Augmentation': augmentation,\n",
    "            'Final Accuracy (%)': final_accuracy,\n",
    "            'Total Time (s)': total_time\n",
    "        }, ignore_index=True)\n",
    "\n",
    "results_df['Total Time (min)'] = results_df['Total Time (s)'] / 60\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = results_df.groupby(['Optimizer', 'Model Architecture', 'Dataset', 'Data Augmentation']).agg({\n",
    "    'Final Accuracy (%)': ['mean', 'std'],\n",
    "    'Total Time (min)': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_df.columns = [\n",
    "    'Optimizer', \n",
    "    'Model Architecture', \n",
    "    'Dataset', \n",
    "    'Data Augmentation', \n",
    "    'Final Accuracy Mean (%)', \n",
    "    'Final Accuracy Std (%)',\n",
    "    'Total Time Mean (min)', \n",
    "    'Total Time Std (min)' \n",
    "]\n",
    "\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = grouped_df.sort_values(by=['Dataset', 'Data Augmentation'])\n",
    "\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define the custom order for Data Augmentation\n",
    "augmentation_order = [\"basicaug\", \"cutout\", \"autoaug\", \"randaug\"]\n",
    "optimizer_order = [\"sgd\", \"nonaccelgamsgd\", \"gamsgd\"]\n",
    "\n",
    "# Create a categorical type for Data Augmentation with the specified order\n",
    "grouped_df[\"Data Augmentation\"] = pd.Categorical(grouped_df[\"Data Augmentation\"], categories=augmentation_order, ordered=True)\n",
    "\n",
    "# Filter and sort DataFrame to match the desired format\n",
    "df_sorted = grouped_df.sort_values(by=[\"Dataset\", \"Data Augmentation\", \"Optimizer\"])\n",
    "df_sorted = df_sorted.loc[df_sorted[\"Data Augmentation\"] != \"randaug\"]\n",
    "df_cifar10 = df_sorted[df_sorted[\"Dataset\"] == \"cifar10\"]\n",
    "df_cifar100 = df_sorted[df_sorted[\"Dataset\"] == \"cifar100\"]\n",
    "\n",
    "# Prepare the CIFAR-10 and CIFAR-100 parts of the table\n",
    "table_cifar10 = df_cifar10.pivot(index=\"Data Augmentation\", columns=\"Optimizer\", values=\"Final Accuracy Mean (%)\").reset_index()\n",
    "table_cifar100 = df_cifar100.pivot(index=\"Data Augmentation\", columns=\"Optimizer\", values=\"Final Accuracy Mean (%)\").reset_index()\n",
    "\n",
    "table_cifar10 = table_cifar10[[\"Data Augmentation\"] + optimizer_order]\n",
    "\n",
    "# Add a new column to distinguish between CIFAR-10 and CIFAR-100\n",
    "table_cifar10.insert(0, 'Dataset', 'CIFAR-10')\n",
    "table_cifar100.insert(0, 'Dataset', 'CIFAR-100')\n",
    "\n",
    "# Combine the tables\n",
    "table = pd.concat([table_cifar10, table_cifar100], axis=0)\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table, headers='keys', tablefmt='pipe'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Norm Only Minimization (GNOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNOM learning rate testing (20 epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory\n",
    "folder_path_gnom = 'logs/GNOM-Tests'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epoch_time(file_path):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract total time and final accuracy\n",
    "    epoch_time = 0\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if 'Epoch' and 'time:' in line:\n",
    "            count += 1\n",
    "            epoch_time += float(line.strip().split(': ')[1].split(' ')[0])\n",
    "    return (epoch_time / count)\n",
    "\n",
    "def extract_test_info(file_path):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    parts = file_name.split('-')\n",
    "    epochs = 20\n",
    "    if len(parts) >= 6:\n",
    "        optimizer = parts[0]\n",
    "        model = parts[1]\n",
    "        dataset = parts[2]\n",
    "        augmentation = parts[3]\n",
    "    else:\n",
    "        optimizer, model, dataset, augmentation, learning_rate, batch_size = None, None, None, None, None, None\n",
    "    \n",
    "    if len(parts) == 7:\n",
    "        learning_rate = parts[5]\n",
    "        batch_size = parts[6].split('.')[0] \n",
    "        if batch_size == 'full':\n",
    "            batch_size = 128\n",
    "            epochs = 200\n",
    "    else:\n",
    "        learning_rate = parts[5][:-4]\n",
    "        batch_size = 128\n",
    "    \n",
    "    return optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnom_df = pd.DataFrame(columns=['Optimizer', 'Model Architecture', 'Dataset', 'Data Augmentation', 'Learning Rate', 'Batch Size', \"Epochs\", 'Final Accuracy (%)', 'Total Time (s)', 'Avg Epoch Time (s)'])\n",
    "\n",
    "# Process each file in the directory\n",
    "for file in os.listdir(folder_path_gnom):\n",
    "    if '-lr-' in file:  \n",
    "        file_path = os.path.join(folder_path_gnom, file)\n",
    "        total_time, final_accuracy = extract_info_from_file(file_path)\n",
    "        epoch_time = extract_epoch_time(file_path)\n",
    "        optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs = extract_test_info(file_path)\n",
    "        \n",
    "        gnom_df = gnom_df._append({\n",
    "            'Optimizer': optimizer,\n",
    "            'Model Architecture': model,\n",
    "            'Dataset': dataset,\n",
    "            'Data Augmentation': augmentation,\n",
    "            'Learning Rate': float(learning_rate),\n",
    "            'Batch Size': int(batch_size),\n",
    "            'Epochs': epochs,\n",
    "            'Final Accuracy (%)': final_accuracy,\n",
    "            'Total Time (s)': total_time,\n",
    "            'Avg Epoch Time (s)': epoch_time\n",
    "        }, ignore_index=True)\n",
    "\n",
    "gnom_df['Total Time (min)'] = gnom_df['Total Time (s)'] / 60\n",
    "\n",
    "gnom_df_sorted = gnom_df.sort_values(by=['Batch Size', 'Learning Rate'])\n",
    "\n",
    "gnom_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNOM Gradient Norm and Hessian Eigenvalue testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ev_test_info(file_path):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    parts = file_name.split('-')\n",
    "    if len(parts) == 9:\n",
    "        optimizer = parts[0]\n",
    "        model = parts[1]\n",
    "        dataset = parts[2]\n",
    "        augmentation = parts[3]\n",
    "        learning_rate = parts[5]\n",
    "        batch_size = parts[6]\n",
    "        epochs = parts[7]\n",
    "    elif len(parts) == 8:\n",
    "        optimizer = parts[0]\n",
    "        model = parts[1]\n",
    "        dataset = parts[2]\n",
    "        augmentation = parts[3]\n",
    "        learning_rate = parts[5]\n",
    "        batch_size = parts[6]\n",
    "        epochs = parts[7][:-4]\n",
    "    else:\n",
    "        optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs = None, None, None, None, None, None, None\n",
    "    \n",
    "    return optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs\n",
    "\n",
    "def extract_grad_info(file_path):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract total time and final accuracy\n",
    "    grad_norm = None\n",
    "    top_ev = None\n",
    "    smallest_ev = None\n",
    "    for line in reversed(lines):\n",
    "        if 'Smallest Hessian Eigenvalue:' in line:\n",
    "            smallest_ev = float(line.strip().split(': ')[1])\n",
    "        if 'Norm of the Gradient:' in line:\n",
    "            grad_norm = float(line.strip().split(': ')[1])\n",
    "        if 'Largest Hessian Eigenvalue:' in line:\n",
    "            top_ev = float(line.strip().split(': ')[1])\n",
    "            break\n",
    "\n",
    "    return grad_norm, top_ev, smallest_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_gnom_ev = 'logs/GNOM-Saved-Models'  \n",
    "gnom_ev_df = pd.DataFrame(columns=['Optimizer', 'Learning Rate', 'Batch Size', \"Epochs\", 'Final Accuracy (%)', 'Total Time (s)', 'Avg Epoch Time (s)', 'Gradient Norm', 'Largest Eigenvalue of Hessian', 'Smallest Eigenvalue of Hessian'])\n",
    "\n",
    "# Process each file in the directory\n",
    "for file in os.listdir(folder_path_gnom_ev):\n",
    "    file_path = os.path.join(folder_path_gnom_ev, file)\n",
    "    total_time, final_accuracy = extract_info_from_file(file_path)\n",
    "    epoch_time = extract_epoch_time(file_path)\n",
    "    optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs = extract_ev_test_info(file_path)\n",
    "    grad_norm, top_ev, smallest_ev = extract_grad_info(file_path)\n",
    "    \n",
    "    gnom_ev_df = gnom_ev_df._append({\n",
    "        'Optimizer': optimizer,\n",
    "        'Learning Rate': float(learning_rate),\n",
    "        'Batch Size': int(batch_size),\n",
    "        'Epochs': epochs,\n",
    "        'Final Accuracy (%)': final_accuracy,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Avg Epoch Time (s)': epoch_time,\n",
    "        'Gradient Norm': grad_norm,\n",
    "        'Largest Eigenvalue of Hessian': top_ev,\n",
    "        'Smallest Eigenvalue of Hessian': smallest_ev\n",
    "    }, ignore_index=True)\n",
    "\n",
    "gnom_ev_df['Total Time (min)'] = gnom_ev_df['Total Time (s)'] / 60\n",
    "gnom_ev_df['Avg Epoch Time (min)'] = gnom_ev_df['Avg Epoch Time (s)'] / 60\n",
    "gnom_ev_df.drop(columns=['Total Time (s)', 'Avg Epoch Time (s)'], inplace=True)\n",
    "\n",
    "gnom_ev_df_sorted = gnom_ev_df.sort_values(by=['Epochs','Optimizer','Batch Size'])\n",
    "\n",
    "print(\"All of the below runs were done with ResNet18, CIFAR-10, and basic data augmentations\")\n",
    "gnom_ev_df_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNOM Noised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noise_info(file_path):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Extract total time and final accuracy\n",
    "    batches_noised = None\n",
    "    for line in reversed(lines):\n",
    "        if 'Noise applied in' in line:\n",
    "            batches_noised = float(line.strip().split(' ')[8])\n",
    "            break\n",
    "\n",
    "    return batches_noised\n",
    "\n",
    "def extract_noised_test_info(file_path):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    parts = file_name.split('-')\n",
    "    if len(parts) >= 8:\n",
    "        optimizer = parts[0]\n",
    "        model = parts[1]\n",
    "        dataset = parts[2]\n",
    "        augmentation = parts[3]\n",
    "        learning_rate = parts[5]\n",
    "        batch_size = parts[6]\n",
    "    \n",
    "    if len(parts) == 10:\n",
    "        epochs = parts[7]\n",
    "        threshold = parts[8]\n",
    "        radius = parts[9][:-4]\n",
    "    else:\n",
    "        epochs = parts[7][:-4]\n",
    "        threshold, radius = 0.1, 0.01\n",
    "    \n",
    "    return optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs, threshold, radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_gnom_noised = 'logs/GNOM-Noised'  \n",
    "gnom_noised_df = pd.DataFrame(columns=['Optimizer', 'Learning Rate', 'Batch Size', \"Epochs\", 'Final Accuracy (%)', 'Total Time (s)', 'Avg Epoch Time (s)', 'Gradient Norm', 'Largest Eigenvalue of Hessian', 'Smallest Eigenvalue of Hessian'])\n",
    "\n",
    "# Process each file in the directory\n",
    "for file in os.listdir(folder_path_gnom_noised):\n",
    "    file_path = os.path.join(folder_path_gnom_noised, file)\n",
    "    total_time, final_accuracy = extract_info_from_file(file_path)\n",
    "    epoch_time = extract_epoch_time(file_path)\n",
    "    optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs, threshold, radius = extract_noised_test_info(file_path)\n",
    "    grad_norm, top_ev, smallest_ev = extract_grad_info(file_path)\n",
    "    batches_noised = extract_noise_info(file_path)\n",
    "    \n",
    "    gnom_noised_df = gnom_noised_df._append({\n",
    "        'Optimizer': optimizer,\n",
    "        'Learning Rate': float(learning_rate),\n",
    "        'Batch Size': int(batch_size),\n",
    "        'Epochs': epochs,\n",
    "        'Final Accuracy (%)': final_accuracy,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Avg Epoch Time (s)': epoch_time,\n",
    "        'Gradient Norm': grad_norm,\n",
    "        'Largest Eigenvalue of Hessian': top_ev,\n",
    "        'Smallest Eigenvalue of Hessian': smallest_ev,\n",
    "        'Batches Noise was Added (%)': batches_noised,\n",
    "        'Noise Gradient Norm Threshold': threshold,\n",
    "        'Noise Added Radius': radius\n",
    "    }, ignore_index=True)\n",
    "\n",
    "gnom_noised_df['Total Time (min)'] = gnom_noised_df['Total Time (s)'] / 60\n",
    "gnom_noised_df['Avg Epoch Time (min)'] = gnom_noised_df['Avg Epoch Time (s)'] / 60\n",
    "gnom_noised_df.drop(columns=['Total Time (s)', 'Avg Epoch Time (s)'], inplace=True)\n",
    "\n",
    "gnom_noised_df_sorted = gnom_noised_df.sort_values(by=['Final Accuracy (%)'], ascending=False)\n",
    "\n",
    "print(\"All of the below runs were done with ResNet18, CIFAR-10, and basic data augmentations\")\n",
    "\n",
    "gnom_noised_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNOM Normal vs. Noised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.concat([gnom_ev_df_sorted, gnom_noised_df_sorted], ignore_index=True)\n",
    "gnom_results = combined_results[combined_results['Optimizer'].isin([\"gnomsgd\"])]\n",
    "gnom_results = gnom_results.sort_values(by=['Final Accuracy (%)'])\n",
    "gnom_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNOM Noised with Partitioned Gradient Calculation Dataset\n",
    "#### Batch fixed at 256 and learning rate fixed at 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_gnom_noised_part = 'logs/GNOM-Noised-Partitioned'  \n",
    "gnom_noised_part_df = pd.DataFrame(columns=['Optimizer', 'Learning Rate', 'Batch Size', \"Epochs\", 'Final Accuracy (%)', 'Total Time (s)', 'Avg Epoch Time (s)', 'Gradient Norm', 'Largest Eigenvalue of Hessian', 'Smallest Eigenvalue of Hessian'])\n",
    "\n",
    "# Process each file in the directory\n",
    "for file in os.listdir(folder_path_gnom_noised_part):\n",
    "    file_path = os.path.join(folder_path_gnom_noised_part, file)\n",
    "    total_time, final_accuracy = extract_info_from_file(file_path)\n",
    "    epoch_time = extract_epoch_time(file_path)\n",
    "    optimizer, model, dataset, augmentation, learning_rate, batch_size, epochs, threshold, radius = extract_noised_test_info(file_path)\n",
    "    grad_norm, top_ev, smallest_ev = extract_grad_info(file_path)\n",
    "    batches_noised = extract_noise_info(file_path)\n",
    "    \n",
    "    gnom_noised_part_df = gnom_noised_part_df._append({\n",
    "        'Optimizer': optimizer,\n",
    "        'Learning Rate': float(learning_rate),\n",
    "        'Batch Size': int(batch_size),\n",
    "        'Epochs': epochs,\n",
    "        'Final Accuracy (%)': final_accuracy,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Avg Epoch Time (s)': epoch_time,\n",
    "        'Gradient Norm': grad_norm,\n",
    "        'Largest Eigenvalue of Hessian': top_ev,\n",
    "        'Smallest Eigenvalue of Hessian': smallest_ev,\n",
    "        'Batches Noise was Added (%)': batches_noised,\n",
    "        'Noise Gradient Norm Threshold': threshold,\n",
    "        'Noise Added Radius': radius\n",
    "    }, ignore_index=True)\n",
    "\n",
    "gnom_noised_part_df['Total Time (min)'] = gnom_noised_part_df['Total Time (s)'] / 60\n",
    "gnom_noised_part_df['Avg Epoch Time (min)'] = gnom_noised_part_df['Avg Epoch Time (s)'] / 60\n",
    "gnom_noised_part_df.drop(columns=['Total Time (s)', 'Avg Epoch Time (s)'], inplace=True)\n",
    "\n",
    "gnom_noised_part_df_sorted = gnom_noised_part_df.sort_values(by=['Final Accuracy (%)'], ascending=False)\n",
    "\n",
    "gnom_noised_part_df_sorted"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7427c5886eccbbde56fc6db227e9c3ee16be9c2b7aa728bd26124698ee6970d8"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit ('gam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
